{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"text-align:center\"> <h1>Prostrate Cancer Classification</h1></p>\n",
    "\n",
    "Date:  12 October 2017 \n",
    " \n",
    "Version: 1\n",
    "\n",
    "Environment: R 3.3.2 and Jupyter notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook covers a complete data analysis task, which deals with **Exploratory Data Analysis**, **Regression**, and **Classification**. It involves exploring the data through getting different statistics, creating a regression model to impute several missing values, and analyzing different classifiers to determine which is suitable for the dataset. As said in the **no free lunch theorem**, no model can universally solve all problems, so indeed a comparison of different models is important.\n",
    "\n",
    "**Exploratory Data Analysis** is a concept that has been a vital part of any data science task. It's always a must that one must explore that data to be able to propose an appropriate data analysis approach.\n",
    "\n",
    "**Regression** has also been discussed in the past notebooks, so this will only cover a brief revision of the topic. Regression models are used to impute new or missing quantitative values based on the past or available values in the dataset. \n",
    "\n",
    "**Classification** is performed when the target variable is qualitative (or *categorical*). A previous notebook has already dealt with binary classification, and this notebook will feature multi-class classification.\n",
    "\n",
    "## Contents\n",
    "1. Exploratory Data Analysis\n",
    "    <br/>1.1 Data Description \n",
    "    <br/>1.2 Determining Variable Type\n",
    "    <br/>1.3 Data Cleaning\n",
    "    <br/>1.4 Missing Observations\n",
    "    <br/>1.5 Basic Statistics\n",
    "    <br/>1.6 Variable Distribution\n",
    "    <br/>1.7 Relationship Among Variables\n",
    "2. Regression\n",
    "    <br/>2.1 Predicting ATT1\n",
    "    <br/>2.2 Predicting ATT3\n",
    "    <br/>2.3 Predicting ATT4\n",
    "    <br/>2.4 Predicting ATT5\n",
    "    <br/>2.5 Summary\n",
    "3. Classificaton\n",
    "    <br/> 3.1 Naive Bayes Classifier\n",
    "    <br/> 3.2 Decision Trees\n",
    "    <br/> 3.3 Support Vector Machines (SVMs)\n",
    "    <br/> 3.4 Neural Networks\n",
    "4. Summary of the Classification Models \n",
    "5. Conclusion\n",
    "6. References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R version 3.3.2 (2016-10-31)\n",
       "Platform: x86_64-w64-mingw32/x64 (64-bit)\n",
       "Running under: Windows 10 x64 (build 15063)\n",
       "\n",
       "locale:\n",
       "[1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252   \n",
       "[3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C                      \n",
       "[5] LC_TIME=English_Australia.1252    \n",
       "\n",
       "attached base packages:\n",
       "[1] stats     graphics  grDevices utils     datasets  methods   base     \n",
       "\n",
       "loaded via a namespace (and not attached):\n",
       " [1] R6_2.2.2            magrittr_1.5        IRdisplay_0.4.4    \n",
       " [4] pbdZMQ_0.2-6        tools_3.3.2         crayon_1.3.2       \n",
       " [7] uuid_0.1-2          stringi_1.1.5       IRkernel_0.8.7.9000\n",
       "[10] jsonlite_1.5        stringr_1.2.0       digest_0.6.12      \n",
       "[13] repr_0.12.0         evaluate_0.10.1    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# information about the R kernel and the machine used\n",
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'reshape2' was built under R version 3.3.3\"Warning message:\n",
      "\"package 'car' was built under R version 3.3.3\"Warning message:\n",
      "\"package 'glmnet' was built under R version 3.3.3\"Loading required package: Matrix\n",
      "Warning message:\n",
      "\"package 'Matrix' was built under R version 3.3.3\"Loading required package: foreach\n",
      "Warning message:\n",
      "\"package 'foreach' was built under R version 3.3.3\"Loaded glmnet 2.0-10\n",
      "\n",
      "Warning message:\n",
      "\"package 'klaR' was built under R version 3.3.3\"Loading required package: MASS\n",
      "Warning message:\n",
      "\"package 'MASS' was built under R version 3.3.3\"Warning message:\n",
      "\"package 'caret' was built under R version 3.3.3\"Loading required package: lattice\n",
      "Warning message:\n",
      "\"package 'lattice' was built under R version 3.3.3\"Loading required package: ggplot2\n",
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.3.3\"Warning message:\n",
      "\"package 'tree' was built under R version 3.3.3\"Warning message:\n",
      "\"package 'randomForest' was built under R version 3.3.3\"randomForest 4.6-12\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n",
      "Warning message:\n",
      "\"package 'e1071' was built under R version 3.3.3\"\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Your next step is to start H2O:\n",
      "    > h2o.init()\n",
      "\n",
      "For H2O package documentation, ask for help:\n",
      "    > ??h2o\n",
      "\n",
      "After starting H2O, you can use the Web UI at http://localhost:54321\n",
      "For more information visit http://docs.h2o.ai\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Attaching package: 'h2o'\n",
      "\n",
      "The following objects are masked from 'package:stats':\n",
      "\n",
      "    cor, sd, var\n",
      "\n",
      "The following objects are masked from 'package:base':\n",
      "\n",
      "    %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n",
      "    colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n",
      "    log10, log1p, log2, round, signif, trunc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imported libraries\n",
    "library(reshape2) # for using melt()\n",
    "library(car)\n",
    "library(glmnet) # for regression\n",
    "library(klaR) # for naive bayes\n",
    "library(caret) # for naive bayes\n",
    "library(tree) # for decision trees\n",
    "library(randomForest) # for random forest\n",
    "library(e1071) # for svm\n",
    "library(h2o) # for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploration\n",
    "\n",
    "This part comprises of the following tasks:\n",
    "1. Importing the data\n",
    "2. Checking the variable types\n",
    "3. Cleaning the data \n",
    "4. Checking out missing values\n",
    "5. Checking the basic statistics of the variables\n",
    "6. Checking for the relationship between variables\n",
    "\n",
    "## 1.1 Data Description\n",
    "\n",
    "The data was imported using the `read.csv()` function.\n",
    "\n",
    "The **data used** is a CSV file that describes different stages of prostrate cancer with 10 numerical, discrete features. There are four classes of prostrate cancer: 0 for **curable**, 1 for **tumour stage**, 2 for **node stage**, and 3 for **incurable cancers**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>ID</th><th scope=col>ATT1</th><th scope=col>ATT2</th><th scope=col>ATT3</th><th scope=col>ATT4</th><th scope=col>ATT5</th><th scope=col>ATT6</th><th scope=col>ATT7</th><th scope=col>ATT8</th><th scope=col>ATT9</th><th scope=col>ATT10</th><th scope=col>Result</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1 </td><td>1 </td><td>4 </td><td>1 </td><td>4 </td><td>3 </td><td>7 </td><td> 1</td><td>2 </td><td>6 </td><td>8 </td><td>0 </td></tr>\n",
       "\t<tr><td>2 </td><td>? </td><td>8 </td><td>9 </td><td>1 </td><td>1 </td><td>1 </td><td> 1</td><td>5 </td><td>6 </td><td>1 </td><td>1 </td></tr>\n",
       "\t<tr><td>3 </td><td>10</td><td>7 </td><td>? </td><td>7 </td><td>? </td><td>5 </td><td> 2</td><td>7 </td><td>1 </td><td>1 </td><td>2 </td></tr>\n",
       "\t<tr><td>4 </td><td>3 </td><td>4 </td><td>3 </td><td>? </td><td>2 </td><td>8 </td><td> 4</td><td>6 </td><td>7 </td><td>2 </td><td>1 </td></tr>\n",
       "\t<tr><td>5 </td><td>3 </td><td>5 </td><td>2 </td><td>1 </td><td>6 </td><td>5 </td><td> 3</td><td>1 </td><td>7 </td><td>1 </td><td>0 </td></tr>\n",
       "\t<tr><td>6 </td><td>2 </td><td>7 </td><td>3 </td><td>2 </td><td>1 </td><td>4 </td><td>10</td><td>3 </td><td>9 </td><td>5 </td><td>1 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " ID & ATT1 & ATT2 & ATT3 & ATT4 & ATT5 & ATT6 & ATT7 & ATT8 & ATT9 & ATT10 & Result\\\\\n",
       "\\hline\n",
       "\t 1  & 1  & 4  & 1  & 4  & 3  & 7  &  1 & 2  & 6  & 8  & 0 \\\\\n",
       "\t 2  & ?  & 8  & 9  & 1  & 1  & 1  &  1 & 5  & 6  & 1  & 1 \\\\\n",
       "\t 3  & 10 & 7  & ?  & 7  & ?  & 5  &  2 & 7  & 1  & 1  & 2 \\\\\n",
       "\t 4  & 3  & 4  & 3  & ?  & 2  & 8  &  4 & 6  & 7  & 2  & 1 \\\\\n",
       "\t 5  & 3  & 5  & 2  & 1  & 6  & 5  &  3 & 1  & 7  & 1  & 0 \\\\\n",
       "\t 6  & 2  & 7  & 3  & 2  & 1  & 4  & 10 & 3  & 9  & 5  & 1 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "ID | ATT1 | ATT2 | ATT3 | ATT4 | ATT5 | ATT6 | ATT7 | ATT8 | ATT9 | ATT10 | Result | \n",
       "|---|---|---|---|---|---|\n",
       "| 1  | 1  | 4  | 1  | 4  | 3  | 7  |  1 | 2  | 6  | 8  | 0  | \n",
       "| 2  | ?  | 8  | 9  | 1  | 1  | 1  |  1 | 5  | 6  | 1  | 1  | \n",
       "| 3  | 10 | 7  | ?  | 7  | ?  | 5  |  2 | 7  | 1  | 1  | 2  | \n",
       "| 4  | 3  | 4  | 3  | ?  | 2  | 8  |  4 | 6  | 7  | 2  | 1  | \n",
       "| 5  | 3  | 5  | 2  | 1  | 6  | 5  |  3 | 1  | 7  | 1  | 0  | \n",
       "| 6  | 2  | 7  | 3  | 2  | 1  | 4  | 10 | 3  | 9  | 5  | 1  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  ID ATT1 ATT2 ATT3 ATT4 ATT5 ATT6 ATT7 ATT8 ATT9 ATT10 Result\n",
       "1 1  1    4    1    4    3    7     1   2    6    8     0     \n",
       "2 2  ?    8    9    1    1    1     1   5    6    1     1     \n",
       "3 3  10   7    ?    7    ?    5     2   7    1    1     2     \n",
       "4 4  3    4    3    ?    2    8     4   6    7    2     1     \n",
       "5 5  3    5    2    1    6    5     3   1    7    1     0     \n",
       "6 6  2    7    3    2    1    4    10   3    9    5     1     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- read.csv('data3000Final.csv')\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Determining Variable Type\n",
    "\n",
    "To get an initial overview of the resulting dataframe, check the following:\n",
    "- dimensions of the dataframe (how many rows of records? how many columns?)\n",
    "- the initial parsing of the data (what are the data types?)\n",
    "- are the data in the correct format?\n",
    "\n",
    "#### Dimensions of the Dataframe\n",
    "\n",
    "To check for the dimension, the function `dim()` is used. This function accepts a data frame and outputs the dimensions in *rows* x *columns*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>3000</li>\n",
       "\t<li>12</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 3000\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 3000\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 3000   12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Types\n",
    "\n",
    "There are 11 features, and all of them looks discrete. The features are named **ID, ATT1, ATT2, ATT3, ATT4, ATT5, ATT6, ATT7, ATT8, ATT9, and ATT10**, while the target variable is named **Result**. The feature names are not very descriptive, so it's hard to put them into context. However, we can still use basic statistics to determine some interesting properties of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t3000 obs. of  12 variables:\n",
      " $ ID    : int  1 2 3 4 5 6 7 8 9 10 ...\n",
      " $ ATT1  : Factor w/ 11 levels \"?\",\"1\",\"10\",\"2\",..: 2 1 3 5 5 4 2 10 4 2 ...\n",
      " $ ATT2  : int  4 8 7 4 5 7 1 5 9 1 ...\n",
      " $ ATT3  : Factor w/ 11 levels \"?\",\"1\",\"10\",\"2\",..: 2 11 1 5 4 5 5 4 4 2 ...\n",
      " $ ATT4  : Factor w/ 11 levels \"?\",\"1\",\"10\",\"2\",..: 6 2 9 1 2 4 7 8 2 10 ...\n",
      " $ ATT5  : Factor w/ 11 levels \"?\",\"1\",\"10\",\"2\",..: 5 2 1 4 8 2 11 4 6 11 ...\n",
      " $ ATT6  : int  7 1 5 8 5 4 3 7 6 2 ...\n",
      " $ ATT7  : int  1 1 2 4 3 10 7 3 8 2 ...\n",
      " $ ATT8  : int  2 5 7 6 1 3 1 6 8 5 ...\n",
      " $ ATT9  : int  6 6 1 7 7 9 10 7 4 1 ...\n",
      " $ ATT10 : int  8 1 1 2 1 5 7 3 3 3 ...\n",
      " $ Result: int  0 1 2 1 0 1 1 1 1 0 ...\n"
     ]
    }
   ],
   "source": [
    "str(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ATT1, ATT3, ATT4, and ATT5 has missing values care of \"?\" sign.\n",
    "\n",
    "## 1.3 Dataset Cleaning\n",
    "\n",
    "As above, we can see there's an attribute called **ID** that is just used to uniquely identify each observation. Since this is irrelevant when performing data analysis, it is better to remove it.\n",
    "\n",
    "A value of **\"?\" ** was also used to indicate missing values in the dataset. To conform with R standards, this variable must be turned into \"NA\". This makes it easier to filter and detect.\n",
    "\n",
    "The remaining features which were classified as factor due to the existence of \"?\" were then converted to numeric to prepare it for the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t3000 obs. of  11 variables:\n",
      " $ ATT1  : num  1 NA 10 3 3 2 1 8 2 1 ...\n",
      " $ ATT2  : int  4 8 7 4 5 7 1 5 9 1 ...\n",
      " $ ATT3  : num  1 9 NA 3 2 3 3 2 2 1 ...\n",
      " $ ATT4  : num  4 1 7 NA 1 2 5 6 1 8 ...\n",
      " $ ATT5  : num  3 1 NA 2 6 1 9 2 4 9 ...\n",
      " $ ATT6  : int  7 1 5 8 5 4 3 7 6 2 ...\n",
      " $ ATT7  : int  1 1 2 4 3 10 7 3 8 2 ...\n",
      " $ ATT8  : int  2 5 7 6 1 3 1 6 8 5 ...\n",
      " $ ATT9  : int  6 6 1 7 7 9 10 7 4 1 ...\n",
      " $ ATT10 : int  8 1 1 2 1 5 7 3 3 3 ...\n",
      " $ Result: int  0 1 2 1 0 1 1 1 1 0 ...\n"
     ]
    }
   ],
   "source": [
    "# data cleaning\n",
    "\n",
    "# remove 'ID' column\n",
    "data <- data[, -1]\n",
    "\n",
    "# replace ? with NA\n",
    "data[data == '?'] <- NA\n",
    "\n",
    "# convert data type to INT\n",
    "data[, c(1, 3, 4, 5)] <- sapply(data[, c(1, 3, 4, 5)], as.character)\n",
    "data[, c(1, 3, 4, 5)] <- sapply(data[, c(1, 3, 4, 5)], as.numeric)\n",
    "\n",
    "str(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Checking for Missing Variables\n",
    "\n",
    "Since the \"?\" observations were converted into `NA`, one can easily count the number of missing features by just using the `is.na()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "\t<li>&lt;NA&gt;</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\item <NA>\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. &lt;NA&gt;\n",
       "2. &lt;NA&gt;\n",
       "3. &lt;NA&gt;\n",
       "4. &lt;NA&gt;\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] NA NA NA NA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "4"
      ],
      "text/latex": [
       "4"
      ],
      "text/markdown": [
       "4"
      ],
      "text/plain": [
       "[1] 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how many NA's do we have?\n",
    "data[is.na(data)]\n",
    "length(data[is.na(data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based from the `str()` call above, these four `NA`s are under **ATT1, ATT3, ATT4, and ATT5**. \n",
    "\n",
    "Since there are 4 `NA`s located in unique features, we need to produce 4 regression models.\n",
    "\n",
    "## 1.5 Computing for Data Statistics\n",
    "\n",
    "One can easily get various statistics per feature by calling the `summary()` function. It also counts the number of `NA`'s per feature to validate the number of missing values that the data have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      ATT1             ATT2             ATT3             ATT4      \n",
       " Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  \n",
       " 1st Qu.: 2.000   1st Qu.: 2.000   1st Qu.: 2.000   1st Qu.: 2.00  \n",
       " Median : 4.000   Median : 4.000   Median : 4.000   Median : 4.00  \n",
       " Mean   : 4.425   Mean   : 4.501   Mean   : 4.422   Mean   : 4.44  \n",
       " 3rd Qu.: 7.000   3rd Qu.: 7.000   3rd Qu.: 7.000   3rd Qu.: 6.00  \n",
       " Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  \n",
       " NA's   :1                         NA's   :1        NA's   :1      \n",
       "      ATT5             ATT6             ATT7             ATT8       \n",
       " Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.000  \n",
       " 1st Qu.: 2.000   1st Qu.: 2.000   1st Qu.: 2.000   1st Qu.: 2.000  \n",
       " Median : 4.000   Median : 4.000   Median : 4.000   Median : 4.000  \n",
       " Mean   : 4.326   Mean   : 4.431   Mean   : 4.479   Mean   : 4.478  \n",
       " 3rd Qu.: 6.000   3rd Qu.: 6.000   3rd Qu.: 7.000   3rd Qu.: 7.000  \n",
       " Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n",
       " NA's   :1                                                          \n",
       "      ATT9            ATT10            Result     \n",
       " Min.   : 1.000   Min.   : 1.000   Min.   :0.000  \n",
       " 1st Qu.: 2.000   1st Qu.: 2.000   1st Qu.:0.000  \n",
       " Median : 4.000   Median : 4.000   Median :1.000  \n",
       " Mean   : 4.446   Mean   : 4.459   Mean   :0.984  \n",
       " 3rd Qu.: 6.000   3rd Qu.: 7.000   3rd Qu.:2.000  \n",
       " Max.   :10.000   Max.   :10.000   Max.   :3.000  \n",
       "                                                  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflections**\n",
    "- All the attributes are 'scaled' from 1 to 10.\n",
    "- The `summary()` call validates that there are indeed four missing values in different attributes.\n",
    "- All of the mean and median of the attributes are around ~4, which makes it likely that all of them follow the **Normal Distribution**.\n",
    "\n",
    "## 1.6 Determining the Variables' Distribution\n",
    "\n",
    "It is a good practice to check the distribution of the features by plotting its histogram. To do this we can use `ggplot()` that creates pretty and neat histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No id variables; using all as measure variables\n",
      "Warning message:\n",
      "\"Removed 4 rows containing non-finite values (stat_bin).\""
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAABNTU1ZWVloaGh8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\nTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "melted.data <- melt(data[1:(length(data))])\n",
    "histograms <- ggplot(data = melted.data, mapping = aes(x = value)) + \n",
    "                    geom_histogram(bins = 50) + facet_wrap(~variable, scales = 'free_x') + theme_light() \n",
    "histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflections**\n",
    "- Apparently, all of the features do **not** follow the Normal Distribution. However, they all follow the same distribution that resembles the Binomial distribution.\n",
    "- There's also a class imbalance that exists. From the histogram of **Result** we can see that the class **3** is under-represented. Putting it into context, based on the dataset there are less people with **incurable prostrate cancer** than those whose cancers are treatable, which is a good thing!\n",
    "\n",
    "## 1.7 Determining the Association Between Variables\n",
    "\n",
    "To find any *linear* correlation between two variables, we can just get their correlation coefficients since all of them are numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>ATT1</th><th scope=col>ATT2</th><th scope=col>ATT3</th><th scope=col>ATT4</th><th scope=col>ATT5</th><th scope=col>ATT6</th><th scope=col>ATT7</th><th scope=col>ATT8</th><th scope=col>ATT9</th><th scope=col>ATT10</th><th scope=col>Result</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>ATT1</th><td>1.00000000</td><td>0.04610493</td><td>0.08060515</td><td>0.05601720</td><td>0.04422308</td><td>0.05452177</td><td>0.07122578</td><td>0.05026351</td><td>0.05418733</td><td>0.05468297</td><td>0.3742627 </td></tr>\n",
       "\t<tr><th scope=row>ATT2</th><td>0.04610493</td><td>1.00000000</td><td>0.06709630</td><td>0.06157343</td><td>0.03169298</td><td>0.03936361</td><td>0.01121757</td><td>0.02974010</td><td>0.09424293</td><td>0.07271654</td><td>0.3512550 </td></tr>\n",
       "\t<tr><th scope=row>ATT3</th><td>0.08060515</td><td>0.06709630</td><td>1.00000000</td><td>0.04656806</td><td>0.06498553</td><td>0.04017917</td><td>0.03144363</td><td>0.02602814</td><td>0.06029647</td><td>0.08543024</td><td>0.3652525 </td></tr>\n",
       "\t<tr><th scope=row>ATT4</th><td>0.05601720</td><td>0.06157343</td><td>0.04656806</td><td>1.00000000</td><td>0.08003001</td><td>0.04547098</td><td>0.05860170</td><td>0.02125053</td><td>0.02801731</td><td>0.08264849</td><td>0.3603542 </td></tr>\n",
       "\t<tr><th scope=row>ATT5</th><td>0.04422308</td><td>0.03169298</td><td>0.06498553</td><td>0.08003001</td><td>1.00000000</td><td>0.04995859</td><td>0.07143985</td><td>0.02907593</td><td>0.08373312</td><td>0.08360732</td><td>0.3742799 </td></tr>\n",
       "\t<tr><th scope=row>ATT6</th><td>0.05452177</td><td>0.03936361</td><td>0.04017917</td><td>0.04547098</td><td>0.04995859</td><td>1.00000000</td><td>0.02315537</td><td>0.04078043</td><td>0.06728710</td><td>0.06281401</td><td>0.3338361 </td></tr>\n",
       "\t<tr><th scope=row>ATT7</th><td>0.07122578</td><td>0.01121757</td><td>0.03144363</td><td>0.05860170</td><td>0.07143985</td><td>0.02315537</td><td>1.00000000</td><td>0.07823218</td><td>0.03721329</td><td>0.06658368</td><td>0.3406121 </td></tr>\n",
       "\t<tr><th scope=row>ATT8</th><td>0.05026351</td><td>0.02974010</td><td>0.02602814</td><td>0.02125053</td><td>0.02907593</td><td>0.04078043</td><td>0.07823218</td><td>1.00000000</td><td>0.04319090</td><td>0.03377578</td><td>0.3271048 </td></tr>\n",
       "\t<tr><th scope=row>ATT9</th><td>0.05418733</td><td>0.09424293</td><td>0.06029647</td><td>0.02801731</td><td>0.08373312</td><td>0.06728710</td><td>0.03721329</td><td>0.04319090</td><td>1.00000000</td><td>0.01185411</td><td>0.3495902 </td></tr>\n",
       "\t<tr><th scope=row>ATT10</th><td>0.05468297</td><td>0.07271654</td><td>0.08543024</td><td>0.08264849</td><td>0.08360732</td><td>0.06281401</td><td>0.06658368</td><td>0.03377578</td><td>0.01185411</td><td>1.00000000</td><td>0.3730892 </td></tr>\n",
       "\t<tr><th scope=row>Result</th><td>0.37426265</td><td>0.35125501</td><td>0.36525246</td><td>0.36035416</td><td>0.37427994</td><td>0.33383607</td><td>0.34061210</td><td>0.32710481</td><td>0.34959023</td><td>0.37308920</td><td>1.0000000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & ATT1 & ATT2 & ATT3 & ATT4 & ATT5 & ATT6 & ATT7 & ATT8 & ATT9 & ATT10 & Result\\\\\n",
       "\\hline\n",
       "\tATT1 & 1.00000000 & 0.04610493 & 0.08060515 & 0.05601720 & 0.04422308 & 0.05452177 & 0.07122578 & 0.05026351 & 0.05418733 & 0.05468297 & 0.3742627 \\\\\n",
       "\tATT2 & 0.04610493 & 1.00000000 & 0.06709630 & 0.06157343 & 0.03169298 & 0.03936361 & 0.01121757 & 0.02974010 & 0.09424293 & 0.07271654 & 0.3512550 \\\\\n",
       "\tATT3 & 0.08060515 & 0.06709630 & 1.00000000 & 0.04656806 & 0.06498553 & 0.04017917 & 0.03144363 & 0.02602814 & 0.06029647 & 0.08543024 & 0.3652525 \\\\\n",
       "\tATT4 & 0.05601720 & 0.06157343 & 0.04656806 & 1.00000000 & 0.08003001 & 0.04547098 & 0.05860170 & 0.02125053 & 0.02801731 & 0.08264849 & 0.3603542 \\\\\n",
       "\tATT5 & 0.04422308 & 0.03169298 & 0.06498553 & 0.08003001 & 1.00000000 & 0.04995859 & 0.07143985 & 0.02907593 & 0.08373312 & 0.08360732 & 0.3742799 \\\\\n",
       "\tATT6 & 0.05452177 & 0.03936361 & 0.04017917 & 0.04547098 & 0.04995859 & 1.00000000 & 0.02315537 & 0.04078043 & 0.06728710 & 0.06281401 & 0.3338361 \\\\\n",
       "\tATT7 & 0.07122578 & 0.01121757 & 0.03144363 & 0.05860170 & 0.07143985 & 0.02315537 & 1.00000000 & 0.07823218 & 0.03721329 & 0.06658368 & 0.3406121 \\\\\n",
       "\tATT8 & 0.05026351 & 0.02974010 & 0.02602814 & 0.02125053 & 0.02907593 & 0.04078043 & 0.07823218 & 1.00000000 & 0.04319090 & 0.03377578 & 0.3271048 \\\\\n",
       "\tATT9 & 0.05418733 & 0.09424293 & 0.06029647 & 0.02801731 & 0.08373312 & 0.06728710 & 0.03721329 & 0.04319090 & 1.00000000 & 0.01185411 & 0.3495902 \\\\\n",
       "\tATT10 & 0.05468297 & 0.07271654 & 0.08543024 & 0.08264849 & 0.08360732 & 0.06281401 & 0.06658368 & 0.03377578 & 0.01185411 & 1.00000000 & 0.3730892 \\\\\n",
       "\tResult & 0.37426265 & 0.35125501 & 0.36525246 & 0.36035416 & 0.37427994 & 0.33383607 & 0.34061210 & 0.32710481 & 0.34959023 & 0.37308920 & 1.0000000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | ATT1 | ATT2 | ATT3 | ATT4 | ATT5 | ATT6 | ATT7 | ATT8 | ATT9 | ATT10 | Result | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| ATT1 | 1.00000000 | 0.04610493 | 0.08060515 | 0.05601720 | 0.04422308 | 0.05452177 | 0.07122578 | 0.05026351 | 0.05418733 | 0.05468297 | 0.3742627  | \n",
       "| ATT2 | 0.04610493 | 1.00000000 | 0.06709630 | 0.06157343 | 0.03169298 | 0.03936361 | 0.01121757 | 0.02974010 | 0.09424293 | 0.07271654 | 0.3512550  | \n",
       "| ATT3 | 0.08060515 | 0.06709630 | 1.00000000 | 0.04656806 | 0.06498553 | 0.04017917 | 0.03144363 | 0.02602814 | 0.06029647 | 0.08543024 | 0.3652525  | \n",
       "| ATT4 | 0.05601720 | 0.06157343 | 0.04656806 | 1.00000000 | 0.08003001 | 0.04547098 | 0.05860170 | 0.02125053 | 0.02801731 | 0.08264849 | 0.3603542  | \n",
       "| ATT5 | 0.04422308 | 0.03169298 | 0.06498553 | 0.08003001 | 1.00000000 | 0.04995859 | 0.07143985 | 0.02907593 | 0.08373312 | 0.08360732 | 0.3742799  | \n",
       "| ATT6 | 0.05452177 | 0.03936361 | 0.04017917 | 0.04547098 | 0.04995859 | 1.00000000 | 0.02315537 | 0.04078043 | 0.06728710 | 0.06281401 | 0.3338361  | \n",
       "| ATT7 | 0.07122578 | 0.01121757 | 0.03144363 | 0.05860170 | 0.07143985 | 0.02315537 | 1.00000000 | 0.07823218 | 0.03721329 | 0.06658368 | 0.3406121  | \n",
       "| ATT8 | 0.05026351 | 0.02974010 | 0.02602814 | 0.02125053 | 0.02907593 | 0.04078043 | 0.07823218 | 1.00000000 | 0.04319090 | 0.03377578 | 0.3271048  | \n",
       "| ATT9 | 0.05418733 | 0.09424293 | 0.06029647 | 0.02801731 | 0.08373312 | 0.06728710 | 0.03721329 | 0.04319090 | 1.00000000 | 0.01185411 | 0.3495902  | \n",
       "| ATT10 | 0.05468297 | 0.07271654 | 0.08543024 | 0.08264849 | 0.08360732 | 0.06281401 | 0.06658368 | 0.03377578 | 0.01185411 | 1.00000000 | 0.3730892  | \n",
       "| Result | 0.37426265 | 0.35125501 | 0.36525246 | 0.36035416 | 0.37427994 | 0.33383607 | 0.34061210 | 0.32710481 | 0.34959023 | 0.37308920 | 1.0000000  | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "       ATT1       ATT2       ATT3       ATT4       ATT5       ATT6      \n",
       "ATT1   1.00000000 0.04610493 0.08060515 0.05601720 0.04422308 0.05452177\n",
       "ATT2   0.04610493 1.00000000 0.06709630 0.06157343 0.03169298 0.03936361\n",
       "ATT3   0.08060515 0.06709630 1.00000000 0.04656806 0.06498553 0.04017917\n",
       "ATT4   0.05601720 0.06157343 0.04656806 1.00000000 0.08003001 0.04547098\n",
       "ATT5   0.04422308 0.03169298 0.06498553 0.08003001 1.00000000 0.04995859\n",
       "ATT6   0.05452177 0.03936361 0.04017917 0.04547098 0.04995859 1.00000000\n",
       "ATT7   0.07122578 0.01121757 0.03144363 0.05860170 0.07143985 0.02315537\n",
       "ATT8   0.05026351 0.02974010 0.02602814 0.02125053 0.02907593 0.04078043\n",
       "ATT9   0.05418733 0.09424293 0.06029647 0.02801731 0.08373312 0.06728710\n",
       "ATT10  0.05468297 0.07271654 0.08543024 0.08264849 0.08360732 0.06281401\n",
       "Result 0.37426265 0.35125501 0.36525246 0.36035416 0.37427994 0.33383607\n",
       "       ATT7       ATT8       ATT9       ATT10      Result   \n",
       "ATT1   0.07122578 0.05026351 0.05418733 0.05468297 0.3742627\n",
       "ATT2   0.01121757 0.02974010 0.09424293 0.07271654 0.3512550\n",
       "ATT3   0.03144363 0.02602814 0.06029647 0.08543024 0.3652525\n",
       "ATT4   0.05860170 0.02125053 0.02801731 0.08264849 0.3603542\n",
       "ATT5   0.07143985 0.02907593 0.08373312 0.08360732 0.3742799\n",
       "ATT6   0.02315537 0.04078043 0.06728710 0.06281401 0.3338361\n",
       "ATT7   1.00000000 0.07823218 0.03721329 0.06658368 0.3406121\n",
       "ATT8   0.07823218 1.00000000 0.04319090 0.03377578 0.3271048\n",
       "ATT9   0.03721329 0.04319090 1.00000000 0.01185411 0.3495902\n",
       "ATT10  0.06658368 0.03377578 0.01185411 1.00000000 0.3730892\n",
       "Result 0.34061210 0.32710481 0.34959023 0.37308920 1.0000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor <- cor(as.matrix(na.omit(data)))\n",
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like getting the correlation coefficients does not yield any interesting results, as most of them are between 0 to 0.3. This says that a *weak linear relationship* exists among the variables.\n",
    "\n",
    "We can also create a scatter plot matrix among the attributes to check if we can spot any trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAADFBMVEUAAAAAzQD/AAD///8w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\nAAQARgKAAMBIABCA/wBuwu8dy0JSrwAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(warn=-1)\n",
    "scatterplotMatrix(data,cex=0.2)\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the trends are not exactly linear; however, their relationship with result *somehow* hints a linear structure. Since  linearity is less apparent with this dataset, non-linear methods might perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Regression\n",
    "\n",
    "After exploring the data, it's now time to impute missing observations.\n",
    "\n",
    "**Regression Models Used**\n",
    "\n",
    "Since the dataset is very huge, less flexible models can be used to impute the data. One of the easier yet robust methods for imputing the data is to use a **generalized linear model with regularization.** Both **Lasso** and **Ridge** linear regression are easy to do and interpret.\n",
    "\n",
    "If these models don't do well (i.e. produce a high RMSE) then more complex models can ba used.\n",
    "\n",
    "**Recap: Linear Regression**\n",
    "\n",
    "To recap, a linear regression model takes the form\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + ... + \\beta_nX_n + \\epsilon\n",
    "$$\n",
    "\n",
    "Where $Y$ is the target variable,\n",
    "<br/>$\\beta_0$ is the intercept, \n",
    "<br/>$\\beta_n$ is the slope with respect to feature $X_n$ that describes its relationship to the target variable, and\n",
    "<br/>$\\epsilon$ is the residual or the irreducible error.\n",
    "\n",
    "The goal of linear regression is to estimate the regression coefficients $\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n$ that minimises the residual sum of squares (RSS).\n",
    "\n",
    "$$\n",
    "RSS = \\sum_{i=1}^{n}(y_i - y_\\text{pred})^2\n",
    "$$\n",
    "\n",
    "Which just says that the coefficients must result to predicted $y$ values that are close to the actual $y$ values as possible.\n",
    "\n",
    "\n",
    "**Recap: Regularization**\n",
    "\n",
    "Regularization is used to minimize the error function\n",
    "\n",
    "$$\n",
    "E(\\pmb{w}) = \\sum_{i=1}^{n} (\\pmb{y}_\\text{pred} - \\pmb{y}_i)^2 + \\lambda *\\sum_{j=1}^{m}(||\\pmb{w}_j||^q)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$\\pmb{y}_\\text{pred} = \\pmb{w_0} + \\pmb{w_1}.\\mathbf{x_1} + ... + \\pmb{w_i}.\\mathbf{x_i}$ is the linear regression prediction function.\n",
    "\n",
    "Note: the main parameter of regularization is just the added term \"$\\lambda *||\\pmb{w}||^q$\" with $\\lambda$ as the **shrinkage penalty** that forces the coefficients to shrink to avoid overfitting.\n",
    "\n",
    "There are two types of regularization, depending on the value of **q** above:\n",
    "\n",
    "1. **Ridge Regularization** - also known as L2 penalty, this is when q = 2.  \n",
    "2. **Lasso Regularization** - also known as L1 penalty, this is when q = 1.\n",
    "\n",
    "In addition, Lasso Regularization yields *sparse models*--that is, models that only use a subset of the features, while Ridge Regularization always includes them all.\n",
    "\n",
    "**Training and Testing Dataset**\n",
    "\n",
    "The first step to imputing the missing values is to divide the data into training and testing sets to reduce bias. The most common form of this division is to devote 80% of the dataset to training while the remaining 20% to testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "2397"
      ],
      "text/latex": [
       "2397"
      ],
      "text/markdown": [
       "2397"
      ],
      "text/plain": [
       "[1] 2397"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "600"
      ],
      "text/latex": [
       "600"
      ],
      "text/markdown": [
       "600"
      ],
      "text/plain": [
       "[1] 600"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split data into training and test sets\n",
    "train.len <- nrow(data) * 0.80\n",
    "train.index <- sample(1:nrow(data), train.len)\n",
    "train <- data[train.index,]\n",
    "test <- data[-train.index,]\n",
    "\n",
    "train <- na.omit(train)\n",
    "test <- na.omit(test)\n",
    "\n",
    "nrow(train)\n",
    "nrow(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to declare the error function to get the **root mean squared-error** that is just the square-root version of the RSS. This describes the average deviation of the predicted to the actual target values.\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\sum_{i=1}^{n}(y_i - y_\\text{pred})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get.rmse <- function(predicted.values, actual.values){\n",
    "    p.length <- length(predicted.values)\n",
    "    a.length <- length(actual.values)\n",
    "    \n",
    "    if (p.length == a.length){\n",
    "        # square the errors (difference of predicted and actual)\n",
    "        squared.errors <- (predicted.values - actual.values)^2\n",
    "        \n",
    "        # get the mean\n",
    "        mean.squared.errors <- mean(squared.errors)\n",
    "        \n",
    "        # return the resulting square root\n",
    "        return (sqrt(mean.squared.errors))\n",
    "    }\n",
    "    else{\n",
    "        stop(\"Both numeric vectors must be equal!\")\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling Procedure**\n",
    "\n",
    "Creating the Ridge and Lasso Regression will need several of the tools that were used in previous noteboooks. To create the linear model, the function `glmnet()` is used because it allows regularization. **Cross validation** of the training data is used to find the best lambda via `cv.glmnet()` which is then used in the `predict()` function to predict. There are two predictions involved, one using the testing data to capture the **RMSE** that describes the goodness-of-fit, while another is to actually predict the missing values.\n",
    "\n",
    "## 2.1 Predicting ATT1\n",
    "\n",
    "In this section, we have to create a regularized linear regression to predict the feature **ATT1**, which will become the target variable.\n",
    "\n",
    "The corresponding linear model is\n",
    "\n",
    "**ATT1** = $\\beta_0$ + $\\beta_1$**ATT2** + $\\beta_2$**ATT3** + $\\beta_3$**ATT4** + $\\beta_4$**ATT5** + $\\beta_5$**ATT6** + $\\beta_6$**ATT7** + $\\beta_7$**ATT8** + $\\beta_8$**ATT9** + $\\beta_9$**ATT10** + $\\beta_\\text{10}$**Result**\n",
    "\n",
    "With the coefficients subject to L1 and L2 regularizer.\n",
    "\n",
    "Note that I have included `Result` as a feature for this model because out of all the attributes it is the one with the highest correlation to ATT1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Lambda:  0.1114247\n",
      "Ridge Regression Model for Predicting ATT1:\n",
      "\n",
      "(Intercept)        ATT2        ATT3        ATT4        ATT5        ATT6 \n",
      "  9.1627327  -0.1852387  -0.1694569  -0.1853875  -0.2080058  -0.1802417 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.1733493  -0.1767690  -0.1961661  -0.1943645   2.7348671 \n",
      "\n",
      "Ridge RMSE:  2.316864\n",
      "Ridge prediction:  5.888388\n",
      "\n",
      "==========================================================================\n",
      "Best Lambda:  0.001507629\n",
      "\n",
      "Lasso Regression Model for Predicting ATT1:\n",
      "\n",
      "(Intercept)        ATT2        ATT3        ATT4        ATT5        ATT6 \n",
      " 11.1302836  -0.2539226  -0.2380944  -0.2554540  -0.2766663  -0.2455601 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.2381096  -0.2460284  -0.2615091  -0.2618323   3.4794411 \n",
      "\n",
      "Lasso RMSE:  2.287218\n",
      "Lasso prediction:  6.358676"
     ]
    }
   ],
   "source": [
    "# create matrices but remove its intercept\n",
    "training.mat <- model.matrix(ATT1 ~ ., data = train)[,-1]\n",
    "testing.mat <- model.matrix(ATT1 ~., data = test)[,-1]\n",
    "\n",
    "# put NAs as 0 for prediction\n",
    "predict.data <- data[1:5,]\n",
    "predict.data[is.na(predict.data)] <- 0\n",
    "predict.matrix <- model.matrix(ATT1 ~ ., data = predict.data)[,-1]\n",
    "\n",
    "# create a ridge regression model for attribute 1\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT1, alpha = 0)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT1, alpha = 0)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\nRidge Regression Model for Predicting ATT1:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att1.ridge.rmse <- get.rmse(predicted, test$ATT1)\n",
    "cat(\"\\nRidge RMSE: \", att1.ridge.rmse)\n",
    "\n",
    "att1.ridge.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[2]\n",
    "cat(\"\\nRidge prediction: \", att1.ridge.pred)\n",
    "\n",
    "cat(\"\\n\\n==========================================================================\")\n",
    "\n",
    "# create a lasso regression model for attribute 1\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT1, alpha = 1)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT1, alpha = 1)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\n\\nLasso Regression Model for Predicting ATT1:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att1.lasso.rmse <- get.rmse(predicted, test$ATT1)\n",
    "cat(\"\\nLasso RMSE: \", att1.lasso.rmse)\n",
    "att1.lasso.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[2]\n",
    "cat(\"\\nLasso prediction: \", att1.lasso.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the models**\n",
    "- For **Ridge Regularization**, the best lambda is at **0.11**, which means that the coefficients did not shrink that much. The RMSE of the ridge model is at 2.32, which means on average the predicted value of ATT1 is just +/- 2 from the actual value, which is a good fit. The rounded predicted value is **6**.\n",
    "<br/><br/>\n",
    "- For **Lasso Regularization**, the best lambda is even lower at **0.002**, which is why the model included all of the features. The shrinkage penalty is small, so the model was not penalized harshly. The RMSE of the lasso model is at 2.28, which suggests a good fit. The rounded predicted value is **6**.\n",
    "\n",
    "In this case since the Lasso produced a slightly lower RMSE we get the imputed value from the Lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data$ATT1[is.na(data$ATT1)] <- round(att1.lasso.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Predicting ATT3\n",
    "\n",
    "In this section, we have to create a regularized linear regression to predict the feature **ATT3**, which will become the target variable.\n",
    "\n",
    "The corresponding linear model is\n",
    "\n",
    "**ATT3** = $\\beta_0$ + $\\beta_1$**ATT1** + $\\beta_2$**ATT2** + $\\beta_3$**ATT4** + $\\beta_4$**ATT5** + $\\beta_5$**ATT6** + $\\beta_6$**ATT7** + $\\beta_7$**ATT8** + $\\beta_8$**ATT9** + $\\beta_9$**ATT10** + $\\beta_\\text{10}$**Result**\n",
    "\n",
    "With the coefficients subject to L1 and L2 regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Lambda:  0.111552\n",
      "Ridge Regression Model for Predicting ATT3:\n",
      "\n",
      "(Intercept)        ATT1        ATT2        ATT4        ATT5        ATT6 \n",
      "  8.9081773  -0.1737813  -0.1567280  -0.1981111  -0.1696120  -0.1956056 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.1937489  -0.1944582  -0.1599971  -0.1547668   2.6596837 \n",
      "\n",
      "Ridge RMSE:  2.249016\n",
      "Ridge prediction:  6.964361\n",
      "\n",
      "==========================================================================\n",
      "Best Lambda:  0.001656511\n",
      "Lasso Regression Model for Predicting ATT3:\n",
      "\n",
      "(Intercept)        ATT1        ATT2        ATT4        ATT5        ATT6 \n",
      " 10.8685129  -0.2454073  -0.2254344  -0.2664504  -0.2387652  -0.2591901 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.2564524  -0.2616092  -0.2257816  -0.2226459   3.3976473 \n",
      "\n",
      "Lasso RMSE:  2.210324\n",
      "Lasso prediction:  7.677994"
     ]
    }
   ],
   "source": [
    "# create matrices but remove its intercept\n",
    "training.mat <- model.matrix(ATT3 ~ ., data = train)[,-1]\n",
    "testing.mat <- model.matrix(ATT3 ~., data = test)[,-1]\n",
    "\n",
    "# put NAs as 0 for prediction\n",
    "predict.data <- data[1:5,]\n",
    "predict.data[is.na(predict.data)] <- 0\n",
    "predict.matrix <- model.matrix(ATT3 ~ ., data = predict.data)[,-1]\n",
    "\n",
    "# create a ridge regression model for attribute 3\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT3, alpha = 0)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT3, alpha = 0)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\nRidge Regression Model for Predicting ATT3:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att3.ridge.rmse <- get.rmse(predicted, test$ATT3)\n",
    "cat(\"\\nRidge RMSE: \", att3.ridge.rmse)\n",
    "\n",
    "att3.ridge.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[3]\n",
    "cat(\"\\nRidge prediction: \", att3.ridge.pred)\n",
    "\n",
    "cat(\"\\n\\n==========================================================================\")\n",
    "\n",
    "# create a lasso regression model for attribute 3b\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT3, alpha = 1)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT3, alpha = 1)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\nLasso Regression Model for Predicting ATT3:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att3.lasso.rmse <- get.rmse(predicted, test$ATT3)\n",
    "cat(\"\\nLasso RMSE: \", att3.lasso.rmse)\n",
    "att3.lasso.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[3]\n",
    "cat(\"\\nLasso prediction: \", att3.lasso.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the models**\n",
    "- For **ridge regularization**, the best lambda is at **0.11**, which means that the coefficients did not shrink that much. The RMSE of the ridge model is at 2.2, which means on average the predicted value of ATT3 is just +/- 2 from the actual value, which is a good fit. The rounded predicted value is **7**.\n",
    "<br/><br/>\n",
    "- For **lasso regularization**, the best lambda is even lower at **0.002**, which is why the model included all of the features. The shrinkage penalty is small, so the model was not penalized harshly. The RMSE of the lasso model is at 2.2, which suggests a good fit. The rounded predicted value is **8**.\n",
    "\n",
    "Since the Lasso model is slightly better, assign the Lasso prediction to the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data$ATT3[is.na(data$ATT3)] <- round(att3.lasso.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Predicting ATT4\n",
    "\n",
    "In this section, we have to create a regularized linear regression to predict the feature **ATT4**, which will become the target variable.\n",
    "\n",
    "The corresponding linear model is\n",
    "\n",
    "**ATT4** = $\\beta_0$ + $\\beta_1$**ATT1** + $\\beta_2$**ATT2** + $\\beta_3$**ATT3** + $\\beta_4$**ATT5** + $\\beta_5$**ATT6** + $\\beta_6$**ATT7** + $\\beta_7$**ATT8** + $\\beta_8$**ATT9** + $\\beta_9$**ATT10** + $\\beta_\\text{10}$**Result**\n",
    "\n",
    "With the coefficients subject to L1 and L2 regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Lambda:  0.1096343\n",
      "Ridge Regression Model for Predicting ATT4:\n",
      "\n",
      "(Intercept)        ATT1        ATT2        ATT3        ATT5        ATT6 \n",
      "  8.9336669  -0.1849580  -0.1729899  -0.1927152  -0.1615991  -0.1689068 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.1621612  -0.2081055  -0.1942829  -0.1574861   2.6543911 \n",
      "\n",
      "Ridge RMSE:  2.299298\n",
      "Ridge prediction:  4.516396\n",
      "\n",
      "==========================================================================\n",
      "Best Lambda:  0.001483405\n",
      "Lasso Regression Model for Predicting ATT4:\n",
      "\n",
      "(Intercept)        ATT1        ATT2        ATT3        ATT5        ATT6 \n",
      " 10.8722113  -0.2556558  -0.2404377  -0.2586604  -0.2307082  -0.2331553 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.2257977  -0.2741895  -0.2581296  -0.2249593   3.3861746 \n",
      "\n",
      "Lasso RMSE:  2.262615\n",
      "Lasso prediction:  4.621874"
     ]
    }
   ],
   "source": [
    "# create matrices but remove its intercept\n",
    "training.mat <- model.matrix(ATT4 ~ ., data = train)[,-1]\n",
    "testing.mat <- model.matrix(ATT4 ~., data = test)[,-1]\n",
    "\n",
    "# put NAs as 0 for prediction\n",
    "predict.data <- data[1:5,]\n",
    "predict.data[is.na(predict.data)] <- 0\n",
    "predict.matrix <- model.matrix(ATT4 ~ ., data = predict.data)[,-1]\n",
    "\n",
    "# create a ridge regression model for attribute 3\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT4, alpha = 0)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT4, alpha = 0)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\nRidge Regression Model for Predicting ATT4:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att4.ridge.rmse <- get.rmse(predicted, test$ATT4)\n",
    "cat(\"\\nRidge RMSE: \", att4.ridge.rmse)\n",
    "\n",
    "att4.ridge.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[4]\n",
    "cat(\"\\nRidge prediction: \", att4.ridge.pred)\n",
    "\n",
    "cat(\"\\n\\n==========================================================================\")\n",
    "\n",
    "# create a lasso regression model for attribute 3b\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT4, alpha = 1)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT4, alpha = 1)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\nLasso Regression Model for Predicting ATT4:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att4.lasso.rmse <- get.rmse(predicted, test$ATT4)\n",
    "cat(\"\\nLasso RMSE: \", att4.lasso.rmse)\n",
    "att4.lasso.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[4]\n",
    "cat(\"\\nLasso prediction: \", att4.lasso.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the models**\n",
    "- For **ridge regularization**, the best lambda is at **0.1**, which means that the coefficients did not shrink that much. The RMSE of the ridge model is at 2.3, which means on average the predicted value of ATT3 is just +/- 2 from the actual value, which is a good fit. The rounded predicted value is **5**.\n",
    "<br/><br/>\n",
    "- For **lasso regularization**, the best lambda is even lower at **0.001**, which is why the model included all of the features. The shrinkage penalty is small, so the model was not penalized harshly. The RMSE of the lasso model is also at 2.3, which suggests a good fit. The rounded predicted value is also **5**.\n",
    "\n",
    "Since both models predicted the same rounded value then either models can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data$ATT4[is.na(data$ATT4)] <- round(att4.lasso.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Predicting ATT5\n",
    "\n",
    "In this section, we have to create a regularized linear regression to predict the feature **ATT5**, which will become the target variable.\n",
    "\n",
    "The corresponding linear model is\n",
    "\n",
    "**ATT5** = $\\beta_0$ + $\\beta_1$**ATT1** + $\\beta_2$**ATT2** + $\\beta_3$**ATT3** + $\\beta_4$**ATT4** + $\\beta_5$**ATT6** + $\\beta_6$**ATT7** + $\\beta_7$**ATT8** + $\\beta_8$**ATT9** + $\\beta_9$**ATT10** + $\\beta_\\text{10}$**Result**\n",
    "\n",
    "With the coefficients subject to L1 and L2 regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Lambda:  0.1046193\n",
      "Ridge Regression Model for Predicting ATT5:\n",
      "\n",
      "(Intercept)        ATT1        ATT2        ATT3        ATT4        ATT6 \n",
      "  8.9330610  -0.2070363  -0.2066924  -0.1649959  -0.1616630  -0.1745022 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.1690380  -0.1968501  -0.1625602  -0.1664461   2.6269584 \n",
      "\n",
      "Ridge RMSE:  2.209214\n",
      "Ridge prediction:  5.300616\n",
      "\n",
      "==========================================================================\n",
      "Best Lambda:  0.001415549\n",
      "Lasso Regression Model for Predicting ATT5:\n",
      "\n",
      "(Intercept)        ATT1        ATT2        ATT3        ATT4        ATT6 \n",
      " 10.8053606  -0.2743154  -0.2704385  -0.2296962  -0.2285861  -0.2362575 \n",
      "       ATT7        ATT8        ATT9       ATT10      Result \n",
      " -0.2301564  -0.2610525  -0.2254528  -0.2311064   3.3349704 \n",
      "\n",
      "Lasso RMSE:  2.898478\n",
      "Lasso prediction:  5.475878"
     ]
    }
   ],
   "source": [
    "# create matrices but remove its intercept\n",
    "training.mat <- model.matrix(ATT5 ~ ., data = train)[,-1]\n",
    "testing.mat <- model.matrix(ATT5 ~., data = test)[,-1]\n",
    "\n",
    "# put NAs as 0 for prediction\n",
    "predict.data <- data[1:5,]\n",
    "predict.data[is.na(predict.data)] <- 0\n",
    "predict.matrix <- model.matrix(ATT5 ~ ., data = predict.data)[,-1]\n",
    "\n",
    "# create a ridge regression model for attribute 5\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT5, alpha = 0)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT5, alpha = 0)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\nRidge Regression Model for Predicting ATT5:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att5.ridge.rmse <- get.rmse(predicted, test$ATT5)\n",
    "cat(\"\\nRidge RMSE: \", att5.ridge.rmse)\n",
    "\n",
    "att5.ridge.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[3]\n",
    "cat(\"\\nRidge prediction: \", att5.ridge.pred)\n",
    "\n",
    "cat(\"\\n\\n==========================================================================\")\n",
    "\n",
    "# create a lasso regression model for attribute 5\n",
    "cv.model <- cv.glmnet(training.mat, train$ATT5, alpha = 1)\n",
    "best.lambda <- cv.model$lambda.min\n",
    "leastsq.mod <- glmnet(training.mat, train$ATT5, alpha = 1)\n",
    "model.coef <- predict(leastsq.mod, type=\"coefficients\", s = best.lambda)[1:ncol(train),]\n",
    "cat(\"\\nBest Lambda: \", best.lambda)\n",
    "cat(\"\\nLasso Regression Model for Predicting ATT5:\\n\\n\")\n",
    "print(model.coef)\n",
    "\n",
    "predicted <- predict(leastsq.mod, s = best.lambda, newx=testing.mat, type = \"response\")\n",
    "att5.lasso.rmse <- get.rmse(predicted, test$ATT4)\n",
    "cat(\"\\nLasso RMSE: \", att5.lasso.rmse)\n",
    "att5.lasso.pred <- predict(leastsq.mod, s = best.lambda, newx=predict.matrix, type = \"response\")[3]\n",
    "cat(\"\\nLasso prediction: \", att5.lasso.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the models**\n",
    "- For **ridge regularization**, the best lambda is at **0.10**, which means that the coefficients did not shrink that much. The RMSE of the ridge model is at 2.2, which means on average the predicted value of ATT3 is just +/- 2 from the actual value, which is a good fit. The rounded predicted value is **5**.\n",
    "<br/><br/>\n",
    "- For **lasso regularization**, the best lambda is even lower at **0.001**, which is why the model included all of the features. The shrinkage penalty is small, so the model was not penalized harshly. The RMSE of the lasso model is at 2.89 which is higher than the ridge but still suggests a good fit. The rounded predicted value is also **5**.\n",
    "\n",
    "Since both models predicted the same rounded value then either models can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data$ATT5[is.na(data$ATT5)] <- round(att5.ridge.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Summary\n",
    "\n",
    "In general, regularized linear regression models worked well in imputing the values. The effect of regularization, however, is not as prominent, as the best lambda is always a small value. Nevertheless, the RMSE of the test data against the model is just around 2.3-2.8, which suggests a good enough fit. \n",
    "\n",
    "# 3. Prostrate Cancer Classification\n",
    "\n",
    "Since we're done imputing the missing values, it's time to create a good classifier for prostrate cancer. The aim of this part of the notebook is to create a model with the highest possible **accuracy**, or the proportion of the correct predictions over the total number of observations.\n",
    "\n",
    "**Classifiers Used**\n",
    "\n",
    "The following classification models are used in this notebook:\n",
    "\n",
    "- **Naive Bayes Classifier** \n",
    "<br/>A *generative* probabilistic model that computes the probability of each observation belonging to a class by using Bayes' Theorem.\n",
    "\n",
    "\n",
    "- **Decision Trees and Random Forest** \n",
    "<br/>A *tree-based* method that splits the predictor space into several simple regions based on a set of splitting rules.\n",
    "\n",
    "\n",
    "- **SVMs** \n",
    "<br/>A method that evolved from a classifier called *maximal-margin classifier* and *support vector classifier* and is known to be one of the most flexible classifiers.\n",
    "\n",
    "\n",
    "- **Neural Networks**\n",
    "<br/>A model that mimics how a human brain learns by generating a collection of neurons that work together to solve a specific problem.\n",
    "\n",
    "Each models will be described in depth in the succeeding sections.\n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "First, let's preprocess the data. I have decided to turn the result into a factor, since it is a categorical variable. As for the attributes, since according to the data description they are rounded <u>numeric</u> measurements I shall leave them as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t3000 obs. of  11 variables:\n",
      " $ ATT1  : num  1 6 10 3 3 2 1 8 2 1 ...\n",
      " $ ATT2  : int  4 8 7 4 5 7 1 5 9 1 ...\n",
      " $ ATT3  : num  1 9 8 3 2 3 3 2 2 1 ...\n",
      " $ ATT4  : num  4 1 7 5 1 2 5 6 1 8 ...\n",
      " $ ATT5  : num  3 1 5 2 6 1 9 2 4 9 ...\n",
      " $ ATT6  : int  7 1 5 8 5 4 3 7 6 2 ...\n",
      " $ ATT7  : int  1 1 2 4 3 10 7 3 8 2 ...\n",
      " $ ATT8  : int  2 5 7 6 1 3 1 6 8 5 ...\n",
      " $ ATT9  : int  6 6 1 7 7 9 10 7 4 1 ...\n",
      " $ ATT10 : int  8 1 1 2 1 5 7 3 3 3 ...\n",
      " $ Result: Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 1 2 3 2 1 2 2 2 2 1 ...\n"
     ]
    }
   ],
   "source": [
    "data$Result <- factor(data$Result, levels = c(0:3))\n",
    "str(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training and Testing Dataset**\n",
    "\n",
    "The dataset is again split into two groups, with 80% of the data used for training and 20% for testing the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/latex": [],
      "text/markdown": [],
      "text/plain": [
       "character(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "2400"
      ],
      "text/latex": [
       "2400"
      ],
      "text/markdown": [
       "2400"
      ],
      "text/plain": [
       "[1] 2400"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "600"
      ],
      "text/latex": [
       "600"
      ],
      "text/markdown": [
       "600"
      ],
      "text/plain": [
       "[1] 600"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t2400 obs. of  11 variables:\n",
      " $ ATT1  : num  9 7 4 9 5 9 2 4 1 1 ...\n",
      " $ ATT2  : int  4 4 3 4 7 9 7 4 3 8 ...\n",
      " $ ATT3  : num  2 1 6 2 2 6 3 3 8 9 ...\n",
      " $ ATT4  : num  5 6 7 6 3 1 2 10 5 5 ...\n",
      " $ ATT5  : num  7 2 3 1 7 7 10 2 3 2 ...\n",
      " $ ATT6  : int  6 5 8 9 5 8 6 2 4 1 ...\n",
      " $ ATT7  : int  5 4 7 5 4 2 7 1 7 7 ...\n",
      " $ ATT8  : int  4 7 5 9 7 6 9 1 9 2 ...\n",
      " $ ATT9  : int  4 2 7 2 5 3 5 1 3 4 ...\n",
      " $ ATT10 : int  5 2 2 9 6 1 4 4 7 9 ...\n",
      " $ Result: Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 3 1 3 3 3 3 3 1 2 2 ...\n",
      "'data.frame':\t600 obs. of  11 variables:\n",
      " $ ATT1  : num  6 7 2 1 4 3 3 6 3 5 ...\n",
      " $ ATT2  : int  2 10 2 2 3 3 8 6 10 9 ...\n",
      " $ ATT3  : num  2 7 5 10 2 2 2 9 5 5 ...\n",
      " $ ATT4  : num  1 10 7 8 2 1 8 6 3 2 ...\n",
      " $ ATT5  : num  1 2 5 3 1 3 6 1 6 9 ...\n",
      " $ ATT6  : int  2 6 1 2 10 1 5 8 9 10 ...\n",
      " $ ATT7  : int  4 9 2 1 1 5 6 7 4 5 ...\n",
      " $ ATT8  : int  8 6 2 8 6 1 2 3 2 4 ...\n",
      " $ ATT9  : int  6 4 3 2 4 4 3 7 1 10 ...\n",
      " $ ATT10 : int  10 2 4 7 4 5 1 9 2 3 ...\n",
      " $ Result: Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 2 4 1 2 1 1 2 4 2 4 ...\n"
     ]
    }
   ],
   "source": [
    "# did we have any problems converting everything into factor?\n",
    "data[is.na(data)]\n",
    "\n",
    "set.seed(1234)\n",
    "# split into train and test sets again\n",
    "train.len <- nrow(data) * 0.80\n",
    "train.index <- sample(1:nrow(data), train.len)\n",
    "train <- data[train.index,]\n",
    "test <- data[-train.index,]\n",
    "\n",
    "train <- na.omit(train)\n",
    "test <- na.omit(test)\n",
    "\n",
    "# was there any NA rows?\n",
    "nrow(train)\n",
    "nrow(test)\n",
    "\n",
    "str(train)\n",
    "str(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Naive Bayes Classifier\n",
    "\n",
    "Note: Materials used in this section are based from FIT 5201: Data Analysis Algorithms lecture notes.\n",
    "\n",
    "**Naive Bayes Classifier** is a classifier that generates a probability distribution for each class and then calculates the probability of an observation to belong in a class using the Bayes' Theorem.\n",
    "\n",
    "The main assumption of this model is that **all features are independent from each other**. Since all of the features exhibit low correlation coefficient, it is possible that the features may be independent.\n",
    "\n",
    "Generating the probability distribution of each class $\\mathcal{C}_k$ requires the following:\n",
    "- prior probabilities $\\pmb{p(\\mathcal{C}_k)}$ which is just the fraction of observations that belong to each class $\\mathcal{C}_k$ in the whole dataset\n",
    "- class means $\\mu_k$\n",
    "- class covariance matrices $\\mathbf{S}_k$\n",
    "- shared covariance matrix$\\Sigma$\n",
    "\n",
    "The conditional probability $p(x | \\mathcal{C}_k)$ is derived from the distribution density function using the $\\mu_k$ and $\\Sigma$. \n",
    "\n",
    "Once you have calculated $p(\\mathcal{C}_k)$ and $p(x|\\mathcal{C}_k)$, Bayes theorem can now be used to calculate the posterior $p(\\mathcal{C}_k|x)$ probabilities for each class. \n",
    "\n",
    "$$\n",
    "p(\\mathcal{C}_k|x) = \\frac{p\\mathcal{C}_k . p(x|\\mathcal{C}_k)}{p(x)}\n",
    "$$\n",
    "\n",
    "Whichever has the bigger posterior probability is the chosen class for the data.\n",
    "\n",
    "To create the classifier, the function `train()` from the package `klaR` is used, with the parameter `'nb'` indicating that we aim for a Naive Bayes Classifier. **Cross-validation** is also performed in order to determine the accuracy of the model against the training data. The one with the best **accuracy** is picked as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Naive Bayes \n",
       "\n",
       "2400 samples\n",
       "  10 predictor\n",
       "   4 classes: '0', '1', '2', '3' \n",
       "\n",
       "No pre-processing\n",
       "Resampling: Cross-Validated (10 fold) \n",
       "Summary of sample sizes: 2161, 2160, 2160, 2160, 2160, 2160, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  usekernel  Accuracy   Kappa    \n",
       "  FALSE      0.7820878  0.6748585\n",
       "   TRUE      0.7808309  0.6711068\n",
       "\n",
       "Tuning parameter 'fL' was held constant at a value of 0\n",
       "Tuning\n",
       " parameter 'adjust' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using  the largest value.\n",
       "The final values used for the model were fL = 0, usekernel = FALSE and adjust\n",
       " = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x <- train[,-ncol(train)]\n",
    "y <- train[,ncol(train)]\n",
    "options(warn=-1)\n",
    "\n",
    "# create a Naive Bayes model by cross-validating the training set\n",
    "naiveBayes <- train(x,y,'nb',trControl=trainControl(method='cv',number=10))\n",
    "naiveBayes\n",
    "\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the model**\n",
    "\n",
    "The average training accuracy of the cross-validated Naive Bayes model is **78%**, which is good enough. There is a lesser chance of overfitting as the model did not have a perfect accuracy for the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Train Accuracy:  0.7941667\n",
      "Naive Bayes Test Accuracy:  0.7733333"
     ]
    }
   ],
   "source": [
    "options(warn=-1)\n",
    "naiveBayes.trainPred <- predict(naiveBayes$finalModel,train[,-ncol(train)])$class\n",
    "naiveBayes.trainAccuracy <- confusionMatrix(naiveBayes.trainPred, train[,ncol(train)])$overall['Accuracy']\n",
    "cat(\"\\nNaive Bayes Train Accuracy: \", naiveBayes.trainAccuracy)\n",
    "\n",
    "naiveBayes.pred <- predict(naiveBayes$finalModel,test[,-ncol(test)])$class\n",
    "naiveBayes.testAccuracy <- confusionMatrix(naiveBayes.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "cat(\"\\nNaive Bayes Test Accuracy: \", naiveBayes.testAccuracy)\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best considered model from cross validation produced an accuracy of **79%**, which is a good fit. When tested against the test data, it produced an accuracy of **77%** which is robust enough. It is impressive considering Naive Bayes Classifier is one of the simplest models to create and computationally inexpensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Decision Trees\n",
    "\n",
    "**Decision Trees** are one of the easiest models to interpret due to its structure. The tree is grown by recursively splitting the predictor space into two, based on a criterion which can either be one the following: *classification error rate*, the *Gini index*, or the *entropy*. Decision trees aim to produce a tree that maximizes **node purity** -- that is, the probability that a node contains all observations from a single class. In the three criteria used, both the *Gini index* and *entropy* takes into account node purity.\n",
    "\n",
    "The reason why I chose to include decision trees is because there is no clear linear relationship that exists in the dataset and because of the resulting models are easier to interpret.\n",
    "\n",
    "### 3.2.1 Basic Decision Tree\n",
    "\n",
    "To create a classification tree, the method `tree()` is used. The function `plot()` and `text()` are then called to display the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\ngAOKBDigSIADigQ4+A9fygKtprP/GQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a cross validated decision tree\n",
    "decisionTree <- tree(Result ~ ., data = train)\n",
    "plot(decisionTree)\n",
    "text(decisionTree, pretty = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Decision Tree Train Accuracy:  0.4979167\n",
      "\n",
      " Decision Tree Test Accuracy:  0.48"
     ]
    }
   ],
   "source": [
    "dt.trainPred <- predict(decisionTree, newdata = train, type=\"class\")\n",
    "dt.trainAccuracy <- confusionMatrix(dt.trainPred, train[,ncol(train)])$overall['Accuracy']\n",
    "cat(\"\\n Decision Tree Train Accuracy: \", dt.trainAccuracy)\n",
    "\n",
    "dt.pred <- predict(decisionTree, newdata = test, type=\"class\")\n",
    "dt.accuracy <- confusionMatrix(dt.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "cat(\"\\n\\n Decision Tree Test Accuracy: \", dt.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the model**\n",
    "- Based on the resulting classification tree, **ATT1** is considered the most important attribute. The data then proceeds to the left if its ATT1 is either **1, 2, 3, 4**, otherwise the data goes to the right. This is how a tree is read, from top down.\n",
    "- There were two terminal nodes that yield that same splitting value, **ATT10** which produces 0 at both splits, and **ATT3** which produces 2 at both splits. The split was performed because it leads to an <u>increase in node purity.</u>\n",
    "- There are four classes in the dataset, but the <u>class **3** is missing from the tree</u>. This might be because class 3 is underrepresented in the dataset.\n",
    "- Because only three classes are classified in the tree, this yields a drop in accuracy and is <u>not considered a very good model.</u>\n",
    "- Computing the accuracy, the model indeed underperforms--it only has **49.7%** train accuracy and **48%** test accuracy.\n",
    "\n",
    "### 3.2.2 Pruned Decision Tree\n",
    "\n",
    "Since the basic decision tree is more likely to overfit the training data (but in this case it *underfits*), **pruning** is performed to decrease the number of splits and decrease variance (with a slight increase in bias). Pruning is performed by using a *threshold*; if the splitting criterion passes the threshold then the splitting is stopped. This produces a *subtree* of the previous decision tree (ie the number of terminal nodes is smaller).\n",
    "\n",
    "The method used to perform pruning is the `cv.tree()` method, which uses cross validation to choose how complex the tree is. Since we're doing classification, the function used is `prune.misclass`, which checks for the number of misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$size\n",
       "[1] 9 6 5 4 3 2 1\n",
       "\n",
       "$dev\n",
       "[1] 1307 1298 1291 1253 1296 1378 1575\n",
       "\n",
       "$k\n",
       "[1] -Inf    0    2   23   55   82  166\n",
       "\n",
       "$method\n",
       "[1] \"misclass\"\n",
       "\n",
       "attr(,\"class\")\n",
       "[1] \"prune\"         \"tree.sequence\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "4"
      ],
      "text/latex": [
       "4"
      ],
      "text/markdown": [
       "4"
      ],
      "text/plain": [
       "[1] 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nAggJEEBIgABCAgT8DzmegElCsXM1AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use cross-validation to determine the optimal level of tree complexity\n",
    "cv.decisionTree <- cv.tree(decisionTree, FUN = prune.misclass)\n",
    "cv.decisionTree\n",
    "\n",
    "# determine the optimal number of terminal nodes\n",
    "tree.min <- which.min(cv.decisionTree$dev)\n",
    "bestSubTree <- cv.decisionTree$size[tree.min]\n",
    "bestSubTree\n",
    "\n",
    "plot(cv.decisionTree$size, cv.decisionTree$dev, type = \"b\")\n",
    "points(cv.decisionTree$size[tree.min], cv.decisionTree$dev[tree.min], col = \"red\", cex = 2, pch = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the optimal tree is the one with **4 terminal nodes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Classification tree:\n",
       "snip.tree(tree = decisionTree, nodes = c(4L, 7L, 6L, 5L))\n",
       "Variables actually used in tree construction:\n",
       "[1] \"ATT1\"  \"ATT5\"  \"ATT10\"\n",
       "Number of terminal nodes:  4 \n",
       "Residual mean deviance:  2.245 = 5379 / 2396 \n",
       "Misclassification error rate: 0.5125 = 1230 / 2400 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nJyH3AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use prune.tree function\n",
    "prune.decisionTree <- prune.misclass(decisionTree, best = bestSubTree)\n",
    "summary(prune.decisionTree)\n",
    "\n",
    "#plot the tree\n",
    "plot(prune.decisionTree)\n",
    "text(prune.decisionTree, pretty = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pruned Decision Tree Train Accuracy:  0.4875\n",
      "\n",
      "Pruned Decision Tree Test Accuracy:  0.4633333"
     ]
    }
   ],
   "source": [
    "prunedDT.trainPred <- predict(prune.decisionTree, newdata = train, type=\"class\")\n",
    "pruneDT.trainAccuracy <- confusionMatrix(prunedDT.trainPred, train[,ncol(train)])$overall['Accuracy']\n",
    "cat(\"\\nPruned Decision Tree Train Accuracy: \", pruneDT.trainAccuracy)\n",
    "\n",
    "prunedDT.pred <- predict(prune.decisionTree, newdata = test, type=\"class\")\n",
    "pruneDT.accuracy <- confusionMatrix(prunedDT.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "cat(\"\\n\\nPruned Decision Tree Test Accuracy: \", pruneDT.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the Model**\n",
    "- The model produced a subtree of the previous decision tree, which only uses the features **ATT1, ATT5, and ATT10**. \n",
    "- The model still has <u>not included</u> class *3*, which will yield a high error rate.\n",
    "- The resulting train accuracy of the pruned decision tree is **49%**, while for the test accuracy it is only **46%**, which is a drop in performance from using the whole tree.\n",
    "- The resulting decision trees <u>underfits</u> the data, because it does not include all classes in the terminal nodes and still has a low training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Random Forest\n",
    "\n",
    "The concept of **Random Forest** is used to significantly improve the performance of decision trees. Since the earlier trees underfit the data, creating multiple subtrees which will predict based on a consensus can result in performance improvement. In general, performing random forest is a way to decrease **overfitting**, however, in the case of this example the tree clearly underfits because of its very low training accuracy. \n",
    "\n",
    "**Creating a random forest**\n",
    "\n",
    "A collection of trees are created from the bootstrapped training data, and is splitted according to a subset of *m* features (usually $\\sqrt{p}$, where $p$ is the number of features). This produces a tree that uses a *subset* of the training data and a *subset* of predictors, thereby ensuring that the result is not overfitted. However, since this results in a number of subtrees for classification the model is harder to interpret than a normal decision tree.\n",
    "\n",
    "To perform random forest, the method `randomForest()` is used from the `randomForest` package. Random forest is performed with differing number of bootstraped subtrees indicated in the `ntree` parameter: 10, 50, 100, 500, and 1000. Each resulting randomForest is then analyzed according to their test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?randomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K <- c(10, 50, 100, 500, 1000)\n",
    "\n",
    "# use Random Forest\n",
    "rf.accuracies <- as.data.frame(matrix(NA,nrow=length(K), ncol=3))\n",
    "names(rf.accuracies) <- c('k', 'train.accuracy', 'test.accuracy')\n",
    "i <- 1\n",
    "\n",
    "mtry.val <- sqrt(10)\n",
    "i <- 1\n",
    "for (k in K){\n",
    "    randomForest <- randomForest(Result ~ ., data = train, ntree = k, mtry = mtry.val, importance = TRUE)\n",
    "    randomForest.pred <- predict(randomForest, newdata = test, type = \"class\")\n",
    "    randomForest.accuracy <- confusionMatrix(randomForest.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "    \n",
    "    randomForest.trainPred <- predict(randomForest, newdata = train, type = \"class\")\n",
    "    randomForest.trainAccuracy <- confusionMatrix(randomForest.trainPred, train[,ncol(train)])$overall['Accuracy']\n",
    "\n",
    "\n",
    "    rf.accuracies[i, 'k'] <- k\n",
    "    rf.accuracies[i, 'train.accuracy'] <- randomForest.trainAccuracy\n",
    "    rf.accuracies[i, 'test.accuracy'] <- randomForest.accuracy\n",
    "    i <- i + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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\nFYaQGKbCEBLDVBhCYpgKQ0gMU2H+B6fdbh/XitTtAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ggplot(data=rf.accuracies,aes(x=k, y=train.accuracy)) + geom_line() + geom_point(shape=21, fill='red')  +\n",
    "    ggtitle('Train Accuracy vs. K (number of trees)') + theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAABNTU1oaGh8fHyM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\nCgwhMUyBISSGKTCExDAFhpAYpsD8D3XzVkTes3bbAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ggplot(data=rf.accuracies,aes(x=k, y=test.accuracy)) + geom_line() + geom_point(shape=21, fill='red')  +\n",
    "    ggtitle('Test Accuracy vs. K (number of trees)') + theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Random Forest Models\n",
    "\n",
    "The table below shows the overall performance of doing random forest in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracies"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>k</th><th scope=col>train.accuracy</th><th scope=col>test.accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>  10     </td><td>0.99375  </td><td>0.6833333</td></tr>\n",
       "\t<tr><td>  50     </td><td>1.00000  </td><td>0.7616667</td></tr>\n",
       "\t<tr><td> 100     </td><td>1.00000  </td><td>0.7783333</td></tr>\n",
       "\t<tr><td> 500     </td><td>1.00000  </td><td>0.7900000</td></tr>\n",
       "\t<tr><td>1000     </td><td>1.00000  </td><td>0.7966667</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " k & train.accuracy & test.accuracy\\\\\n",
       "\\hline\n",
       "\t   10      & 0.99375   & 0.6833333\\\\\n",
       "\t   50      & 1.00000   & 0.7616667\\\\\n",
       "\t  100      & 1.00000   & 0.7783333\\\\\n",
       "\t  500      & 1.00000   & 0.7900000\\\\\n",
       "\t 1000      & 1.00000   & 0.7966667\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "k | train.accuracy | test.accuracy | \n",
       "|---|---|---|---|---|\n",
       "|   10      | 0.99375   | 0.6833333 | \n",
       "|   50      | 1.00000   | 0.7616667 | \n",
       "|  100      | 1.00000   | 0.7783333 | \n",
       "|  500      | 1.00000   | 0.7900000 | \n",
       "| 1000      | 1.00000   | 0.7966667 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  k    train.accuracy test.accuracy\n",
       "1   10 0.99375        0.6833333    \n",
       "2   50 1.00000        0.7616667    \n",
       "3  100 1.00000        0.7783333    \n",
       "4  500 1.00000        0.7900000    \n",
       "5 1000 1.00000        0.7966667    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best K: 1000"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>k</th><th scope=col>train.accuracy</th><th scope=col>test.accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>5</th><td>1000     </td><td>1        </td><td>0.7966667</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & k & train.accuracy & test.accuracy\\\\\n",
       "\\hline\n",
       "\t5 & 1000      & 1         & 0.7966667\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | k | train.accuracy | test.accuracy | \n",
       "|---|\n",
       "| 5 | 1000      | 1         | 0.7966667 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  k    train.accuracy test.accuracy\n",
       "5 1000 1              0.7966667    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat(\"Random Forest Accuracies\")\n",
    "rf.accuracies\n",
    "\n",
    "bestK <- rf.accuracies[rf.accuracies$test.accuracy == max(rf.accuracies$test.accuracy),]$k\n",
    "cat(\"\\nBest K:\", bestK)\n",
    "\n",
    "rf.accuracies[rf.accuracies$test.accuracy == max(rf.accuracies$test.accuracy),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the Models**\n",
    "- Random forest did improve the performance of decision trees significantly. \n",
    "- At `k = 50` onwards, the random forest model was able to predict all training data correctly, which is considered <u>overfitting</u>. \n",
    "- Because of bootstrapping, even if the model overfits the training data, random forest produced formidable test accuracies of almost **~80%**.\n",
    "- Overfitting is severe at `k = 10`, but variance decreases as `k` increases. This is why the best model considered is the one with the most subtrees (`k = 1000`).\n",
    "\n",
    "**Variable importance**\n",
    "\n",
    "To get a better insight at the resulting random forest model, variable importance is retrieved using the `varImp()` function.\n",
    "The parameter `type = 1` means we want to retrieve the overall variable importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomForest <- randomForest(Result ~ ., data = train, ntree = bestK, importance = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\ngg==",
      "text/plain": [
       "Plot with title \"Random Forest Variable Importance\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf.varImp <- varImp(randomForest, type = 1)\n",
    "rf.varImp[,'Attributes'] <- rownames(rf.varImp)\n",
    "rf.varImp <- rf.varImp[order(rf.varImp$Overall),]\n",
    "\n",
    "# plot the variable importance\n",
    "barplot(rf.varImp$Overall, horiz = TRUE, names.arg = rf.varImp$Attributes, cex.names = 0.8)\n",
    "title(\"Random Forest Variable Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features according to the randomForest model are **ATT1, ATT10, and ATT7**, while **ATT5, ATT4, and ATT6** were the least important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Support Vector Machines\n",
    "\n",
    "**Support Vector Machines** (SVM) are one of the most popular ways of classification especially for non-linear decision boundaries. SVMs enlarge the feature space through the use of *similarity functions* or **kernels** to accommodate non-linear boundary between classes. There are three used kernels in this notebook: **radial kernel, linear kernel, and polynomial kernel.**\n",
    "\n",
    "To perform SVM, the `svm()` function is used from the `e1071` package.\n",
    "\n",
    "SVMs are parameterized by a **cost** value. The cost value describes how strict would you allow certain outliers from each classes to be misclassfieid. It also determines the number of support vectors needed to detect the decision boundary. If the cost value is <u>large</u>, there is a possibility of having a <u>high bias but low variance</u> predictions. On the other hand, if the cost value is <u>small</u>, the model is prone to <u>low bias but high variance</u>. Therefore it is important to produce an optimal cost parameter. To do this, a 10-fold cross validation method is used in the `tune()` function from the `e1071` package.\n",
    "\n",
    "As this type of model is said to be very robust and is said to be more powerful than other classification models, I have decided to include this in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Radial Kernel SVM\n",
    "\n",
    "**Radial Kernel SVMs** are one of the most popular kernels available, which only considers nearby training observations for predicting the class of the test observation. \n",
    "\n",
    "The kernel function is\n",
    "\n",
    "\n",
    "$$\n",
    "K(x_i, l_i) = exp(-\\gamma\\sum_{j=1}^{p}(x_\\text{ij} - l_\\text{ij})^2)\n",
    "$$\n",
    "\n",
    "A positive constant parameter called $\\gamma$ is used to aid in predicting the non-linear decision boundary. Together with the **cost** parameter, the **gamma** parameter is also tuned via cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Parameter tuning of 'svm':\n",
       "\n",
       "- sampling method: 10-fold cross validation \n",
       "\n",
       "- best parameters:\n",
       " cost gamma\n",
       "  100  0.01\n",
       "\n",
       "- best performance: 0.01833333 \n",
       "\n",
       "- Detailed performance results:\n",
       "    cost gamma      error  dispersion\n",
       "1  1e-02 1e-02 0.65083333 0.017546235\n",
       "2  1e-01 1e-02 0.28458333 0.023899384\n",
       "3  1e+00 1e-02 0.05916667 0.017213259\n",
       "4  5e+00 1e-02 0.03291667 0.013241874\n",
       "5  1e+01 1e-02 0.02500000 0.007607258\n",
       "6  1e+02 1e-02 0.01833333 0.011653432\n",
       "7  1e-02 1e-01 0.49083333 0.038530091\n",
       "8  1e-01 1e-01 0.16166667 0.023471811\n",
       "9  1e+00 1e-01 0.06291667 0.011528614\n",
       "10 5e+00 1e-01 0.06916667 0.015365907\n",
       "11 1e+01 1e-01 0.06875000 0.014197950\n",
       "12 1e+02 1e-01 0.06416667 0.013058510\n",
       "13 1e-02 1e+00 0.65083333 0.017546235\n",
       "14 1e-01 1e+00 0.65083333 0.017546235\n",
       "15 1e+00 1e+00 0.39250000 0.024279748\n",
       "16 5e+00 1e+00 0.37375000 0.016787755\n",
       "17 1e+01 1e+00 0.37375000 0.016787755\n",
       "18 1e+02 1e+00 0.37375000 0.016787755\n",
       "19 1e-02 5e+00 0.65083333 0.017546235\n",
       "20 1e-01 5e+00 0.65083333 0.017546235\n",
       "21 1e+00 5e+00 0.60708333 0.064670640\n",
       "22 5e+00 5e+00 0.59791667 0.065653470\n",
       "23 1e+01 5e+00 0.59791667 0.065653470\n",
       "24 1e+02 5e+00 0.59791667 0.065653470\n",
       "25 1e-02 1e+01 0.65083333 0.017546235\n",
       "26 1e-01 1e+01 0.65083333 0.017546235\n",
       "27 1e+00 1e+01 0.64833333 0.020050092\n",
       "28 5e+00 1e+01 0.64833333 0.020050092\n",
       "29 1e+01 1e+01 0.64833333 0.020050092\n",
       "30 1e+02 1e+01 0.64833333 0.020050092\n",
       "31 1e-02 1e+02 0.65083333 0.017546235\n",
       "32 1e-01 1e+02 0.65083333 0.017546235\n",
       "33 1e+00 1e+02 0.65083333 0.017546235\n",
       "34 5e+00 1e+02 0.65083333 0.017546235\n",
       "35 1e+01 1e+02 0.65083333 0.017546235\n",
       "36 1e+02 1e+02 0.65083333 0.017546235\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = Result ~ ., data = train, kernel = \"radial\", cost = tune.radial$best.parameter$cost, \n",
       "    gamma = tune.radial$best.parameter$gamma)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  radial \n",
       "       cost:  100 \n",
       "      gamma:  0.01 \n",
       "\n",
       "Number of Support Vectors:  362\n",
       "\n",
       " ( 106 73 148 35 )\n",
       "\n",
       "\n",
       "Number of Classes:  4 \n",
       "\n",
       "Levels: \n",
       " 0 1 2 3\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.radial <- tune(svm, Result ~ ., \n",
    "                    data = train, \n",
    "                    kernel = \"radial\", \n",
    "                    ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), \n",
    "                    gamma = c(0.01, 0.1, 1, 5, 10, 100)))\n",
    "summary(tune.radial)\n",
    "\n",
    "svm.radial <- svm(Result ~ ., \n",
    "                  kernel = \"radial\", \n",
    "                  data = train, \n",
    "                  cost = tune.radial$best.parameter$cost, \n",
    "                  gamma =  tune.radial$best.parameter$gamma)\n",
    "summary(svm.radial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the model**\n",
    "- The model took a moderate amount of time to be trained, as radial SVMs are computationally expensive. \n",
    "- According to the CV, the optimal **cost** parameter is 100, while the radial kernel's optimal **gamma** parameter is 0.01.\n",
    "- It took a total of **362** support vectors, **106** support vectors for class 0, **73** support vectors for class 1, **148** support vectors for class 2, and **35** vectors for class 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Linear Kernel SVM\n",
    "\n",
    "**Linear Kernel SVM** is equivalent to creating a **support vector classifier**, which is a soft margin classifier that allows some outliers to be on the wrong side of the *linear* decision boundary of the hyperplane. It uses the *Pearson Correlation* to determine the similarity of two points. Basically, this version of SVM gives out a standard linear classifier. \n",
    "The kernel function is\n",
    "\n",
    "$$\n",
    "K(x_i, l_i) = \\sum_{j=1}^{p}x_\\text{ij}l_\\text{ij}\n",
    "$$\n",
    "\n",
    "Aside from the cost parameter, no additional parameters are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Parameter tuning of 'svm':\n",
       "\n",
       "- sampling method: 10-fold cross validation \n",
       "\n",
       "- best parameters:\n",
       " cost\n",
       "   10\n",
       "\n",
       "- best performance: 0.0004166667 \n",
       "\n",
       "- Detailed performance results:\n",
       "          cost        error  dispersion\n",
       "1   0.01000000 0.0804166667 0.024140311\n",
       "2   0.01778279 0.0495833333 0.015768630\n",
       "3   0.03162278 0.0362500000 0.011955744\n",
       "4   0.05623413 0.0270833333 0.008838835\n",
       "5   0.10000000 0.0245833333 0.008436857\n",
       "6   0.17782794 0.0158333333 0.007027284\n",
       "7   0.31622777 0.0075000000 0.004730385\n",
       "8   0.56234133 0.0141666667 0.005624571\n",
       "9   1.00000000 0.0100000000 0.006860605\n",
       "10  1.77827941 0.0045833333 0.002365193\n",
       "11  3.16227766 0.0025000000 0.002151657\n",
       "12  5.62341325 0.0020833333 0.002196026\n",
       "13 10.00000000 0.0004166667 0.001317616\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = Result ~ ., data = train, kernel = \"linear\", cost = tune.linear$best.parameter$cost)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  linear \n",
       "       cost:  10 \n",
       "      gamma:  0.1 \n",
       "\n",
       "Number of Support Vectors:  372\n",
       "\n",
       " ( 90 76 178 28 )\n",
       "\n",
       "\n",
       "Number of Classes:  4 \n",
       "\n",
       "Levels: \n",
       " 0 1 2 3\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.linear <- tune(svm, Result ~ ., \n",
    "                    data = train, \n",
    "                    kernel = \"linear\", \n",
    "                    ranges = list(cost = 10^seq(-2, 1, by = 0.25)))\n",
    "summary(tune.linear)\n",
    "svm.linear <- svm(Result ~ ., \n",
    "                  kernel = \"linear\", \n",
    "                  data = train, \n",
    "                  cost = tune.linear$best.parameter$cost)\n",
    "summary(svm.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the model**\n",
    "- This SVM was the fastest to be trained among other SVMs, as it only uses a linear similarity function. \n",
    "- According to the CV, the optimal **cost** parameter is 10.\n",
    "- It took a total of **372** support vectors, **90** support vectors for class 0, **76** support vectors for class 1, **178** support vectors for class 2, and **28** vectors for class 3.\n",
    "- There were less support vectors needed because the decision boundary is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Polynomial Kernel SVM of Order 2\n",
    "\n",
    "**Polynomial kernels** extend the linear kernel by just extending the linear kernel's equation into higher dimension. In this case, the polynomial kernel of order 2 extends it to a quadratic function, which produces a non-linear decision boundary so it is considered more robust than the linear kernel but also more computationally expensive.\n",
    "\n",
    "The kernel function is\n",
    "\n",
    "$$\n",
    "K(x_i, l_i) = (1 + \\sum_{j=1}^{p}x_\\text{ij}l_\\text{ij})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Parameter tuning of 'svm':\n",
       "\n",
       "- sampling method: 10-fold cross validation \n",
       "\n",
       "- best parameters:\n",
       " cost\n",
       "   10\n",
       "\n",
       "- best performance: 0.2479167 \n",
       "\n",
       "- Detailed performance results:\n",
       "          cost     error dispersion\n",
       "1   0.01000000 0.5937500 0.05713890\n",
       "2   0.01778279 0.5666667 0.03742276\n",
       "3   0.03162278 0.5291667 0.03339115\n",
       "4   0.05623413 0.4950000 0.03741245\n",
       "5   0.10000000 0.4508333 0.03267876\n",
       "6   0.17782794 0.3933333 0.03063122\n",
       "7   0.31622777 0.3508333 0.03009758\n",
       "8   0.56234133 0.3137500 0.03390994\n",
       "9   1.00000000 0.2937500 0.03442932\n",
       "10  1.77827941 0.2675000 0.02898755\n",
       "11  3.16227766 0.2600000 0.03346041\n",
       "12  5.62341325 0.2520833 0.03607102\n",
       "13 10.00000000 0.2479167 0.03174758\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = Result ~ ., data = train, kernel = \"polynomial\", degree = 2, \n",
       "    cost = tune.poly2$best.parameter$cost)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  polynomial \n",
       "       cost:  10 \n",
       "     degree:  2 \n",
       "      gamma:  0.1 \n",
       "     coef.0:  0 \n",
       "\n",
       "Number of Support Vectors:  1341\n",
       "\n",
       " ( 439 527 281 94 )\n",
       "\n",
       "\n",
       "Number of Classes:  4 \n",
       "\n",
       "Levels: \n",
       " 0 1 2 3\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.poly2 <- tune(svm, Result ~ ., \n",
    "                  data = train, \n",
    "                  kernel = \"polynomial\", \n",
    "                  degree = 2, \n",
    "                  ranges = list(cost = 10^seq(-2, 1, by = 0.25)))\n",
    "summary(tune.poly2)\n",
    "svm.poly2 <- svm(Result ~ ., \n",
    "                  kernel = \"polynomial\", \n",
    "                  degree = 2,\n",
    "                  data = train, \n",
    "                  cost = tune.poly2$best.parameter$cost)\n",
    "summary(svm.poly2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the model**\n",
    "- This SVM was the one of the slowest to be trained, due to its complex similarity function. \n",
    "- According to the CV, the optimal **cost** parameter is 10, which is between the cost of linear and radial kernels.\n",
    "- It took a total of **1341** support vectors, **439** support vectors for class 0, **527** support vectors for class 1, **281** support vectors for class 2, and **94** vectors for class 3.\n",
    "- The number of vectors for order 2 is way more than both the radial and linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Polynomial Kernel SVM of Order 3\n",
    "\n",
    "Again, a polynomial kernel is implemented in this case with order 3, which makes it more complicated than order 2 but is more flexible in terms of identifying the decision boundary.\n",
    "\n",
    "The kernel function is\n",
    "\n",
    "$$\n",
    "K(x_i, l_i) = (1 + \\sum_{j=1}^{p}x_\\text{ij}l_\\text{ij})^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Parameter tuning of 'svm':\n",
       "\n",
       "- sampling method: 10-fold cross validation \n",
       "\n",
       "- best parameters:\n",
       " cost\n",
       "   10\n",
       "\n",
       "- best performance: 0.13375 \n",
       "\n",
       "- Detailed performance results:\n",
       "          cost     error dispersion\n",
       "1   0.01000000 0.5904167 0.12212828\n",
       "2   0.01778279 0.5216667 0.07489190\n",
       "3   0.03162278 0.4129167 0.03795760\n",
       "4   0.05623413 0.2662500 0.03358987\n",
       "5   0.10000000 0.2012500 0.02332755\n",
       "6   0.17782794 0.1633333 0.01593292\n",
       "7   0.31622777 0.1458333 0.01631575\n",
       "8   0.56234133 0.1395833 0.01995848\n",
       "9   1.00000000 0.1345833 0.02265635\n",
       "10  1.77827941 0.1370833 0.01888582\n",
       "11  3.16227766 0.1366667 0.02020344\n",
       "12  5.62341325 0.1391667 0.01610153\n",
       "13 10.00000000 0.1337500 0.01233690\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = Result ~ ., data = train, kernel = \"polynomial\", degree = 3, \n",
       "    cost = tune.poly3$best.parameter$cost)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  polynomial \n",
       "       cost:  10 \n",
       "     degree:  3 \n",
       "      gamma:  0.1 \n",
       "     coef.0:  0 \n",
       "\n",
       "Number of Support Vectors:  827\n",
       "\n",
       " ( 222 187 369 49 )\n",
       "\n",
       "\n",
       "Number of Classes:  4 \n",
       "\n",
       "Levels: \n",
       " 0 1 2 3\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.poly3 <- tune(svm, Result ~ ., \n",
    "                  data = train, \n",
    "                  kernel = \"polynomial\", \n",
    "                  degree = 3, \n",
    "                  ranges = list(cost = 10^seq(-2, 1, by = 0.25)))\n",
    "summary(tune.poly3)\n",
    "svm.poly3 <- svm(Result ~ ., \n",
    "                  kernel = \"polynomial\", \n",
    "                  degree = 3,\n",
    "                  data = train, \n",
    "                  cost = tune.poly3$best.parameter$cost)\n",
    "summary(svm.poly3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the model**\n",
    "- This SVM was the one of the slowest to be trained, due to its complex similarity function. \n",
    "- According to the CV, the optimal **cost** parameter is 10.\n",
    "- It took a total of **827** support vectors, **222** support vectors for class 0, **187** support vectors for class 1, **369** support vectors for class 2, and **49** vectors for class 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5 Polynomial SVM of Order 4\n",
    "\n",
    "The kernel function of this polynomial SVM is\n",
    "\n",
    "$$\n",
    "K(x_i, l_i) = (1 + \\sum_{j=1}^{p}x_\\text{ij}l_\\text{ij})^4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Parameter tuning of 'svm':\n",
       "\n",
       "- sampling method: 10-fold cross validation \n",
       "\n",
       "- best parameters:\n",
       "     cost\n",
       " 5.623413\n",
       "\n",
       "- best performance: 0.2366667 \n",
       "\n",
       "- Detailed performance results:\n",
       "          cost     error dispersion\n",
       "1   0.01000000 0.6350000 0.05118955\n",
       "2   0.01778279 0.5716667 0.06780851\n",
       "3   0.03162278 0.5295833 0.03039414\n",
       "4   0.05623413 0.4545833 0.01938980\n",
       "5   0.10000000 0.3912500 0.02269038\n",
       "6   0.17782794 0.3512500 0.01757370\n",
       "7   0.31622777 0.3125000 0.02315741\n",
       "8   0.56234133 0.2800000 0.02551930\n",
       "9   1.00000000 0.2541667 0.02900085\n",
       "10  1.77827941 0.2529167 0.02469334\n",
       "11  3.16227766 0.2429167 0.02959670\n",
       "12  5.62341325 0.2366667 0.02797155\n",
       "13 10.00000000 0.2408333 0.02203042\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "svm(formula = Result ~ ., data = train, kernel = \"polynomial\", degree = 4, \n",
       "    cost = tune.poly4$best.parameter$cost)\n",
       "\n",
       "\n",
       "Parameters:\n",
       "   SVM-Type:  C-classification \n",
       " SVM-Kernel:  polynomial \n",
       "       cost:  5.623413 \n",
       "     degree:  4 \n",
       "      gamma:  0.1 \n",
       "     coef.0:  0 \n",
       "\n",
       "Number of Support Vectors:  1651\n",
       "\n",
       " ( 404 574 580 93 )\n",
       "\n",
       "\n",
       "Number of Classes:  4 \n",
       "\n",
       "Levels: \n",
       " 0 1 2 3\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tune.poly4 <- tune(svm, Result ~ ., \n",
    "                  data = train, \n",
    "                  kernel = \"polynomial\", \n",
    "                  degree = 4, \n",
    "                  ranges = list(cost = 10^seq(-2, 1, by = 0.25)))\n",
    "summary(tune.poly4)\n",
    "svm.poly4 <- svm(Result ~ ., \n",
    "                  kernel = \"polynomial\", \n",
    "                  degree = 4,\n",
    "                  data = train, \n",
    "                  cost = tune.poly4$best.parameter$cost)\n",
    "summary(svm.poly4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the model**\n",
    "- This SVM was the one of the slowest to be trained, due to its complex similarity function. \n",
    "- According to the CV, the optimal **cost** parameter is 0.1, which is prone to being biased.\n",
    "- It took a total of **1651** support vectors, **404** support vectors for class 0, **574** support vectors for class 1, **580** support vectors for class 2, and **93** vectors for class 3.\n",
    "\n",
    "It can be inferred then that polynomial kernels with orders 3 and 4 is not recommended in this dataset, as they do not pose any improvements on the model made with order 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6 Summary of SVM Models\n",
    "\n",
    "So far we have generated a lot of models. To determine its performance against the dataset, the training accuracy and the testing accuracy of all the models are computed, and is consolidated on a single table for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a table that summarizes the accuracy\n",
    "# number of models made\n",
    "n <- 5\n",
    "svm.accuracies <- as.data.frame(matrix(NA,nrow=n, ncol=3))\n",
    "names(svm.accuracies) <- c('model', 'train.accuracy', 'test.accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Getting the training accuracy\n",
    "## and adding it to the summary table\n",
    "##\n",
    "\n",
    "i <- 1\n",
    "\n",
    "# get train prediction accuracy for the radial model\n",
    "radial.train.pred <- predict(svm.radial, train)\n",
    "radial.train.accuracy <- confusionMatrix(radial.train.pred, train[,ncol(train)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'model'] <- 'SVM with radial kernel'\n",
    "svm.accuracies[i, 'train.accuracy'] <- radial.train.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get train prediction accuracy for the linear model\n",
    "linear.train.pred <- predict(svm.linear, train)\n",
    "linear.train.accuracy <- confusionMatrix(linear.train.pred, train[,ncol(train)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'model'] <- 'SVM with linear kernel'\n",
    "svm.accuracies[i, 'train.accuracy'] <- linear.train.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get train prediction accuracy for the polynomial model\n",
    "poly2.train.pred <- predict(svm.poly2, train)\n",
    "poly2.train.accuracy <- confusionMatrix(poly2.train.pred, train[,ncol(train)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'model'] <- 'SVM with polynomial kernel of order 2'\n",
    "svm.accuracies[i, 'train.accuracy'] <- poly2.train.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get train prediction accuracy for the polynomial model\n",
    "poly3.train.pred <- predict(svm.poly3, train)\n",
    "poly3.train.accuracy <- confusionMatrix(poly3.train.pred, train[,ncol(train)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'model'] <- 'SVM with polynomial kernel of order 3'\n",
    "svm.accuracies[i, 'train.accuracy'] <- poly3.train.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get train prediction accuracy for the polynomial model\n",
    "poly4.train.pred <- predict(svm.poly4, train)\n",
    "poly4.train.accuracy <- confusionMatrix(poly4.train.pred, train[,ncol(train)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'model'] <- 'SVM with polynomial kernel of order 4'\n",
    "svm.accuracies[i, 'train.accuracy'] <- poly4.train.accuracy\n",
    "i <- i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##\n",
    "## Getting the testing accuracy\n",
    "## and adding it to the summary table\n",
    "##\n",
    "\n",
    "i <- 1\n",
    "\n",
    "# get test prediction accuracy for the radial model\n",
    "radial.test.pred <- predict(svm.radial, test)\n",
    "radial.test.accuracy <- confusionMatrix(radial.test.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'test.accuracy'] <- radial.test.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get test prediction accuracy for the linear model\n",
    "linear.test.pred <- predict(svm.linear, test)\n",
    "linear.test.accuracy <- confusionMatrix(linear.test.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'test.accuracy'] <- linear.test.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get test prediction accuracy for the polynomial model\n",
    "poly2.test.pred <- predict(svm.poly2, test)\n",
    "poly2.test.accuracy <- confusionMatrix(poly2.test.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'test.accuracy'] <- poly2.test.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get test prediction accuracy for the polynomial model\n",
    "poly3.test.pred <- predict(svm.poly3, test)\n",
    "poly3.test.accuracy <- confusionMatrix(poly3.test.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'test.accuracy'] <- poly3.test.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "# get test prediction accuracy for the polynomial model\n",
    "poly4.test.pred <- predict(svm.poly4, test)\n",
    "poly4.test.accuracy <- confusionMatrix(poly4.test.pred, test[,ncol(test)])$overall['Accuracy']\n",
    "svm.accuracies[i, 'test.accuracy'] <- poly4.test.accuracy\n",
    "i <- i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>model</th><th scope=col>train.accuracy</th><th scope=col>test.accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>SVM with radial kernel               </td><td>0.9991667                            </td><td>0.9833333                            </td></tr>\n",
       "\t<tr><td>SVM with linear kernel               </td><td>0.9995833                            </td><td>1.0000000                            </td></tr>\n",
       "\t<tr><td>SVM with polynomial kernel of order 2</td><td>0.7900000                            </td><td>0.7750000                            </td></tr>\n",
       "\t<tr><td>SVM with polynomial kernel of order 3</td><td>0.9679167                            </td><td>0.8950000                            </td></tr>\n",
       "\t<tr><td>SVM with polynomial kernel of order 4</td><td>0.9454167                            </td><td>0.7883333                            </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " model & train.accuracy & test.accuracy\\\\\n",
       "\\hline\n",
       "\t SVM with radial kernel                & 0.9991667                             & 0.9833333                            \\\\\n",
       "\t SVM with linear kernel                & 0.9995833                             & 1.0000000                            \\\\\n",
       "\t SVM with polynomial kernel of order 2 & 0.7900000                             & 0.7750000                            \\\\\n",
       "\t SVM with polynomial kernel of order 3 & 0.9679167                             & 0.8950000                            \\\\\n",
       "\t SVM with polynomial kernel of order 4 & 0.9454167                             & 0.7883333                            \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "model | train.accuracy | test.accuracy | \n",
       "|---|---|---|---|---|\n",
       "| SVM with radial kernel                | 0.9991667                             | 0.9833333                             | \n",
       "| SVM with linear kernel                | 0.9995833                             | 1.0000000                             | \n",
       "| SVM with polynomial kernel of order 2 | 0.7900000                             | 0.7750000                             | \n",
       "| SVM with polynomial kernel of order 3 | 0.9679167                             | 0.8950000                             | \n",
       "| SVM with polynomial kernel of order 4 | 0.9454167                             | 0.7883333                             | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  model                                 train.accuracy test.accuracy\n",
       "1 SVM with radial kernel                0.9991667      0.9833333    \n",
       "2 SVM with linear kernel                0.9995833      1.0000000    \n",
       "3 SVM with polynomial kernel of order 2 0.7900000      0.7750000    \n",
       "4 SVM with polynomial kernel of order 3 0.9679167      0.8950000    \n",
       "5 SVM with polynomial kernel of order 4 0.9454167      0.7883333    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the summary of accuracies\n",
    "svm.accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best SVM model: SVM with linear kernel"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>model</th><th scope=col>train.accuracy</th><th scope=col>test.accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>2</th><td>SVM with linear kernel</td><td>0.9995833             </td><td>1                     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & model & train.accuracy & test.accuracy\\\\\n",
       "\\hline\n",
       "\t2 & SVM with linear kernel & 0.9995833              & 1                     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | model | train.accuracy | test.accuracy | \n",
       "|---|\n",
       "| 2 | SVM with linear kernel | 0.9995833              | 1                      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  model                  train.accuracy test.accuracy\n",
       "2 SVM with linear kernel 0.9995833      1            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bestSVM <- svm.accuracies[svm.accuracies$test.accuracy == max(svm.accuracies$test.accuracy),]$model\n",
    "cat(\"\\nBest SVM model:\", bestSVM)\n",
    "\n",
    "svm.accuracies[svm.accuracies$test.accuracy == max(svm.accuracies$test.accuracy),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of model performance**\n",
    "- The SVM with **radial kernel** and **linear kernel** were very good and powerful in determining the actual decision boundary. Both kernels almost got all the training observations right, which might suggest overfitting, but it also got high test accuracies, which tells that the model was very robust and was able to generalize well. \n",
    "- Both high test and train accuracies may be because of the large data size. More data means better performance. \n",
    "- Based on the accuracies (and the computation complexity), the **SVM with linear kernel** is a good model to the data. It has very high accuracy and does not take long to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Neural Networks\n",
    "\n",
    "Note: Materials used in this section are based from FIT 5201: Data Analysis Algorithms lecture notes.\n",
    "\n",
    "**Neural Networks** is another popular model for classification. Like SVM, it is also good at detecting non-linear decision boundaries. It is made up of a collection of neurons on multiple layers that may be used to enhance the feature space like SVM through the use of **activation functions**.\n",
    "\n",
    "A basic neuron is made up of a collection of inputs $x_0, x_1, x_2, ..., x_j$ that goes through an *activation function* $h_\\text{w,b}$ whose results are placed as another input to another neuron or the final output of the network. It is robust because Neural Networks can produce non-linear hidden units that enhances the feature space. One of the most popular activation function is the **logistic function** $\\sigma(z)$ which is also featured in Logistic Regression.\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Another popular activation function is the **tanh** function\n",
    "\n",
    "$$\n",
    "\\text{tanh}(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "where z is the total weighted sum of the inputs\n",
    "\n",
    "$$\n",
    "z^{(l)} = \\sum_{j=1}^{n}W_\\text{ij}^{(l-1)}x_j + b_i^{(l-1)}\n",
    "$$\n",
    "\n",
    "These non-linear activation functions make it possible to detect non-linearities.\n",
    "\n",
    "Neural Networks are parameterized by the weights $W_\\text{ij}^{(l)}$ and the bias terms $b_i$ of the neuron $i$ with respect to input $j$ of the layer $l$.   \n",
    "\n",
    "To implement Neural Networks in R, an external library called `h2o` is used. This library is a fast and scalable Machine Learning API that is popular among Data Scientist for being open-source. The installation options can be found <a href=\"http://h2o-release.s3.amazonaws.com/h2o/rel-weierstrass/6/index.html\">here</a> (it's not just a simple `install.package()` call, unfortunately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Connection successful!\n",
      "\n",
      "R is connected to the H2O cluster: \n",
      "    H2O cluster uptime:         17 minutes 46 seconds \n",
      "    H2O cluster version:        3.15.0.4046 \n",
      "    H2O cluster version age:    13 days  \n",
      "    H2O cluster name:           mrkjse \n",
      "    H2O cluster total nodes:    1 \n",
      "    H2O cluster total memory:   3.52 GB \n",
      "    H2O cluster total cores:    4 \n",
      "    H2O cluster allowed cores:  4 \n",
      "    H2O cluster healthy:        TRUE \n",
      "    H2O Connection ip:          localhost \n",
      "    H2O Connection port:        54321 \n",
      "    H2O Connection proxy:       NA \n",
      "    H2O Internal Security:      FALSE \n",
      "    H2O API Extensions:         Algos, AutoML, Core V3, Core V4 \n",
      "    R Version:                  R version 3.3.2 (2016-10-31) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just a simple Neural Network using the h2o library\n",
    "\n",
    "# initialise h2o instance\n",
    "localH2O <- h2o.init(nthreads = -1, port = 54321, startH2O = FALSE, max_mem_size = '6G', strict_version_check = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing a Neural Network**\n",
    "\n",
    "To train a Neural Network in `h2o`, the function `deeplearning()` is used. The parameters are self-explanatory, but those specific to neural networks are `hidden`, which indicates the number of hidden layers and their neurons (eg. hidden = c(100, 200, 100) means 3 hidden layers with the first and third hidden layer having 100 neurons while the second has 200). The activation function which can be `Sigmoid` or `Tanh`, `autoencoder` which is set to FALSE, and a regularizer `l2`.\n",
    "\n",
    "In here I tested 10 different Neural Network models of varying hidden layer neurons from 100 to 1000 and recorded both their training and testing accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n",
      "  |======================================================================| 100%\n"
     ]
    }
   ],
   "source": [
    "nn.train <- as.h2o(train)\n",
    "nn.test <- as.h2o(test)\n",
    "K <- seq(100, 1000, 100)\n",
    "\n",
    "nn.accuracies <- as.data.frame(matrix(NA,nrow=length(K), ncol=3))\n",
    "names(nn.accuracies) <- c('k', 'train.accuracy', 'test.accuracy')\n",
    "i <- 1\n",
    "\n",
    "set.seed(1234)\n",
    "options(warn=-1)\n",
    "for (k in K){\n",
    "    nn.model <- h2o.deeplearning(    \n",
    "          x = 1:(ncol(train) - 1), # select the feature space\n",
    "          y = ncol(train), # select the result\n",
    "          training_frame = nn.train, # specify the frame     \n",
    "          hidden = c(k), # number of layers and their units\n",
    "          epochs = 50, # maximum number of iteration \n",
    "          activation = 'Tanh', # activation function \n",
    "          autoencoder = FALSE, # is it an autoencoder? No\n",
    "          l2 = 0.1 )\n",
    "\n",
    "    nn.trainPredict <- h2o.predict(nn.model, nn.train)$predict\n",
    "    nn.testPredict <- h2o.predict(nn.model, nn.test)$predict\n",
    "    \n",
    "    nn.trainAccuracy <- confusionMatrix(as.data.frame(nn.trainPredict)[,1], train[,ncol(train)])$overall['Accuracy']\n",
    "    nn.testAccuracy <- confusionMatrix(as.data.frame(nn.testPredict)[,1], test[,ncol(test)])$overall['Accuracy']\n",
    "    \n",
    "    nn.accuracies[i, 'k'] <- k\n",
    "    nn.accuracies[i, 'train.accuracy'] <- nn.trainAccuracy\n",
    "    nn.accuracies[i, 'test.accuracy'] <- nn.testAccuracy\n",
    "    i <- i + 1\n",
    "\n",
    "}\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Neural Network Models\n",
    "\n",
    "The Neural Network models are laid out on the suceeding cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Accuracies"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>k</th><th scope=col>train.accuracy</th><th scope=col>test.accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td> 100     </td><td>0.8233333</td><td>0.8333333</td></tr>\n",
       "\t<tr><td> 200     </td><td>0.8379167</td><td>0.8466667</td></tr>\n",
       "\t<tr><td> 300     </td><td>0.8504167</td><td>0.8583333</td></tr>\n",
       "\t<tr><td> 400     </td><td>0.8250000</td><td>0.8383333</td></tr>\n",
       "\t<tr><td> 500     </td><td>0.8220833</td><td>0.8416667</td></tr>\n",
       "\t<tr><td> 600     </td><td>0.8650000</td><td>0.8766667</td></tr>\n",
       "\t<tr><td> 700     </td><td>0.8100000</td><td>0.8300000</td></tr>\n",
       "\t<tr><td> 800     </td><td>0.8595833</td><td>0.8666667</td></tr>\n",
       "\t<tr><td> 900     </td><td>0.8545833</td><td>0.8666667</td></tr>\n",
       "\t<tr><td>1000     </td><td>0.8237500</td><td>0.8383333</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " k & train.accuracy & test.accuracy\\\\\n",
       "\\hline\n",
       "\t  100      & 0.8233333 & 0.8333333\\\\\n",
       "\t  200      & 0.8379167 & 0.8466667\\\\\n",
       "\t  300      & 0.8504167 & 0.8583333\\\\\n",
       "\t  400      & 0.8250000 & 0.8383333\\\\\n",
       "\t  500      & 0.8220833 & 0.8416667\\\\\n",
       "\t  600      & 0.8650000 & 0.8766667\\\\\n",
       "\t  700      & 0.8100000 & 0.8300000\\\\\n",
       "\t  800      & 0.8595833 & 0.8666667\\\\\n",
       "\t  900      & 0.8545833 & 0.8666667\\\\\n",
       "\t 1000      & 0.8237500 & 0.8383333\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "k | train.accuracy | test.accuracy | \n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "|  100      | 0.8233333 | 0.8333333 | \n",
       "|  200      | 0.8379167 | 0.8466667 | \n",
       "|  300      | 0.8504167 | 0.8583333 | \n",
       "|  400      | 0.8250000 | 0.8383333 | \n",
       "|  500      | 0.8220833 | 0.8416667 | \n",
       "|  600      | 0.8650000 | 0.8766667 | \n",
       "|  700      | 0.8100000 | 0.8300000 | \n",
       "|  800      | 0.8595833 | 0.8666667 | \n",
       "|  900      | 0.8545833 | 0.8666667 | \n",
       "| 1000      | 0.8237500 | 0.8383333 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   k    train.accuracy test.accuracy\n",
       "1   100 0.8233333      0.8333333    \n",
       "2   200 0.8379167      0.8466667    \n",
       "3   300 0.8504167      0.8583333    \n",
       "4   400 0.8250000      0.8383333    \n",
       "5   500 0.8220833      0.8416667    \n",
       "6   600 0.8650000      0.8766667    \n",
       "7   700 0.8100000      0.8300000    \n",
       "8   800 0.8595833      0.8666667    \n",
       "9   900 0.8545833      0.8666667    \n",
       "10 1000 0.8237500      0.8383333    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best K: 600"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>k</th><th scope=col>train.accuracy</th><th scope=col>test.accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>6</th><td>600      </td><td>0.865    </td><td>0.8766667</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & k & train.accuracy & test.accuracy\\\\\n",
       "\\hline\n",
       "\t6 & 600       & 0.865     & 0.8766667\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | k | train.accuracy | test.accuracy | \n",
       "|---|\n",
       "| 6 | 600       | 0.865     | 0.8766667 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  k   train.accuracy test.accuracy\n",
       "6 600 0.865          0.8766667    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat(\"Neural Network Accuracies\")\n",
    "nn.accuracies\n",
    "\n",
    "bestK <- nn.accuracies[nn.accuracies$test.accuracy == max(nn.accuracies$test.accuracy),]$k\n",
    "cat(\"\\nBest K:\", bestK)\n",
    "\n",
    "nn.accuracies[nn.accuracies$test.accuracy == max(nn.accuracies$test.accuracy),][1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the models**\n",
    "- The Neural Network models work fine even at `hidden = 100`, with both training and testing accuracies above **80%**, a very impressive performance. \n",
    "- Because we used an external API, the training time was faster compared to most SVM models, but SVM with linear kernel is still faster.\n",
    "- Based from the accuracies, the best model is the Neural Network with **600 hidden layer neurons**. However, as the performance of all the models do not deviate much from each other, it is better to pick the Neural Network with **300 hidden layer neurons** because its performance is pretty close to the best model and is simplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summary of the Classification Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a consolidation of the best models created using different classifiers. There were a total of **23 models** trained and analysed, and the best **4 models** were evaluated below.\n",
    "\n",
    "To recap, the classification model types are:\n",
    "\n",
    "1. Naive Bayes\n",
    "2. Random Forest\n",
    "3. SVM\n",
    "4. Neural Network\n",
    "\n",
    "The analysis of the performance of the classifiers will be discussed in this section. The following cell creates a nice table that displays the best 4 models as indicated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the Models Created\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>model</th><th scope=col>train.accuracy</th><th scope=col>test.accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Naive Bayes Classifier </td><td>0.7941667              </td><td>0.7733333              </td></tr>\n",
       "\t<tr><td>Random Forest          </td><td>1.0000000              </td><td>0.7966667              </td></tr>\n",
       "\t<tr><td>Support Vector Machines</td><td>0.9995833              </td><td>1.0000000              </td></tr>\n",
       "\t<tr><td>Neural Network         </td><td>0.8766667              </td><td>0.8650000              </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " model & train.accuracy & test.accuracy\\\\\n",
       "\\hline\n",
       "\t Naive Bayes Classifier  & 0.7941667               & 0.7733333              \\\\\n",
       "\t Random Forest           & 1.0000000               & 0.7966667              \\\\\n",
       "\t Support Vector Machines & 0.9995833               & 1.0000000              \\\\\n",
       "\t Neural Network          & 0.8766667               & 0.8650000              \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "model | train.accuracy | test.accuracy | \n",
       "|---|---|---|---|\n",
       "| Naive Bayes Classifier  | 0.7941667               | 0.7733333               | \n",
       "| Random Forest           | 1.0000000               | 0.7966667               | \n",
       "| Support Vector Machines | 0.9995833               | 1.0000000               | \n",
       "| Neural Network          | 0.8766667               | 0.8650000               | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  model                   train.accuracy test.accuracy\n",
       "1 Naive Bayes Classifier  0.7941667      0.7733333    \n",
       "2 Random Forest           1.0000000      0.7966667    \n",
       "3 Support Vector Machines 0.9995833      1.0000000    \n",
       "4 Neural Network          0.8766667      0.8650000    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate a summary table of the best four models per classification method\n",
    "cat(\"Summary of the Models Created\\n\\n\")\n",
    "final.accuracies <- as.data.frame(matrix(NA,nrow=4, ncol=3))\n",
    "names(final.accuracies) <- c('model', 'train.accuracy', 'test.accuracy')\n",
    "i <- 1\n",
    "\n",
    "final.accuracies[i, 'model'] <- 'Naive Bayes Classifier'\n",
    "final.accuracies[i, 'train.accuracy'] <- naiveBayes.trainAccuracy\n",
    "final.accuracies[i, 'test.accuracy'] <- naiveBayes.testAccuracy\n",
    "i <- i + 1\n",
    "\n",
    "final.accuracies[i, 'model'] <- 'Random Forest'\n",
    "final.accuracies[i, 'train.accuracy'] <- rf.accuracies[rf.accuracies$test.accuracy == max(rf.accuracies$test.accuracy),]$train.accuracy\n",
    "final.accuracies[i, 'test.accuracy'] <- rf.accuracies[rf.accuracies$test.accuracy == max(rf.accuracies$test.accuracy),]$test.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "final.accuracies[i, 'model'] <- 'Support Vector Machines'\n",
    "final.accuracies[i, 'train.accuracy'] <- svm.accuracies[svm.accuracies$test.accuracy == max(svm.accuracies$test.accuracy),]$train.accuracy\n",
    "final.accuracies[i, 'test.accuracy'] <- svm.accuracies[svm.accuracies$test.accuracy == max(svm.accuracies$test.accuracy),]$test.accuracy\n",
    "i <- i + 1\n",
    "\n",
    "final.accuracies[i, 'model'] <- 'Neural Network'\n",
    "final.accuracies[i, 'train.accuracy'] <- nn.accuracies[nn.accuracies$test.accuracy == max(nn.accuracies$test.accuracy),][1,]$test.accuracy\n",
    "final.accuracies[i, 'test.accuracy'] <- nn.accuracies[nn.accuracies$test.accuracy == max(nn.accuracies$test.accuracy),][1,]$train.accuracy\n",
    "\n",
    "final.accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, all models performed quite well on the data, with the minimum test accuracy at **77%** and the maximum at **100%**.\n",
    "\n",
    "Before deriving which is the *best* model, let us examine each of their characteristics.\n",
    "\n",
    "1. **Naive Bayes**\n",
    "<br/>**PROs:** Good for small training datasets. Less prone to overfitting. Fast to train. Will generally perform on a par with advanced classifiers if the dataset is large.\n",
    "<br/>**CONs:** Assumes all features are independent. Too simplistic look at the data. May suffer from high bias\n",
    "<br/>**Use this for:** If you are sure that the data features are independent from each other; if the number of observations are less than the number of features; or you just want a *classifier-on-the-go* that's fast to implement.\n",
    "<br/><br/><br/>\n",
    "2. **Random Forest**\n",
    "<br/>**PROs**: Decision trees easier to interpret. Can give an insight on variable importance. Can easily handle qualitative features. Handles bias-variance tradeoff very well (due to bootstrapping and feature selection)\n",
    "<br/>**CONs**: Not as robust as other methods. Random forest can take a long execution time but still underperform compared to SVM and NN.\n",
    "<br/>**Use this for:** If you want to create a model that is easy to interpret, or if your dataset has a lot of qualitative features and has a non-linear decision boundary.\n",
    "<br/><br/><br/>\n",
    "3. **SVM**\n",
    "<br/>**PROs**: The *in*, the *popular*, some say it's the *best* (most of the time). SVMs are very robust due to a variety of kernels that may be used to detect various decision boundaries. Does not need a lot of parameters to train (just the `cost` for starters). As an optimisation problem, always arrives at the global minimum.\n",
    "<br/>**CONs**: It is really hard to interpret and takes quite a while to train.\n",
    "<br/>**Use this for:** If you believe the *end justifies the means* i.e. you just want the most robust results, never mind the computational cost. If your dataset looks very, very complex, then it might be better to try this one first then try the simplier models later. Do note that SVMs can still underperform in some scenarios (no free lunch!).\n",
    "<br/><br/><br/>\n",
    "4. **Neural Network**\n",
    "<br/>**PROs**: Also *popular*. Also *in*. This is also very robust, and can be applied to a wide range of classification problems, no matter how complex the decision boundary is. Can learn from any data structure (images, music, speech, text, you name it). \n",
    "<br/>**CONs**: Also hard to interpret and still takes a while to train. As an optimisation problem, is said to get stuck at a local minima. Not as robust as SVMs.\n",
    "<br/>**Use ths for:** Comparable to SVMs, use this if you want a robust classifier and does not care about the computational cost. Opt this if your dataset is non-structured. This gets the job done more often than not.\n",
    "\n",
    "\n",
    "In picking the best model for this dataset, **based on accuracy**, <u>SVM</u> is the best. However, if you take into account **the bias-variance trade-off**, I would pick <u>Neural Networks</u> just because it does not seem to overfit the training data. If I want a **more interpretable model**, I would go for <u>Random Forests</u>, but if I want a **simple *and* interpretable model**, I would go for the <u>Naive Bayes</u>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "This notebook is an example on how to create a complete data analysis task to classify prostrate cancer stages. Results show that the chosen models used for classification perform well enough over the dataset, and the model should be chosen based on a variety of factors. The data analysis task are as follows:\n",
    "\n",
    "1. **Exploratory Data Analysis** - Before doing any data modelling, it is important to perform EDA. This involves parsing the data correctly, cleaning it for any erroneous observations, checking basic statistics like the central tendency measures and the distribution, analysing the outliers, and detecting the relationships of the variables in the data.\n",
    "\n",
    "2. **Regression for Imputation** - Regression is used to predict quantitative variables. It is important that the regression models are regularized to prevent overfitting.\n",
    "\n",
    "3. **Prostrate Cancer Classification** - Several models were analyzed to check which one is the most appropriate classifier. It is important to choose your models carefully depending on the nature of the data and certain limitations like machine capabilities, and to always, always create several models for comparison.\n",
    "\n",
    "4. **Use of Resampling and Cross Validation** - Throughout the whole notebook resampling methods like bootstrapping and cross validation are used to prevent the models from underfitting or overfitting the data.\n",
    "\n",
    "5. **Documentation and Analysis** - The methods above will not matter much if the documentation of the code, analysis of the results, and justification of the steps are lacking. Being able to interpret and analyse your data correctly is half of what it takes to be a Data Scientist.\n",
    "\n",
    "# 6. References Used\n",
    "\n",
    "1. Stack Exchange Inc. (2017). *Neural networks vs support vector machines: are the second definitely superior?*. Retrieved from Cross Validated: https://stats.stackexchange.com/questions/30042/neural-networks-vs-support-vector-machines-are-the-second-definitely-superior.\n",
    "2. James, G. et al. (2017). *An Introduction to Statistical Learning with Applications in R.* Springer Science+Business Media, New York.\n",
    "3. Lecture notes from both FIT 5149 (made by Prof. R. Amarasiri) and FIT 5201 (made by Prof. Y. Kang, Prof. R. Amarasiri, and Prof. G. Haffari). (2017). Faculty of Information Technology, Monash University, Melbourne.\n",
    "4. Ng, A. (2017). *Machine Learning.* Retrieved from Coursera: https://www.coursera.org/learn/machine-learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
