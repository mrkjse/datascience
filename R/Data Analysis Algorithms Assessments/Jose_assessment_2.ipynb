{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5201: Assessment 2\n",
    "## Linear Models for Regression and Classification\n",
    "\n",
    "## Objectives\n",
    "This assignment assesses your understanding of linear models for regression and classification, covered in Modules 2 and 3. The total marks of this assessment is __100__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A. Ridge Regression\n",
    "In this part, you develop Ridge Regression by adding the $L_2$ norm regularization to the linear regression (covered in Activity __1__ of Module __2__). This part assesses your mathematical (calculating derivatives) and programming skills.\n",
    "\n",
    "### Question 1 [Ridge Regression, 35 Marks]\n",
    "__Q1-1__) Given the gradient descent algorithms for linear regression (discussed in Chapter 2 of Module 2), derive weight update steps of stochastic gradient descent (SGD) as well as batch gradient descent (BGD) for linear regression with $L_2$ regularisation norm. Show your work with enough explanation in your PDF report; you should provide the steps of SGD and BGD, separately.\n",
    "__Hint__: Recall that for linear regression we defined the error function $E$ and set its derivation to zero. For this assignment, you only need to add an $L_2$ regularization term to the error function and set the derivative of both terms (error term plus the regularization term) to zero. This question is similar to Activity __1__ of Module __2__.\n",
    "\n",
    "__Q1-2__) Using R (with no use of special libraries), implement SGD and BGD algorithms that you derived in Q1-1. The implementation is straightforward as you are allowed to use the code examples from Activity __1__ in Module __2__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.3.3\""
     ]
    }
   ],
   "source": [
    "library(ggplot2) # for plotting functions.\n",
    "library(reshape2) # for melt and cast functions\n",
    "library(mvtnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################################################################################################\n",
    "#\n",
    "# AUXILIARY FUNCTIONS FOR Q1\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "## Function to make a prediction h(x,w)\n",
    "# X - features/input variables\n",
    "# W - coefficients\n",
    "# returns - predicted continuous value\n",
    "predict_func <- function(X, W){\n",
    "  return(X%*%W)\n",
    "} \n",
    "\n",
    "## Function to calculate the error/objective function\n",
    "# X - features/input variables\n",
    "# W - coefficient of the input variables\n",
    "# Y - actual target variable\n",
    "# lambda - L2 regularisation penalty\n",
    "error_func <- function (X, W, Y, lambda){\n",
    "    return(mean((predict_func(X, W) - Y)^2) * .5 + .5 * lambda * W %*% W)\n",
    "}\n",
    "\n",
    "## Function that computes the coefficients of the input variables (SGD)\n",
    "# Formula: wprev - learning.rate * derivative of the objective function\n",
    "# X - features/input variables\n",
    "# W - (current) coefficient of the input variables\n",
    "# Y - actual target variable\n",
    "# Y_pred - predicted target variable\n",
    "# lambda - L2 regularisation penalty\n",
    "# learning.rate - the learning rate\n",
    "sgd_weight_update <- function(X, W, Y, Y_pred, lambda, learning.rate){   \n",
    "    W_new <- W - learning.rate *  (-(Y - Y_pred) * X + (lambda * W))\n",
    "    return(W_new)\n",
    "} \n",
    "\n",
    "## Function that computes the coefficients of the input variables (BGD)\n",
    "# Formula: wprev - learning.rate * derivative of the objective function\n",
    "# X - features/input variables\n",
    "# W - (current) coefficient of the input variables\n",
    "# Y - actual target variable\n",
    "# Y_pred - predicted target variable\n",
    "# lambda - L2 regularisation penalty\n",
    "# learning.rate - the learning rate\n",
    "bgd_weight_update <- function(X, W, Y, Y_pred, lambda, learning.rate){\n",
    "    W_new <- W\n",
    "    # for each coefficient\n",
    "    for(j in 1:length(W)){\n",
    "        # calculate the derivative of the error function using all X\n",
    "        derivative <- -(Y - Y_pred) * X[,j] + (lambda * W[j])\n",
    "        # important: get the mean() and not the sum()\n",
    "        W_new[j] <- W[j] - learning.rate *  mean(derivative)\n",
    "    }\n",
    "    \n",
    "    return(W_new)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent is an iterative way to build a linear regression model. SGD descents by randomly selecting a random point and updating the weights upon doing so. \n",
    "\n",
    "The steps to performing SGD are:\n",
    "\n",
    "Given:\n",
    "<br/>$\\epsilon$ - error threshold value\n",
    "<br/>$\\eta$ - learning rate\n",
    "<br/>$\\tau_\\text{max}$ - maximum number of iterations\n",
    "<br/>$\\lambda$ - L2 regularisation penalty\n",
    "\n",
    "While the error function $\\pmb{E(w)}$ results to a value > $\\epsilon$ or the maximum number of iterations ($\\tau_\\text{max}$) has not been reached:\n",
    "1. shuffle the training data\n",
    "2. for each training data $\\pmb{x}_n$, \n",
    "    - create prediction, $\\pmb{y}_\\text{pred} = w_0 + w_1x_1 + w_2x_2 + ... + w_jx_j$\n",
    "    - update the weights by $\\pmb{w}_{(i+1)} := \\pmb{w}_{(i)} - \\eta * \\Delta E(\\pmb{w}_{(i)})$\n",
    "    <br />note: $\\Delta E(\\pmb{w}_{(i)}) = -(\\pmb{y}_\\text{pred} - \\pmb{y}_n) \\pmb{x}_n + \\lambda * \\pmb{w}_{(i)}$\n",
    "3. check the error of the new weights\n",
    "$$\n",
    "E(\\pmb{w}_{(i+1)}) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\pmb{y}_\\text{pred} - \\pmb{y}_n)^2 + \\frac{\\lambda}{2} * \\pmb{w}_{(i+1)}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function that performs Stochastic Gradient Descent (SGD) to create a linear regression model\n",
    "# training.label - training target variable (y)\n",
    "# training.data - training input variables (X)\n",
    "# max.iteration - maximum number of iterations\n",
    "# learning.rate - learning rate\n",
    "# threshold - the threshold value of the error function\n",
    "# lambda - L2 regularisation penalty\n",
    "sgd <- function(training.label,training.data,max.iteration,learning.rate,threshold,lambda){\n",
    "    \n",
    "    # initialisation for gradient descent\n",
    "    iteration <- 1\n",
    "    X <- as.matrix(cbind('X0'=1, training.data))\n",
    "    X.length <- nrow(X)  \n",
    "    max.iteration <- max.iteration * X.length\n",
    "  \n",
    "    Y <- training.label\n",
    "    Weights <- matrix(,nrow=max.iteration, ncol=ncol(X)) # be used to store the estimated oefficients\n",
    "    Weights[1,] <- runif(ncol(X)) # initial weights (any better idea?)\n",
    "    terminate <- FALSE\n",
    "      \n",
    "    error <- data.frame('iteration'=1:max.iteration)  # to be used to trace the test and training errors in each iteration\n",
    "\n",
    "    # Stochastic Gradient Descent\n",
    "    while(!terminate){\n",
    "        # check termination criteria:\n",
    "        # - max iteration is not met\n",
    "        # - the error has converged\n",
    "        terminate <- iteration >= max.iteration | error_func(X, Weights[iteration,],Y,lambda)<=threshold\n",
    "\n",
    "        # shuffle data: (this means the algorithm is using SGD)\n",
    "        train.index <- sample(1:nrow(X), nrow(X), replace = FALSE)\n",
    "\n",
    "        # for each shuffled training data point, update the weights\n",
    "        for (i in train.index){\n",
    "            # check termination criteria:\n",
    "            if (iteration >= max.iteration | error_func(X, Weights[iteration,],Y,lambda)<=threshold) {terminate<-TRUE;break}\n",
    "\n",
    "            # predict 'Y' using the feature and the current weights\n",
    "            Y_pred <- predict_func(X[i,], Weights[iteration,])\n",
    "\n",
    "            # update the weights after checking this point\n",
    "            # new weights <- previous weight + learning rate * derivative of predict_func(w) * jth feature of the ith observation\n",
    "            Weights[(iteration + 1),] <- sgd_weight_update(X[i,], Weights[iteration,], Y[i], Y_pred, lambda, learning.rate)\n",
    "\n",
    "            # record the error using the new model:\n",
    "            error[iteration, 'iteration'] <- iteration\n",
    "            error[iteration, 'SGD'] <- error_func(X, Weights[iteration,],Y,lambda)\n",
    "          \n",
    "            # update the counter:\n",
    "            iteration <- iteration + 1   \n",
    "        }\n",
    "    }\n",
    "    return(list('vals'=error,'W'=Weights, 'finalWeights'=Weights[iteration,]))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent is another iterative way to build a linear regression model. Its difference with SGD is that it uses **all** training data to update the weights. This is more computationally expensive than SGD, but it performs a more steady descent and produces a better model every iteration.\n",
    "\n",
    "The steps to performing BGD are:\n",
    "\n",
    "Given:\n",
    "<br/>$\\epsilon$ - error threshold value\n",
    "<br/>$\\eta$ - learning rate\n",
    "<br/>$\\tau_\\text{max}$ - maximum number of iterations\n",
    "<br/>$\\lambda$ - L2 regularisation penalty\n",
    "\n",
    "No shuffling of data is needed, as all are used to update the weights every iteration. See the difference in $\\Delta E(w)$.\n",
    "\n",
    "1. while the error function $E(w)$ result is > $\\epsilon$ or the maximum number of iterations ($\\tau$) has not been reached:\n",
    "    - create prediction, $\\pmb{y}_\\text{pred} = w_0 + w_1x_1 + w_2x_2 + ... + w_jx_j$\n",
    "    - update the weights by $\\pmb{w}_{(i+1)} := \\pmb{w}_{(i)} - \\eta * \\Delta E(w)$\n",
    "    <br />note: $\\Delta E(\\pmb{w}_{(i)}) = -(\\pmb{y}_\\text{pred} - \\pmb{y}_n) \\pmb{x} + \\lambda * \\pmb{w}_{(i)}$\n",
    "3. check the error of the new weights\n",
    "$$\n",
    "E(\\pmb{w}_{(i+1)}) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\pmb{y}_\\text{pred} - \\pmb{y}_n)^2 + \\frac{\\lambda}{2} * \\pmb{w}_{(i+1)}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that performs Batch Gradient Descent (BGD) to create a linear regression model\n",
    "# training.label - training target variable (y)\n",
    "# training.data - training input variables (X)\n",
    "# max.iteration - maximum number of iterations\n",
    "# learning.rate - learning rate\n",
    "# threshold - the threshold value of the error function\n",
    "# lambda - L2 regularisation penalty\n",
    "bgd <- function(training.label, training.data, max.iteration, learning.rate, threshold,lambda){\n",
    "    \n",
    "    # initialisation for gradient descent\n",
    "    iteration <- 1\n",
    "    X <- as.matrix(cbind('X0'=1, training.data))\n",
    "    X.length <- nrow(X)\n",
    "    Y <- training.label\n",
    "    Weights <- matrix(,nrow=max.iteration, ncol=ncol(X)) # be used to store the estimated oefficients\n",
    "    Weights[1,] <- runif(ncol(X)) # initial weights (any better idea?)\n",
    "    terminate <- FALSE\n",
    "    \n",
    "    error <- data.frame('iteration' = 1:max.iteration)  # to be used to trace the test and training errors in each iteration\n",
    "    \n",
    "    # Batch Gradient Descent\n",
    "    while(!terminate){\n",
    "        # check termination criteria:\n",
    "        # - max iteration is not met\n",
    "        # - the error has converged\n",
    "        terminate <- iteration >= max.iteration | error_func(X, Weights[iteration,],Y,lambda)<=threshold\n",
    "\n",
    "        if (terminate){\n",
    "            break\n",
    "        }\n",
    "        \n",
    "        # for BGD, shuffling the training data is not needed since we use them all to compute for the new weights\n",
    "\n",
    "        # update the weights using ALL training data\n",
    "        Y_pred <- predict_func(X, Weights[iteration,])\n",
    "        Weights[(iteration + 1),] <- bgd_weight_update(X, Weights[iteration,], Y, Y_pred, lambda, learning.rate)\n",
    "\n",
    "        # record the error:\n",
    "        # for each (X.length) SGD, there is 1 BGD\n",
    "        iteration_value <- (iteration - 1) * X.length + 1\n",
    "        error[iteration, 'iteration'] <- iteration_value\n",
    "        error[iteration, 'BGD'] <- error_func(X, Weights[iteration,],Y,lambda)\n",
    "        iteration <- iteration + 1\n",
    "    }\n",
    "    \n",
    "    return(list('vals'=error,'W'=Weights, 'finalWeights'=Weights[iteration,]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q1-3__) Now let’s compare SGD and BGD implementations of Ridge regression from Q1-2:\n",
    "\n",
    "__Q1-3A__. Load __Task2A_train.csv__ and __Task2A_test.csv__ datasets,\n",
    "\n",
    "__Q1-3B__. Set the termination criterion as maximum of 20 weight updates for BGD, which is equivalent to $20 \\times N$ weight updates for SGD (where $N$ is the number of training data),\n",
    "\n",
    "__Q1-3C__. Run your implementations of SGD and BGD while all parameter settings (initial values, learning rate etc.) are exactly the same for both algorithms. During run, record training error rate every time the weights get updated. Create a plot of error rates (use different colors for SGD and BGD), where the x-axis is the number of visited data points and y-axis is the error rate. Note that for every $N$ errors for SGD in the plot, you will only have one error for BGD; the total length of the x-axis will be $20 \\times N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"attributes are not identical across measure variables; they will be dropped\""
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAv8RNTU1oaGh8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\nJIAEEBJAAggJIAH/A2H5GtF3fIc4AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "# MAIN LOGIC FOR QUESTION 1\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "# read the data\n",
    "trainingA.file <- read.csv('Task2A_train.csv')\n",
    "testingA.file <- read.csv('Task2A_test.csv')\n",
    "\n",
    "# scale the input variable values (for faster computation - and to avoid producing errors as high as INF (infinity))\n",
    "trainingA.data <- scale(trainingA.file[,-ncol(trainingA.file)])\n",
    "trainingA.value <- trainingA.file[,ncol(trainingA.file)]\n",
    "\n",
    "# initialise the parameters in building the models\n",
    "max_epoch = 20\n",
    "epsilon = .0001 * mean(trainingA.value)\n",
    "eta = .01\n",
    "lambda = 1\n",
    "\n",
    "# create a linear regression model using Stochastic Gradient Descent (SGD)\n",
    "respSGD <- sgd(trainingA.value, trainingA.data, max_epoch, eta, epsilon, lambda)\n",
    "\n",
    "# create a linear regression model using Batch Gradient Descent (BGD)\n",
    "respBGD <- bgd(trainingA.value, trainingA.data, max_epoch, eta, epsilon, lambda)\n",
    "\n",
    "# format the responses for plotting\n",
    "# 1. merge both BGD and SGD (training) errors\n",
    "# 2. melt the table by \"iteration\" column\n",
    "# 3. omit null values\n",
    "errorsA <- merge(respBGD$vals,respSGD$vals,by=\"iteration\", all = TRUE)\n",
    "errorsA.melt <- melt(errorsA, id = \"iteration\")\n",
    "errorsA.melt <- na.omit(errorsA.melt)\n",
    "\n",
    "ggplot(data=errorsA.melt, aes(x=iteration, y=value, color=variable)) + geom_line() +\n",
    "       scale_color_discrete(guide = guide_legend(title = NULL)) + theme_minimal() +\n",
    "       ggtitle(\"Errors - SGD vs BGD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q1-3D__. Explain your observation based on the errors plot you generated in Part C. Particularly, discuss the convergence speed and the fluctuations you see in the error trends.\n",
    "\n",
    "**SGD** converges faster than **BGD**. However, the error **gradually decreases** for BGD until it reaches a minimum, while in SGD the error **fluctuates** from high to low (and vice-versa) due to its stochastic nature. BGD approaches the minimum directly and does not regress into a larger error because it takes a single start point and steeps into a gradual descent. SGD may approach the minimum faster but the resulting errors are unstable (in the sense that the next error may be bigger than the previous one) because it starts at a random point every time it attempts to descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B. Bias-Variance Analysis\n",
    "In this part, you conduct a bias-variance study on the Ridge regression that you have developed in Part A. This task assesses your analytical skills, and is based on Chapter 6 of Module 2. You basically recreate Figure 2.6.3 of Module 2 using your implementation of Ridge regression (with SGD) from Part A.\n",
    "\n",
    "### Question 2 [Bias-Variance for Ridge Regression, 30 Marks]\n",
    "__2A__. Load __Task2B_train.csv__ and __Task2B_test.csv__ sets,\n",
    "\n",
    "__2B__. Sample 50 sets from the provided training set, each of which having 100 randomly selected data points (with replacement).\n",
    "\n",
    "__2C__. For each $\\lambda$ in $\\{0, 0.2, 0.4, 0.6, \\ldots, 5\\}$ do:\n",
    "\n",
    "1. Build 50 regression models using the sampled sets\n",
    "\n",
    "2. Based on the predictions of these models on the testset, calculate the (average) test error, variance, $(\\text{bias})^2$, and  $\\text{variance } +(\\text{bias})^2$.\n",
    "\n",
    "Plot the (average) test error, variance, $(\\text{bias})^2$, and $\\text{variance } +(\\text{ bias})^2$ versus $\\log (\\lambda)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "# MAIN LOGIC FOR QUESTION 2\n",
    "# \n",
    "# Part 1 - initialisation of values\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "# read and scale the training and testing files\n",
    "trainingB.file <- read.csv('Task2B_train.csv')\n",
    "testingB.file <- read.csv('Task2B_test.csv')\n",
    "trainingB.data <- scale(trainingB.file[,-ncol(trainingB.file)])\n",
    "trainingB.value <- scale(trainingB.file[,ncol(trainingB.file)])\n",
    "testingB.data <- scale(testingB.file[,-ncol(testingB.file)])\n",
    "testingB.value <- scale(testingB.file[,ncol(testingB.file)])\n",
    "\n",
    "# add an intercept input variable x0\n",
    "testingB.data <- as.matrix(cbind('X0'=1, testingB.data))\n",
    "\n",
    "# initialise values for computation\n",
    "weights <- c()\n",
    "\n",
    "# initialise lambdas\n",
    "lambdas <- c(0, 0.2, 0.4, 0.6, 0.8, \n",
    "             1.0, 1.2, 1.4, 1.6, 1.8, \n",
    "             2.0, 2.2, 2.4, 2.6, 2.8, \n",
    "             3.0, 3.2, 3.4, 3.6, 3.8, \n",
    "             4.0, 4.2, 4.4, 4.6, 4.8,\n",
    "            5.0)\n",
    "\n",
    "# initialise parameters for the regression models\n",
    "max_epoch = 20\n",
    "epsilon = .001 * mean(trainingB.value)\n",
    "eta = .01\n",
    "\n",
    "D <- 100  # the number of samples in each selected dataset (100)\n",
    "L <- 50 # number of datasets (50 bootstraps) \n",
    "M <- length(lambdas)  # number of lambdas\n",
    "N <- nrow(trainingB.data) # number of training data\n",
    "test.N <- nrow(testingB.data) # number of testing data\n",
    "\n",
    "# generate a dataframe similar to Activity 2.3 called y\n",
    "# structure: \n",
    "# -------------------------------------------------------------\n",
    "# | lambda   | bootstrap          | y1 | y2 | y3 | ... | y930 |\n",
    "#  ---------- -------------------- ---- ---- ---- ----- -------\n",
    "# |(0 - 5.0) | 1-50 (per lambda)  | predicted value for y_n   |\n",
    "# -------------------------------------------------------------\n",
    "# instead of models (polynomial orders), we have lambdas instead\n",
    "\n",
    "# create empty matrix to record result in each sampling iteration\n",
    "y <- data.frame(matrix(0,nrow=L*M, ncol=test.N+2))\n",
    "names(y) <- c('m', 'l',  paste('y',1:(test.N), sep=''))\n",
    "\n",
    "# populate the lambdas (M) in the y data frame\n",
    "j <- 0\n",
    "for(m in 1:M){\n",
    "    for(l in 1:L){\n",
    "        y$m[l + (L*j)] <- lambdas[m]\n",
    "    }\n",
    "    j <- j + 1\n",
    "}\n",
    "\n",
    "# populate (L)\n",
    "y$l <- rep(1:L, M, each = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "# MAIN LOGIC FOR QUESTION 2\n",
    "# \n",
    "# Part 2 - Creating predictions\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "# per lambda\n",
    "for (m in c(1:M)){\n",
    "    for (l in 1:L){\n",
    "        # create L (50) bootstraps \n",
    "        indexes <- sample(1:nrow(trainingB.file), D, replace = TRUE)\n",
    "        # create a model using the training bootstrap\n",
    "        weights <- sgd(trainingB.value[indexes], trainingB.data[indexes,], max_epoch, eta, epsilon, lambdas[m])$finalWeights\n",
    "        # populate the predicted values per y\n",
    "        y[y$m==lambdas[m] & y$l==l, -c(1,2)] <- predict_func(testingB.data, weights)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "Bias describes how accurate can the model predict the actual values. Mathematically, it is the difference between the expected predicted value and the actual value of the target variable. High bias leads to underfitting, where the model does not sufficiently explain the training data.\n",
    "\n",
    "$$\n",
    "\\text{Bias} = E[Y_\\text{predicted}] - Y_\\text{actual}\n",
    "$$\n",
    "\n",
    "## Variance\n",
    "Variance describes the how close (or far) the predicted values are from each other. Mathematically, it is the mean squared differences of the predicted values from its mean. High variance leads to overfitting, where the model extremely fits the training data so well that it fails accommodate other datasets.\n",
    "\n",
    "$$\n",
    "\\text{Variance} = E[(Y_\\text{predicted} - E[Y_\\text{predicted}])^2]\n",
    "$$\n",
    "\n",
    "## Generalisation Error\n",
    "The error is the difference of the squared differences between the predicted value and the actual value. In terms of bias and variance, it is $\\text{bias}^2 + \\text{variance}$.\n",
    "\n",
    "$$\n",
    "\\text{Generalisation Error} = E[(Y_\\text{predicted} - Y_\\text{actual})^2]  \\\\ = (E[Y_\\text{predicted}] - Y_\\text{actual})^2 + E[(Y_\\text{predicted} - E[Y_\\text{predicted}])^2]  \\\\ = \\text{bias}^2 + \\text{variance} \\\\                   \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAAv8RNTU1oaGh8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\nEQhFYhiBUCSGEQhFYhiB/AcZJ3J3oMpENwAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "# MAIN LOGIC FOR QUESTION 2\n",
    "# \n",
    "# Part 3 - Computation of bias, variance, and error\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "# compute mean of the predicted values E[theta_hat]\n",
    "y.bar <- aggregate(y, list(y$m),mean)\n",
    "y.bar <- as.matrix(y.bar[,-c(1:3)])\n",
    "\n",
    "error <- matrix(0,nrow=M)\n",
    "bias2 <- matrix(0,nrow=M)\n",
    "variance <- matrix(0,nrow=M)\n",
    "\n",
    "# for each lambda\n",
    "for (m in c(1:M)){\n",
    "    # bias^2 is E[(E[Y_pred] - Y_actual)^2]\n",
    "    bias2[m] <- mean((y.bar[m,] - testingB.value)^2)\n",
    "    # variance is E[Y_pred - E[Y_pred]]^2\n",
    "    variance[m] <- mean((y[y$m==lambdas[m],-c(1,2)]-y.bar[m,])^2)\n",
    "    # error is E[(Y_pred - Y_actual)^2] \n",
    "    error[m] <- mean((y[y$m==lambdas[m],-c(1,2)]-testingB.value)^2)\n",
    "}\n",
    "\n",
    "# format data for plotting\n",
    "dat <- as.data.frame(cbind(lambdas, bias2=bias2, variance, error, bias2+variance))\n",
    "names(dat) <- c('lambda', 'bias2', 'variance', 'error', 'bias2variance')\n",
    "\n",
    "dat.m <- melt(dat, id='lambda')\n",
    "ggplot(data=dat.m, aes(x=log(lambda), y=value, color=variable)) + geom_line() + \n",
    "        labs(title='Bias-Variance Decomposition') +  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2D__. Based on your plot in the previous step (C), what’s the best value for $\\lambda$? Explain your answer in terms of the bias, variance, and test error.\n",
    "\n",
    "The best value for $\\lambda$ is where the **Mean Squared Error** ($\\text{bias}^2 + \\text{variance}$) is at the minimum. Based on the plot, the ($\\text{bias}^2 + \\text{variance}$) is at lowest when **$\\lambda$ is 5**. According to the lecture slides, the optimal model is where the $\\text{bias}^2$ and $variance$ **both** have low values, which is when $\\pmb{\\lambda = 5}$. Variance decreases dramatically due to the regularisation performed. Bias, although increasing, is increasing at a very slow pace even if we pick higher lambdas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C. Logistic Regression vs. Bayesian Classifier\n",
    "This task assesses your analytical skills. You need to study the performance of two well-known generative and discriminative models, i.e. Bayesian classifier and logistic regression, as the size of the training set increases. Then, you show your understanding of the behaviour of learning curves of typical generative and discriminative models.\n",
    "\n",
    "### Question 3 [Discriminative vs Generative Models, 35 Marks]\n",
    "\n",
    "\n",
    "__3A__. Load __Task2D_train.csv__ and __Task2D_test.csv__ as well as the Bayesian classifier (BC) and logistic regression (LR) codes from Activities 2 and 3 in Module 3.\n",
    "\n",
    "### Naive Bayes' Classifier\n",
    "\n",
    "Naives Bayes Classifier is a generative model. It derives the probability distribution of each class and predicts using these probability distributions, and uses the Bayes' theorem to calculate the posterior probabilities of a data belonging to each class $p(\\mathcal{C}_k|x)$. \n",
    "\n",
    "To generate the probability distribution using the training data, it requires the following:\n",
    "- prior probabilities $\\pmb{p(\\mathcal{C}_k)}$ which is just the fraction of each class in the whole dataset\n",
    "- class means $\\mu_k$\n",
    "- class covariance matrices $\\mathbf{S}_k$\n",
    "- shared covariance matrix$\\Sigma$\n",
    "\n",
    "The conditional probability $p(x | \\mathcal{C}_k)$ is derived from the distribution using the $\\mu_k$ and $\\Sigma$. In this case, we use the function `dmvnorm()` to derive the probability density function using the means and the covariances.\n",
    "\n",
    "Once you have calculated $p(\\mathcal{C}_k)$ and $p(x|\\mathcal{C}_k)$, Bayes theorem can now be used to calculate the posterior $p(\\mathcal{C}_k|x)$ probabilities for each testing set. \n",
    "\n",
    "$$\n",
    "p(\\mathcal{C}_k|x) = \\frac{p\\mathcal{C}_k . p(x|\\mathcal{C}_k)}{p(x)}\n",
    "$$\n",
    "\n",
    "Which ever has the bigger posterior probability is the chosen class for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "# AUXILIARY FUNCTION FOR QUESTION 3\n",
    "# \n",
    "# Part 1 - Naive Bayes' Classifier\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "## Function that creates a Bayesian Classifier \n",
    "# train.data - training input variables (x)\n",
    "# train.label - training target variable (y)\n",
    "# test.data - testing input variables (x)\n",
    "# test.label - testing target variable (y)\n",
    "bayes <- function(train.data, train.label, test.data, test.label) {\n",
    "    train.label <- factor(train.label)\n",
    "    test.label <- factor(test.label)\n",
    "    \n",
    "    c0 <- levels(train.label)[2]\n",
    "    c1 <- levels(train.label)[1]\n",
    "\n",
    "    # Class probabilities:\n",
    "    # total number of samples in class 0 divided by the total nmber of training data\n",
    "    p0.hat <- sum(train.label==c0)/nrow(train.data) \n",
    "    p1.hat <- sum(train.label==c1)/nrow(train.data) # or simply 1 - p1.hat\n",
    "\n",
    "    # Class means:\n",
    "    mu0.hat <- colMeans(train.data[train.label==c0,])\n",
    "    mu1.hat <- colMeans(train.data[train.label==c1,])\n",
    "\n",
    "    # class covariance matrices:\n",
    "    sigma0.hat <- var(train.data[train.label==c0,])\n",
    "    sigma1.hat <- var(train.data[train.label==c1,])\n",
    "\n",
    "    # shared covariance matrix: (will produce an n x n matrix) \n",
    "    sigma.hat <- p0.hat * sigma0.hat + p1.hat * sigma1.hat \n",
    "\n",
    "    # calculate posteriors:\n",
    "    posterior0 <- p0.hat*dmvnorm(x=train.data, mean=mu0.hat, sigma=sigma.hat)\n",
    "    posterior1 <- p1.hat*dmvnorm(x=train.data, mean=mu1.hat, sigma=sigma.hat)\n",
    "\n",
    "    # calculate predictions:\n",
    "    train.predict <- ifelse(posterior0 > posterior1, c0, c1)\n",
    "    test.predict <- ifelse(p0.hat*dmvnorm(x=test.data, mean=mu0.hat, sigma=sigma.hat) > p1.hat*dmvnorm(x=test.data, mean=mu1.hat, sigma=sigma.hat), c0, c1)\n",
    "    \n",
    "    train.error <- 1-sum(train.label==train.predict)/nrow(train.data)\n",
    "    test.error <- 1-sum(test.label==test.predict)/nrow(test.data)\n",
    "    \n",
    "    return(list('trainError'= train.error, 'testError' = test.error))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Logistic Regression, on the other hard, is a discriminative algorithm. It does not derive the probability distributions of each class but is just concerned with finding the **decision boundary**, that is, finding a line  (or lines) that discriminates each classes in the feature space. The posterior probability $p(\\mathcal{C}_k|x)$ of a data belonging to each class is calculated directly using the *sigmoid function*  $\\sigma(\\pmb{w}.\\mathbf{x})$:\n",
    "\n",
    "$$\n",
    "p(\\mathcal{C}_k|x) = \\sigma(\\pmb{w}.\\mathbf{x}) = \\frac{1}{1 + e ^{-(u)}}\n",
    "$$\n",
    "\n",
    "$\\text{where u} = \\pmb{w_0} + \\pmb{w_1}.\\mathbf{x_1} + ... + \\pmb{w_j}.\\mathbf{x_j}$\n",
    "\n",
    "Note that the sigmoid function returns a result $[0, 1]$, which is why it is also $p(\\mathcal{C}_k|x)$.\n",
    "\n",
    "Therefore, in this method we are also concerned with generating the weights of the feature vector, $\\pmb{w}$. Gradient descent algorithm can still be used to compute this. Based on Activity 3.3, the steps are as follows:\n",
    "\n",
    "<ol>\n",
    "\t<li>Implement sigmoid function $\\sigma(\\pmb{w}.\\mathbf{x})$, and initialize weight vector $\\pmb{w}$, learning rate $\\eta$ and stopping criterion $\\epsilon$.</li>\n",
    "\t<li>Repeat the following until the improvement becomes negligible (i.e., $|\\mathcal{L}(\\pmb{w}^{(\\tau+1)})-\\mathcal{L}(\\pmb{w}^{(\\tau)})| \\lt \\epsilon$):\n",
    "<ol>\n",
    "\t<li>Shuffle the training data</li>\n",
    "\t<li>For each datapoint in the training data do:\n",
    "<ol>\n",
    "\t<li>$\\pmb{w}^{(\\tau+1)} := \\pmb{w}^{(\\tau)} - \\eta (\\sigma(\\pmb{w}.\\mathbf{x}) - t_n) \\pmb{x}_n$</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "Therefore in the case the following **auxiliary functions** are needed:\n",
    "\n",
    "- The sigmoid function $\\sigma(\\pmb{w}.\\mathbf{x})$ which is used for prediction\n",
    "- The cost function $|\\mathcal{L}(\\pmb{w}^{(\\tau+1)})-\\mathcal{L}(\\pmb{w}^{(\\tau)})|$ to determine when to terminate the SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "# AUXILIARY FUNCTION FOR QUESTION 3\n",
    "# \n",
    "# Part 2 - Logistic Regression\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "# auxiliary function that predicts class labels\n",
    "predict <- function(w, X, c0, c1){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(ifelse(sig>0.5, c1,c0))\n",
    "}\n",
    "    \n",
    "# auxiliary function that calculates the cost function\n",
    "cost <- function (w, X, T, c0){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(sum(ifelse(T==c0, 1-sig, sig)))\n",
    "}\n",
    "\n",
    "# sigmoid function (=p(C_k|X))\n",
    "sigmoid <- function(w, x){\n",
    "    return(1.0/(1.0+exp(-w%*%t(cbind(1,x)))))    \n",
    "}\n",
    "\n",
    "## Function that creates a Logistic Regression classifier\n",
    "# train.data - training input variables (x)\n",
    "# train.label - training target variable (y)\n",
    "# test.data - testing input variables (x)\n",
    "# test.label - testing target variable (y)\n",
    "# max.iteration - maximum number of iterations\n",
    "# threshold -  error threshold value\n",
    "# learning.rate - the learning.rate\n",
    "logistic <- function(train.data, train.label, test.data, test.label, max.iteration, threshold, learning.rate){\n",
    "    \n",
    "    train.label <- factor(train.label)\n",
    "    test.label <- factor(test.label)\n",
    "    \n",
    "    c0 <- levels(train.label)[2]\n",
    "    c1 <- levels(train.label)[1]\n",
    "\n",
    "    X <- as.matrix(train.data) # rename just for convenience\n",
    "    Y <- ifelse(train.label==c0,0,1)\n",
    "    \n",
    "    train.len <- nrow(X)\n",
    "    \n",
    "    W <- matrix(,nrow=max.iteration, ncol=(ncol(X)+1)) # to be used to store the estimated coefficients\n",
    "    W[1,] <- runif(ncol(W)) # initial weight (any better idea?)\n",
    "\n",
    "    # project data using the sigmoid function (just for convenient)\n",
    "    Y_pred <- sigmoid(W[1,],X)\n",
    "    \n",
    "    costs <- data.frame('iteration'=1:max.iteration)  # to be used to trace the cost in each iteration\n",
    "    costs[1, 'cost'] <- cost(W[1,],X,Y,c0)\n",
    "    \n",
    "    iteration <- 1\n",
    "    terminate <- FALSE\n",
    "    while(!terminate){\n",
    "        # check termination criteria:\n",
    "        terminate <- iteration >= max.iteration | cost(W[iteration,],X,Y,c0)<=threshold\n",
    "\n",
    "        # shuffle data:\n",
    "        train.index <- sample(1:train.len, train.len, replace = FALSE)\n",
    "        X <- X[train.index,]\n",
    "        Y <- Y[train.index]\n",
    "\n",
    "        # for each datapoint:\n",
    "        for (i in 1:train.len){\n",
    "            # check termination criteria:\n",
    "            if (iteration >= max.iteration | cost(W[iteration,],X,T, c0) <=threshold) {terminate<-TRUE;break}\n",
    "\n",
    "            Y_pred <- sigmoid(W[iteration,],X)\n",
    "\n",
    "            # update the weights\n",
    "            W[(iteration+1),] <- W[iteration,] - learning.rate * (Y_pred[i]-Y[i]) * cbind(1, t(X[i,]))\n",
    "\n",
    "            # record the cost:\n",
    "            costs[(iteration+1), 'cost'] <- cost(W[iteration,],X,Y, c0)\n",
    "\n",
    "            # update the counter:\n",
    "            iteration <- iteration + 1\n",
    "\n",
    "            # decrease learning rate:\n",
    "            learning.rate = learning.rate * 0.999\n",
    "        }\n",
    "    }\n",
    "    # Done!\n",
    "    costs <- costs[1:iteration, ] # remove the NaN tail of the vector (in case of early stopping)\n",
    "\n",
    "    # the  final result is:\n",
    "    w <- W[iteration,]\n",
    "    \n",
    "    train.error <- 1 - (sum(predict(w, train.data, c0, c1) == train.label) / nrow(train.data))\n",
    "    test.error <- 1 - (sum(predict(w, test.data, c0, c1) == test.label) / nrow(test.data))\n",
    "    \n",
    "    return(list('trainError'= train.error, 'testError' = test.error))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3B__. Using the first 5 data points from the training set, train a BC and a LR model, and compute their test errors. In a “for loop”, increase the size of training set (5 data points at a time), retrain the models and calculate their test errors until all training data points are used. In one figure, plot the test errors for each model (with different colors) versus the size of the training set; name the plot “Learning Curve” and add it to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAv8RNTU1oaGh8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\nETFdAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "# MAIN LOGIC FOR QUESTION 3\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "## read the data\n",
    "trainingD.file <- read.csv('Task2D_train.csv')\n",
    "testingD.file <- read.csv('Task2D_test.csv')\n",
    "trainD.data <- trainingD.file[,-ncol(trainingD.file)]\n",
    "trainD.label <- trainingD.file[,ncol(trainingD.file)]\n",
    "testD.data <- testingD.file[,-ncol(testingD.file)]\n",
    "testD.label <- testingD.file[,ncol(testingD.file)]\n",
    "\n",
    "## initialise values\n",
    "D <- 5 # increase training data by adding 5 per iteration\n",
    "tau.max <- 1000 # max. number of iterations\n",
    "epsilon <- 0.01 # threshold to determine when to stop\n",
    "eta <- 0.01 # learning rate\n",
    "max.iteration <- nrow(trainingD.file) / D # number of data divided by D \n",
    "errorsC <-  data.frame('training_size' = 1:max.iteration)\n",
    "\n",
    "## main logic\n",
    "for (i in 1:max.iteration){\n",
    "    # retrieve required number of training data\n",
    "    t.data <- trainD.data[1:(D*i),]\n",
    "    t.label <- trainD.label[1:(D*i)]\n",
    "    \n",
    "    # use it to train and predict test values \n",
    "    # errors will be returned by the models\n",
    "    errorsC[i, 'training_size'] <- (D*i)\n",
    "    errorsC[i, 'bayes'] <- bayes(t.data, t.label, testD.data, testD.label)$testError\n",
    "    errorsC[i, 'logistic'] <- logistic(t.data, t.label, testD.data, testD.label, tau.max, epsilon, eta)$testError\n",
    "    \n",
    "}\n",
    "\n",
    "## plot the errors\n",
    "errorsC.melt <- melt(errorsC, id = \"training_size\")\n",
    "errorsC.melt <- na.omit(errorsC.melt)\n",
    "\n",
    "ggplot(data=errorsC.melt, aes(x=training_size, y=value, color=variable)) + geom_line() + \n",
    "        labs(title='Learning Curve') +  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3C__. Explain your observations in your report:\n",
    "    1. What does happen for each classifier when the number of training datapoints is increased?\n",
    "\n",
    "Both **Bayes Classifier (BC)** and **Logistic Regression (LR)** models perform better as the training size increases. In the case of the Task_2D datasets, both  gave out accurate predictions even with small training sizes. However, in general BC trains faster (can learn quickly in small training sets) but LR has better performance (i.e. lower error rates; better accuracy rate) than BC.\n",
    "\n",
    "    2. Which classifier is best suited when the training set is small, and which is best suited when the training set is big?\n",
    "\n",
    "**Bayes Classifier** is better for small training sets, while **Logistic Regression** is better for big training sets.\n",
    "\n",
    "    3. Justify your observations in  C.1 and C.2 by providing some speculations and possible reasons.\n",
    "\n",
    "**Bayes Classifier** is best suited when the training set is small, because the model tends to **have low variance**. It is less prone to overfitting with small samples but tends to **have high bias** because it always assumes the features it uses to predict are independent.\n",
    "\n",
    "**Logistic Regression** is best suited when the training set is big, because the model tends to **have high variance**. LR is more prone to overfitting with small datasets but has **low bias** and is more robust than BC because it also considers correlations between the features and doesn't do any naive assumptions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
