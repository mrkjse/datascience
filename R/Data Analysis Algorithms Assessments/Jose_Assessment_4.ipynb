{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# FIT5201 Assessment 4: Scalable Machine Learning\n",
    "\n",
    "**Mark Joseph Jose**\n",
    "<br/>**28066049**\n",
    "\n",
    "## Objectives\n",
    "This assignment consists of two parts and covers Module 6-scalable machine learning. Total marks for this assessment is 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part A. Soft EM for GMM with Map-Reduce\n",
    "In this part, you implement a Map-Reduce version of soft EM for GMM in Spark (using Python 2).\n",
    "\n",
    "### Question 1 [EM for GMM with Map-Reduce, 50 Marks]\n",
    "1. Load __Task4A.csv__ file,\n",
    "2. Implement soft Expectation Maximisation for Gaussian Mixture Model with Map-Reduce, as discussed in Chapter 5 of Module 6. Submit your well-documented Python code (in IPython Notebook format).\n",
    "3. Set the number of clusters to 3 and run your implementation on the provided data. In a table, report the learnt parameters. Each row of the table belongs to one cluster. Please order the rows based on the cluster size (the smallest at the first row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from operator import add # for adding in reduce and reduceByKey \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal as mvnorm\n",
    "from scipy.misc import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"x1\",\"x2\",\"x3\",\"x4\"\n",
      "5.1,3.5,1.4,0.2\n",
      "4.9,3,1.4,0.2\n",
      "4.7,3.2,1.3,0.2\n",
      "4.6,3.1,1.5,0.2\n",
      "5,3.6,1.4,0.2\n",
      "5.4,3.9,1.7,0.4\n",
      "4.6,3.4,1.4,0.3\n",
      "5,3.4,1.5,0.2\n",
      "4.4,2.9,1.4,0.2\n"
     ]
    }
   ],
   "source": [
    "!head ./Task4A.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# terminate spark object in background\n",
    "sc.stop()\n",
    "sc = SparkContext(appName=\"EM\")\n",
    "# initiate Spark Context object\n",
    "file = \"./Task4A.csv\"\n",
    "lines = sc.textFile(file)\n",
    "# remove header\n",
    "header = lines.first()\n",
    "lines = lines.filter(lambda line:line!=header)\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Map-Reduce Implentation for Soft Expectation Maximization Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm:\n",
    "\n",
    "1. Initialize the GMM parameters (cluster means and cluster covariances)\n",
    "    - K cluster means will take random uniform numbers from 0 to 1 multiplied by 15\n",
    "    - K cluster covariance matrices will take the identity matrix of D rows and K columns where D is the number of features\n",
    "2. While log likelihood is not less than threshold (i.e. convergence is not met):\n",
    "    - Perform Expectation by using **MAP**. Map each observation to each cluster posterior probability\n",
    "    - Perform Maximisation by:\n",
    "        - using **REDUCE** to calculate each cluster prior probabilities\n",
    "        - using **MAP-REDUCE** to calculate each cluster means\n",
    "        - using **MAP-REDUCE** to calculate each cluster covariance matrices\n",
    "    - Calculate the objective (log likelihood) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################### AUXILIARY FUNCTIONS ################################\n",
    "##\n",
    "##    Note: This implementation was based on Johann Faouzi & Hicham Janati's\n",
    "##          Implementation of the Gaussian Mixture Models' EM Algorithm\n",
    "##\n",
    "##    Source: https://github.com/hichamjanati/EM-Spark\n",
    "##\n",
    "################################################################################\n",
    "\n",
    "# the log-likelihood function for convergence\n",
    "def objective(rdd, pis, mus, sigmas):\n",
    "    error = rdd.map(lambda x: logsumexp(np.array([density(x, k, pis, mus, sigmas) for k in range(K)]))).reduce(lambda x,y: x+y)\n",
    "    return error\n",
    "\n",
    "# the gaussian density function\n",
    "def density(x, k, pis, mus, sigmas):\n",
    "    return np.log(pis[k]) + mvnorm.logpdf(x[0],mean=mus[k],cov=sigmas[k],allow_singular=True)\n",
    "\n",
    "# the expectation step \n",
    "def expectation(x):\n",
    "    log_gammas = np.array([density(x, k, pis, mus, sigmas) for k in range(K)])\n",
    "    log_gammas = log_gammas - logsumexp(log_gammas)\n",
    "    return [x[0], np.exp(log_gammas)]\n",
    "\n",
    "# the maximization step\n",
    "def maximization(rdd):\n",
    "    # Calculate n_ks via reduce\n",
    "    # reduce by summing all the posterior probabilities per cluster k\n",
    "    # x[1][k] is the posterior probability for cluster k\n",
    "    sizes = np.array(rdd.reduce(lambda x1,x2: [0, [x1[1][k]+x2[1][k] for k in range(K)]])[1])\n",
    "    sizes = np.clip(sizes,1e-10,np.max(sizes))\n",
    "    pis = sizes / N\n",
    "\n",
    "    # Calculate means via map-reduce\n",
    "    # map each cluster posterior probability to each observation\n",
    "    # note:\n",
    "    # x[0] is the observation\n",
    "    # x[1][k] is the posterior probability for cluster k\n",
    "    mapped_means = rdd.map(lambda x: x + [[x[0] * x[1][k] for k in range(K)]])\n",
    "    # reduce by summing all the mapped posterior probability * observation\n",
    "    # x[2][k] is the mapped posterior * observation for cluster k\n",
    "    reduced_means = mapped_means.reduce(lambda x1,x2: [0,0,[x1[2][k] + x2[2][k] for k in range(K)]])[2]\n",
    "    mus = [reduced_means[k]/sizes[k] for k in range(K)]\n",
    "\n",
    "    # Calculate sigmas via map-reduce\n",
    "    # map each cluster posterior probability to each observation and cluster mus\n",
    "    # x[0] is the observation\n",
    "    # mus[k] is the gaussian mean for cluster k\n",
    "    # x[1][k] is the posterior probability\n",
    "    # x[3][k] is the mapped posterior * (obs-means) * transpose(obs-means) for cluster k\n",
    "    mapped_sigmas = mapped_means.map(lambda x: x + [[x[1][k] * np.outer(x[0]-mus[k],x[0]-mus[k]) for k in range(K)]])\n",
    "    reduced_sigmas = mapped_sigmas.reduce(lambda x1,x2: [0,0,0,[x1[3][k] + x2[3][k] for k in range(K)]])[3]\n",
    "    sigmas = [reduced_sigmas[k]/sizes[k] for k in range(K)]\n",
    "    \n",
    "    return [pis, mus, sigmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 150\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priors</th>\n",
       "      <th>means</th>\n",
       "      <th>covariances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.299</td>\n",
       "      <td>[5.915, 2.778, 4.202, 1.297]</td>\n",
       "      <td>[[0.28, 0.1, 0.18, 0.05], [0.1, 0.09, 0.09, 0.04], [0.18, 0.09, 0.2, 0.06], [0.05, 0.04, 0.06, 0.03]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333</td>\n",
       "      <td>[5.006, 3.428, 1.462, 0.246]</td>\n",
       "      <td>[[0.12, 0.1, 0.02, 0.01], [0.1, 0.14, 0.01, 0.01], [0.02, 0.01, 0.03, 0.01], [0.01, 0.01, 0.01, 0.01]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.367</td>\n",
       "      <td>[6.545, 2.949, 5.48, 1.985]</td>\n",
       "      <td>[[0.39, 0.09, 0.3, 0.06], [0.09, 0.11, 0.08, 0.06], [0.3, 0.08, 0.33, 0.07], [0.06, 0.06, 0.07, 0.09]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   priors                         means  \\\n",
       "0   0.299  [5.915, 2.778, 4.202, 1.297]   \n",
       "1   0.333  [5.006, 3.428, 1.462, 0.246]   \n",
       "2   0.367   [6.545, 2.949, 5.48, 1.985]   \n",
       "\n",
       "                                                                                              covariances  \n",
       "0   [[0.28, 0.1, 0.18, 0.05], [0.1, 0.09, 0.09, 0.04], [0.18, 0.09, 0.2, 0.06], [0.05, 0.04, 0.06, 0.03]]  \n",
       "1  [[0.12, 0.1, 0.02, 0.01], [0.1, 0.14, 0.01, 0.01], [0.02, 0.01, 0.03, 0.01], [0.01, 0.01, 0.01, 0.01]]  \n",
       "2  [[0.39, 0.09, 0.3, 0.06], [0.09, 0.11, 0.08, 0.06], [0.3, 0.08, 0.33, 0.07], [0.06, 0.06, 0.07, 0.09]]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert each line of the text data file into a NumPy Array of float numbers\n",
    "data = lines.map(lambda line: np.array([float(l) for l in line.split(',')]))\n",
    "data.count()\n",
    "\n",
    "# Initialisation\n",
    "K = 3   # number of clusters\n",
    "D = data.first().size # number of dimensions\n",
    "N = lines.count() # number of observations\n",
    "\n",
    "print(K, D, N)\n",
    "\n",
    "# Initialization of parameters\n",
    "data_list = data.map(lambda x: [x])\n",
    "\n",
    "# initiate prior probabilities of cluster k\n",
    "pis = np.ones(K)/K\n",
    "\n",
    "np.random.seed(1)\n",
    "# initiate cluster means\n",
    "mus = np.array([np.random.uniform(0, 1, D) * 15 for i in range(K)])\n",
    "# initiate cluster covariance matrices\n",
    "sigmas = np.concatenate([np.identity(D) for i in range(K)], axis = 0).reshape(K,D,D)\n",
    "\n",
    "# DEBUG\n",
    "# print(mus)\n",
    "# print(sigmas)\n",
    "\n",
    "max_iter = 100 # maximum iteration times\n",
    "threshold = .15  # threshold of convergence\n",
    " \n",
    "i = 1\n",
    "old_obj = new_obj = 1000\n",
    "while (i < max_iter or abs(old_obj - new_obj) > threshold):\n",
    "    \n",
    "    # DEBUG\n",
    "    # print(\"Iteration \", i)\n",
    "    \n",
    "    # save the old log likelihood function \n",
    "    old_obj = new_obj\n",
    "    \n",
    "    # E step - calculate the posterior probability of x belonging to cluster k\n",
    "    gammas = data_list.map(expectation)\n",
    "    \n",
    "    # M step - update the sufficient parameters (pis - prior probability, mus - cluster means, sigmas - cluster covariances) \n",
    "    pis, mus, sigmas = maximization(gammas)    \n",
    "    \n",
    "    # DEBUG\n",
    "    # print(\"\\nPis:\")\n",
    "    # print(pis)\n",
    "    # print(\"\\nMus:\")\n",
    "    # print(mus)\n",
    "    \n",
    "    # compute the new log likelihood function\n",
    "    new_obj = objective(data_list, pis, mus, sigmas)\n",
    "    i = i + 1\n",
    "    \n",
    "print(\"Done!\")\n",
    "pd.set_option('max_colwidth',150)\n",
    "d = {'priors': np.round(pis, 3), 'means': [np.round(x, 3) for x in mus], 'covariances': [np.round(x, 2) for x in sigmas]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df = df[['priors', 'means', 'covariances']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the GMMs produced by calling pyspark's GaussianMixture.train() method\n",
    "\n",
    "The library `pyspark` has an in-house method that performs the EM algorithm using Spark via the `GaussianMixture.train()` method. We can use this to verify the resulting gaussian mixture models from the algorithm above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight =  0.340755948329 mu =  [6.24406944519,2.9711637503,5.06509921261,1.8887674108] sigma =  [[ 0.28487553  0.07050134  0.22350543  0.12127464]\n",
      " [ 0.07050134  0.06589001  0.06882737  0.04711748]\n",
      " [ 0.22350543  0.06882737  0.32373693  0.19536735]\n",
      " [ 0.12127464  0.04711748  0.19536735  0.15231708]]\n",
      "weight =  0.326050449953 mu =  [6.27998510364,2.76812086244,4.73818033973,1.45304713707] sigma =  [[ 0.59228716  0.17774862  0.69283908  0.22065863]\n",
      " [ 0.17774862  0.13433944  0.18407464  0.06787678]\n",
      " [ 0.69283908  0.18407464  0.99228468  0.30960916]\n",
      " [ 0.22065863  0.06787678  0.30960916  0.10983144]]\n",
      "weight =  0.333193601718 mu =  [5.00621125606,3.42847087502,1.4620673392,0.245976903003] sigma =  [[ 0.12170809  0.09703441  0.01600047  0.01013964]\n",
      " [ 0.09703441  0.14034373  0.0113925   0.00914124]\n",
      " [ 0.01600047  0.0113925   0.02955738  0.00595405]\n",
      " [ 0.01013964  0.00914124  0.00595405  0.01088716]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n",
    "\n",
    "# Build the model using a simple function call\n",
    "gmm = GaussianMixture.train(data, K)\n",
    "\n",
    "for i in range(K):\n",
    "    print(\"weight = \", gmm.weights[i], \"mu = \", gmm.gaussians[i].mu,\n",
    "          \"sigma = \", gmm.gaussians[i].sigma.toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resulting gaussians with priors (0.34, 0.33, 0.33) is quite close to the earlier  result (0.30, 0.33, 0.37) then we can say that the algorithm implemented for task 1 is an accurate implementation of EM using Spark map-reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part B. Document Clustering with Map-Reduce\n",
    "In this part, you are asked to write down the steps of Map-Reduce for Document Clustering. In particular, you should specify the parameters to be learnt, as well as the Map and Reduce steps that one should take to implement a distributed document clustering algorithm.\n",
    "\n",
    "__Note:__ __You do not need to implement your document clustering algorithm.__ Providing the algorithm and high-level explanation (using mathematical convention) in your report is sufficient. Indeed, we ask you to provide the steps of such algorithm (Map-Reduce for document clustering) similar to what we have provided for EM for GMMs using Map-Reduce in Chapter 5 of Module 6.\n",
    "\n",
    "### Question 2 [Document Clustering with Map-Reduce, 50 Marks]\n",
    "1. Assume we want to distribute EM for Document Clustering (described in Chapter 4 of Module 4) using Map-Reduce technique in Spark. Answer the following questions:\n",
    "   1. Which quantities should be computed in each mapper?\n",
    "   2. How the reducer updates the quantities that are sent by themappers? \n",
    "   3. Write down the steps of document clustering using Map-Reduce.\n",
    "   \n",
    "## Expectation-Maximisation Steps\n",
    "\n",
    "<u>Initialisation:</u>\n",
    "\n",
    "### 1) Initialise the parameters \n",
    "$$\n",
    "\\theta^{old}=\\ (\\varphi^{old},\\ \\mu_1,\\ \\mu_2,...,\\mu_k)\n",
    "$$\n",
    "\n",
    "Where \n",
    "<br/>all elements in $\\varphi^{old}$ are the **cluster prior probabilities** 1/K where K = number of clusters, and \n",
    "<br/> µ are the **cluster means** and a matrix of random uniform numbers whose row sums up to 1.\n",
    "\n",
    "### WHILE CONVERGENCE IS NOT MET: \n",
    "\n",
    "<u>Expectation:</u>\n",
    " \n",
    "### 2) For each document n, find the log posterior probability $ln(\\gamma\\left(z_{n,k}\\right))$ per cluster k\n",
    "\n",
    "**MAP** : Map each document *n* to cluster *k* by computing its log posterior probability $\\gamma\\left(z_{n,k}\\right)$ for cluster k\n",
    "\n",
    "$$\n",
    "\\ln{\\gamma\\left(z_{n,k}\\right)}=\\Sigma_{n=1}^N\\Sigma_{k=1}^K\\left(\\ln{\\varphi_k+\\ \\Sigma_{w\\in\\mathcal{A}}c\\left(w,n\\right)ln}\\mu_{k,w}\\right)\n",
    "$$\n",
    "\n",
    "Where \n",
    "<br/>$\\gamma\\left(z_{n,k}\\right)$ is the posterior probability of document n belonging to cluster k, \n",
    "<br/>$\\varphi_k$ is the prior probability or the mixing components of cluster k, \n",
    "<br/>$c\\left(w,n\\right)$  is the frequency of the word w in document n and \n",
    "<br/>$\\mu_{k,w}$ is the word proportion of the word w for each cluster k.\n",
    "\n",
    "<u>Maximisation:</u>\n",
    "\n",
    "### 3) For each cluster k, calculate the following:\n",
    "\n",
    "<u>** THE PRIOR PROBABILITIES $\\varphi_k$**</u>\n",
    "\n",
    "**REDUCE**: reduce each prior probability by summing the current posterior probabilities per cluster\n",
    "\n",
    "$$\n",
    "\\varphi_k = \\frac{\\Sigma_{m=1}^M\\sum_{n=1}^{N}\\gamma\\left(z_{n,k}\\right)}{N}\n",
    "$$\n",
    "\n",
    "for all machines m = 1 ... M, documents n = 1 ... N, and clusters k = 1 ... K\n",
    "\n",
    "<u>** THE CLUSTER MEANS $\\mu_k$ **</u>\n",
    "\n",
    "For the cluster means, we need a map-reduce operation for the mean numerator and another map-reduce operation for the mean denominator.\n",
    "\n",
    "### CLUSTER MEANS' NUMERATOR \n",
    "**MAP**: map each word *w* to cluster *k* by mapping the posterior probability of cluster k, $\\gamma(z_{n,k})$  to each occurence of word *w* in the documents $c(w,n)$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mu}_{k,w} = \\tilde{\\mu}_{k,w} + \\gamma(z_{n,k}) * c(w, n)\n",
    "$$\n",
    "\n",
    "for all occurences of word *w* in cluster *k*\n",
    "\n",
    "**REDUCE**: sum all the mapped posterior probabilities of cluster *k* to the word *w* $\\gamma(z_{n,k}) * c(w, n)$\n",
    "\n",
    "$$\n",
    "\\mu_{num} = \\Sigma_{m=1}^M\\tilde{\\mu}_{k,w}\n",
    "$$\n",
    "\n",
    "for all machines m = 1 ... M\n",
    "\n",
    "### CLUSTER MEANS' DENOMINATOR\n",
    "\n",
    "**MAP**: map <u>all</u> words *w'* to each cluster *k* by mapping each cluster's posterior probability to <u>all</u> words in the Dictionary $w'\\ \\in\\mathcal{A}$\n",
    "\n",
    "$$\n",
    "\\tilde{\\mu}_{k, w'} = \\tilde{\\mu}_{k, w'} + \\gamma(z_{n,k})* c(w',n)\n",
    "$$\n",
    "\n",
    "for all w' in the dictionary\n",
    "\n",
    "**REDUCE**: sum all the mapped posterior probabilities and words w' in cluster k \n",
    "\n",
    "$$\n",
    "\\mu_{denom} = \\Sigma_{m=1}^M\\tilde{\\mu}_{k, w'}\n",
    "$$\n",
    "\n",
    "for all machines m = 1 ... M\n",
    "\n",
    "Then, to get the cluster means of word *w* $\\mu_{k,w}$, divide the map-reduced $\\mu_\\text{num}$ by $\\mu_\\text{denom}$\n",
    "\n",
    "$$\n",
    "\\mu_{k,w} = \\frac{\\mu_\\text{num}}{\\mu_\\text{denom}}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
