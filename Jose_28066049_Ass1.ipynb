{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 1\n",
    "#### Student Name: Mark Joseph Jose\n",
    "#### Student ID: 28066049\n",
    "\n",
    "Date: 23/03/2017\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* BeautifulSoup\n",
    "* nltk\n",
    "* itertools (to help iterating dictionaries)\n",
    "* CountVectorizer (to create a count vector)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "This assessment is about extracting information from the XML file of patents and generating the count vectors of each patent's abstracts. \n",
    "\n",
    "To extract information from the file, I used BeautifulSoup, which makes the retrieval of data easy to do. One of the earliest problems I've encountered is that the XML is not made up of a single parent, but it starts with a collection of sibling nodes. Fortunately, BeautifulSoup's default parser (\"lxml\") parses the file without any problems, so I went on with it.\n",
    "\n",
    "To generate the count vector from each abstract, I've used the concepts discussed in the lectures 3 and 4, mainly about using NLTK libraries to tokenize and manipulate these tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.13 :: Anaconda 4.3.0 (64-bit)\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.probability import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import MWETokenizer\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2500 patents are stored in an xml file, and each patent is formatted with the following structure:\n",
    "\n",
    "``` xml\n",
    "    <us-patent-grant>\n",
    "        <publication-reference>\n",
    "            <doc-number></doc-number>\n",
    "        </publication-reference>\n",
    "        \n",
    "        <!-- more nodes here -->\n",
    "        \n",
    "        <classifications-ipcr>\n",
    "            <classification-ipcr>\n",
    "                <section> </section>\n",
    "                <class> </class>\n",
    "                <subclass> </subclass>\n",
    "                <maingroup> </maingroup>\n",
    "                <subgroup> </subgroup>\n",
    "            </classification-ipcr>\n",
    "        </classifications-ipcr>\n",
    "        \n",
    "        <!-- more nodes here -->\n",
    "        \n",
    "        <abstract>\n",
    "        </abstract>\n",
    "        \n",
    "        <!-- more nodes here -->\n",
    "    </us-patent-grant>\n",
    "         ...\n",
    "```\n",
    "\n",
    "It was easy to determine which of the tags pertain to the patent id, classification, etc. thanks to the sample output files given. I just had to follow the pattern in the file while mining the information.\n",
    "\n",
    "Here I used BeautifulSoup's default parser to get all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = open(\"patents.xml\", \"r\")\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check - did we find all the patents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('us-patent-grant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we've got all 2500 of them. **Now let's start mining the data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing XML and Extracting all the required information \n",
    "\n",
    "This section is made up of the first two tasks in the assessment:\n",
    "1. Extracting the *classification* - where we find a patent's section, class, subclass, main group, and sub group\n",
    "2. Extracting the *citation network* - where we find the respective patent ids that a patent has cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize lists and dictionary to store data\n",
    "patent_ids = []\n",
    "classifications = []\n",
    "citations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I have defined a method to write the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(file_name, list_name):\n",
    "    file = open(file_name, \"w\")\n",
    "    for item in list_name:\n",
    "      file.write(\"%s\\n\" % item.encode(\"utf-8\"))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extract all the classifications\n",
    "\n",
    "For this I used BeautifulSoup's **find_all()** and **find()** methods. I was expecting some problems like missing nodes or invalid data, but fortunately all the required patent data are complete so the extraction was pretty straightforward.\n",
    "\n",
    "I would have liked to use XPath to retrieve the text but sadly BeautifulSoup does not support this. Nevertheless, this library is the easiest one to use so I sticked with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "# extract classifications\n",
    "# for each <us-patent-grant>, find <publication-reference>\n",
    "for upg in soup.find_all('us-patent-grant'):\n",
    "    # extract the <doc-number> inside the <publication-reference>\n",
    "    patent_id = upg.find('publication-reference').find('doc-number').text\n",
    "    if(patent_id != None):\n",
    "        # create a list of patent ids. we will need it later.\n",
    "        patent_ids.append(patent_id)\n",
    "        # extract the classifications under <classification-ipcr>\n",
    "        classification = upg.find('classifications-ipcr').find_all('classification-ipcr')\n",
    "\n",
    "        # extract section\n",
    "        section_text = classification[0].find('section').text\n",
    "        # extract class\n",
    "        class_text = classification[0].find('class').text\n",
    "        # extract subclass\n",
    "        subclass_text = classification[0].find('subclass').text\n",
    "        # extract main group\n",
    "        maingroup_text = classification[0].find('main-group').text\n",
    "        # extract subgroup\n",
    "        subgroup_text = classification[0].find('subgroup').text\n",
    "                \n",
    "        # append everything into the prescribed format\n",
    "        classifications.append(patent_ids[index] + ':' \n",
    "                                   + section_text + ',' \n",
    "                                   + class_text + ',' \n",
    "                                   + subclass_text + ',' \n",
    "                                   + maingroup_text + ',' \n",
    "                                   + subgroup_text) \n",
    "\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check.** Did we get the correct data and format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PP021722:A,01,H,5,00',\n",
       " u'RE042159:G,01,B,7,14',\n",
       " u'RE042170:G,06,F,11,00',\n",
       " u'07891018:A,41,D,13,00',\n",
       " u'07891019:A,41,D,13,00']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now put everything into the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_file(\"classification.txt\", classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extract the patent citation network\n",
    "\n",
    "Continuing from the last task, I used BeautifulSoup's **find_all()** and **find()** to crawl the file and extract each of the patent's citation network.\n",
    "\n",
    "All of a patent's citations are stored under the *references-cited* node, and we can get each citation's patent id under each *citation* node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract citations\n",
    "citations_strings = []\n",
    "index = 0\n",
    "for upg in soup.find_all('us-patent-grant'):\n",
    "    citations = upg.find('references-cited').find_all('citation')\n",
    "    citations_list = []\n",
    "    \n",
    "    # add each citation to the citation list\n",
    "    for citation in citations:\n",
    "        citations_list.append(citation.find('doc-number').get_text())\n",
    "\n",
    "    # format into the required way\n",
    "    # note: here I re-used the patent_ids list that I made earlier\n",
    "    citations_strings.append(patent_ids[index] +':' + '%s' % ','.join(citations_list)) \n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check.** Have I got the correct data and format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PP021722:PP17672,PP18482,PP18483',\n",
       " u'RE042159:4954776,4956606,5015948,5115193,5180978,5332966,5332996,5351003,5381090,5521496,5914593',\n",
       " u'RE042170:3988719,4206996,4803623,4905098,5012281,5161222,5172244,5253152,5263153,5270775,5301262,5341363,5355490,5410754,5537626,5559958,5574859,5580177,5611046,5647056,5828864',\n",
       " u'07891018:4561124,4831666,4920577,5105473,5134726,D338281,5611081,5729832,5845333,6115838,6332224,6805957,7089598',\n",
       " u'07891019:4355632,4702235,5032705,5148002,5603648,6439942,6757916,6910229']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations_strings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now put everything into the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_file(\"citations.txt\", citations_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing the abstracts \n",
    "\n",
    "*In this section, you should write Python code to pre-process the abstracts, convert them into the required format, and store the processed data. We suggest that you refer to the lecture and tutorial materials and the references provided in those materials.*\n",
    "\n",
    "This section utilises a lot from Python's NLTK library to derive the abstracts' count vectors. It is made up of the following steps:\n",
    "1. extracting the abstracts from the file\n",
    "2. tokenizing the abstracts\n",
    "3. creating an initial unigram vocabulary\n",
    "4. extracting meaningful bigrams to enrich the vocabulary\n",
    "5. filtering it to remove irrelevant words like stopwords, most frequent words, and hapaxes \n",
    "6. creating the final vocabulary\n",
    "7. creating the count vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Extract the abstracts from the file\n",
    "Extract the abstracts per patent and store them into the dictionary. Note that this is where I do **case-normalization** as I turn the abstracts into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary of abstracts, where the patent id is the key\n",
    "abstracts = {}\n",
    "index = 0\n",
    "for upg in soup.find_all('us-patent-grant'):\n",
    "    abstracts[patent_ids[index]] = upg.find('abstract').find('p').text.lower()\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have the correct abstracts. Define a method that prints our dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_dictionary(dictionary, length):\n",
    "    index = 0\n",
    "    for key, value in dictionary.iteritems():\n",
    "        print key + '\\t' + str(value)\n",
    "        if index == length-1:\n",
    "            break\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we got the abstracts all right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07910771\tthe invention relates to a method for producing acrylic acid in one step by an oxydehydration reaction of glycerol in the presence of molecular oxygen. the reaction preferably carried out in gaseous phase in the presence of a suitable catalyst.\n",
      "07910479\ta method for manufacturing a photodiode array includes providing a semiconductor substrate having first and second main surfaces opposite to each other. the semiconductor substrate has a first layer of a first conductivity proximate the first main surface and a second layer of a second conductivity proximate the second main surface. a via is formed in the substrate which extends to a first depth position relative to the first main surface. the via has a first aspect ratio. generally simultaneously with forming the via, an isolation trench is formed in the substrate spaced apart from the via which extends to a second depth position relative to the first main surface. the isolation trench has a second aspect ratio different from the first aspect ratio.\n",
      "07909645\ta housing is provided for a coaxial cable connector that terminates a coaxial cable. the housing includes a cable-receiving end portion configured to engage an insulating cover of the coaxial cable, a mating end portion configured to engage another coaxial connector, and a base extending between the cable-receiving end portion and the mating end portion. the base is configured to engage an outer electrical conductor of the coaxial cable. the housing also includes an electrical contact. the electrical contact includes an extension extending outwardly from the electrical contact, wherein the extension is configured to engage the outer electrical conductor of the coaxial cable.\n"
     ]
    }
   ],
   "source": [
    "print_dictionary(abstracts, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tokenize each patent abstract\n",
    "Now it's time to tokenize the abstracts. For this I used **nltk's built in tokenizer** but also included an **isalpha()** check to ensure all of the characters in each token is an alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize the abstracts\n",
    "tokenized_abstracts = {} \n",
    "for key,value in abstracts.iteritems():\n",
    "    tokenized_abstracts[key] = [word for word in nltk.word_tokenize(value) if word.isalpha()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07910771\t[u'the', u'invention', u'relates', u'to', u'a', u'method', u'for', u'producing', u'acrylic', u'acid', u'in', u'one', u'step', u'by', u'an', u'oxydehydration', u'reaction', u'of', u'glycerol', u'in', u'the', u'presence', u'of', u'molecular', u'oxygen', u'the', u'reaction', u'preferably', u'carried', u'out', u'in', u'gaseous', u'phase', u'in', u'the', u'presence', u'of', u'a', u'suitable', u'catalyst']\n",
      "07909794\t[u'an', u'autoinflating', u'catheter', u'and', u'balloon', u'assembly', u'having', u'an', u'autoregulating', u'structure', u'to', u'prevent', u'overinflation', u'of', u'the', u'balloon', u'as', u'a', u'result', u'of', u'variable', u'fluid', u'flow', u'rates', u'through', u'the', u'catheter', u'lumen', u'a', u'elastomeric', u'balloon', u'is', u'provided', u'on', u'the', u'distal', u'end', u'of', u'the', u'catheter', u'body', u'and', u'the', u'assembly', u'is', u'constructed', u'so', u'that', u'at', u'least', u'a', u'portion', u'of', u'the', u'fluid', u'flow', u'through', u'the', u'lumen', u'is', u'directed', u'to', u'the', u'balloon', u'to', u'inflate', u'it', u'as', u'the', u'balloon', u'is', u'inflated', u'more', u'and', u'more', u'of', u'the', u'fluid', u'flow', u'through', u'the', u'catheter', u'is', u'discharged', u'from', u'the', u'catheter', u'thereby', u'preventing', u'overinflation', u'of', u'the', u'balloon']\n",
      "07892023\t[u'a', u'connector', u'capable', u'of', u'reading', u'image', u'includes', u'an', u'outer', u'shell', u'a', u'seat', u'body', u'a', u'rear', u'lid', u'body', u'and', u'an', u'module', u'the', u'outer', u'shell', u'has', u'a', u'front', u'decorative', u'plate', u'two', u'sides', u'of', u'which', u'are', u'extended', u'two', u'lateral', u'plates', u'corresponding', u'to', u'each', u'other', u'and', u'an', u'upper', u'edge', u'of', u'which', u'has', u'a', u'folded', u'plate', u'folded', u'between', u'the', u'two', u'lateral', u'plates', u'and', u'which', u'has', u'a', u'plurality', u'of', u'perforations', u'arranged', u'thereon', u'the', u'seat', u'body', u'is', u'disposed', u'in', u'the', u'outer', u'shell', u'and', u'has', u'a', u'first', u'interface', u'and', u'a', u'second', u'interface', u'both', u'of', u'which', u'are', u'arranged', u'at', u'a', u'front', u'face', u'of', u'the', u'seat', u'body', u'and', u'corresponded', u'to', u'the', u'plural', u'perforations', u'an', u'electric', u'connector', u'is', u'disposed', u'in', u'the', u'second', u'interface', u'the', u'rear', u'lid', u'body', u'is', u'disposed', u'at', u'a', u'rear', u'side', u'of', u'the', u'seat', u'body', u'the', u'module', u'which', u'is', u'disposed', u'in', u'the', u'first', u'interface', u'is', u'comprised', u'of', u'a', u'circuit', u'board', u'a', u'camera', u'a', u'unit', u'and', u'a', u'plurality', u'of', u'electrically', u'conductive', u'terminals', u'after', u'the', u'camera', u'reads', u'the', u'external', u'image', u'the', u'analogous', u'signals', u'are', u'converted', u'into', u'digital', u'signals', u'by', u'a', u'unit', u'then', u'the', u'digital', u'signals', u'are', u'output', u'via', u'the', u'plural', u'thermally', u'conductive', u'terminals']\n"
     ]
    }
   ],
   "source": [
    "print_dictionary(tokenized_abstracts, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create initial vocabulary\n",
    "\n",
    "In creating the initial vocabulary I've decided to create a method that generates *meaningful bigrams* first. I based it from this  <a href=\"http://www.nltk.org/howto/collocations.html\">tutorial</a>. \n",
    "\n",
    "To ensure a diverse set of bigrams, I've used both PMI and the tokens' POS tags. **NLTK's built-in BigramCollocationFinder** allows users to generate bigrams from tokens. Afterwards, to ensure that I'll get meaningful bigrams, I filtered them further **based on their POS tags**.\n",
    "\n",
    "Bigrams which more likely to be meaningful have patterns like\n",
    "\n",
    "Pattern|Example\n",
    "--|--\n",
    "JJ+NN|secure strap\n",
    "NN+NN|crown portion\n",
    "VBG+NN|measuring cup\n",
    "VBN+NN|installed state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a precursor let's retrieve the **stop words** first. Filtering bigrams without stop words will help us find more meaningful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate stopwords from nltk's built-in stopwords list\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stop_words)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaningful Bigram Finder\n",
    "Here is my defined method for generating meaningful bigrams. First thing to do is try to generate the POS tags of the words, so the method must accept a list of sentences that will aid in giving context for word tagging. \n",
    "\n",
    "After tagging the words, remove unnecessary stopwords which we don't need in bigram finding. Only then we can find the bigrams.\n",
    "\n",
    "Then, filter the bigrams based on the combination of their POS tags as described above. Only bigrams that fit into the given pattern will be deemed meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_meaningful_bigrams(sentences_list, length):\n",
    "    \n",
    "    tagged_bigrams = []\n",
    "    bigrams = []\n",
    "    \n",
    "    for sentences in sentences_list:\n",
    "        for sentence in sentences:\n",
    "            # tokenize sentence\n",
    "            uni_sent = [word for word in nltk.word_tokenize(sentence) if word.isalpha()]\n",
    "            \n",
    "            # tag each word in the sentence\n",
    "            tagged_sent = nltk.tag.pos_tag(uni_sent)\n",
    "            \n",
    "            # remove stop words so bigrams won't have stop words\n",
    "            stopped_tagged_sent = [x for x in tagged_sent if x[0] not in stop_words] \n",
    "            \n",
    "            # find bigrams based on their tags\n",
    "            bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "            finder = nltk.collocations.BigramCollocationFinder.from_words(stopped_tagged_sent)\n",
    "            tagged_bigrams.extend(finder.nbest(bigram_measures.pmi, 5))\n",
    "    \n",
    "    # get unique bigrams\n",
    "    unique_bigrams = list(set(tagged_bigrams))\n",
    "            \n",
    "    # filter bigrams by POS tags to determine which of them is meaningful\n",
    "    # meaningful bigrams are usually in the pattern of JJ + NN, VBN + NN, VBG + NN and so on\n",
    "    filtered_bigrams = [bigram for bigram in unique_bigrams \n",
    "                        if bigram[0][1] in ['JJ','NN','VBN','VBG','NNP','NNS'] \n",
    "                        and bigram[1][1] in ['NN','NNP','NNS']]\n",
    "    \n",
    "    # remove POS tags from the filtered bigrams\n",
    "    bigram_tuples = []\n",
    "    for bigram in filtered_bigrams:\n",
    "        bigram_tuples.append((bigram[0][0], bigram[1][0]))\n",
    "        \n",
    "    # pass the whole list of filtered bigrams if less than the requested length\n",
    "    # otherwise only pass the requested number of bigrams\n",
    "    return (bigram_tuples if (len(bigrams) < length) else bigram_tuples[:length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I've retrieved the unique set of unigrams in the abstracts then added the meaningful bigrams generated from the method **generate_meaningful_bigrams()**. \n",
    "\n",
    "This will be used to re-tokenize abstracts and produce multiword tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create vocabulary that we will use to generate multiword tokens\n",
    "words = list(chain.from_iterable(tokenized_abstracts.values()))\n",
    "vocab = list(set(words))\n",
    "\n",
    "# generate bigrams\n",
    "# first generate a list of all sentences from the abstracts\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences_list = [sent_detector.tokenize(value.strip()) for value in abstracts.values()]\n",
    "\n",
    "# create 100 meaningful bigrams from these sentences\n",
    "bigrams = generate_meaningful_bigrams(sentences_list, 100)\n",
    "\n",
    "# extend the vocabulary with the generated bigrams\n",
    "vocab.extend(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the generated bigrams. Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'thickness', u'sensors'),\n",
       " (u'crown', u'portion'),\n",
       " (u'light', u'switch'),\n",
       " (u'front', u'backpack'),\n",
       " (u'electrode', u'layer'),\n",
       " (u'mounted', u'underside'),\n",
       " (u'measuring', u'plaque'),\n",
       " (u'second', u'encoding'),\n",
       " (u'impedance', u'transmission'),\n",
       " (u'data', u'record'),\n",
       " (u'ratio', u'npr'),\n",
       " (u'echo', u'cancellation'),\n",
       " (u'piezoelectric', u'transducers'),\n",
       " (u'installed', u'state'),\n",
       " (u'front', u'surface'),\n",
       " (u'memory', u'response'),\n",
       " (u'ring', u'segments'),\n",
       " (u'hydrogen', u'chloride'),\n",
       " (u'epitaxial', u'semiconductor'),\n",
       " (u'power', u'output'),\n",
       " (u'lip', u'portions'),\n",
       " (u'biasing', u'element'),\n",
       " (u'pressure', u'springs'),\n",
       " (u'exposing', u'subject'),\n",
       " (u'ink', u'reservoirs'),\n",
       " (u'extending', u'inboard'),\n",
       " (u'mobile', u'phone'),\n",
       " (u'flange', u'properties'),\n",
       " (u'hole', u'tree'),\n",
       " (u'acquired', u'imaging')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like they do! :^)** Now tokenize the abstracts again using our newly formed vocabulary, this time utilising the **MWETokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use MWETokenizer and tokenize it again\n",
    "mwe_tokenizer = MWETokenizer(vocab)\n",
    "mwe_tokenized_abs = {}\n",
    "for key,value in tokenized_abstracts.iteritems():\n",
    "    mwe_tokenized_abs[key] = mwe_tokenizer.tokenize(value) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the bigrams we have selected are mostly meaningful and do not include any stop words.\n",
    "\n",
    "Check if the MWE tokenizer worked. Did it include bigram tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07910771\t[u'the', u'invention', u'relates', u'to', u'a', u'method', u'for', u'producing', u'acrylic_acid', u'in', u'one', u'step', u'by', u'an', u'oxydehydration', u'reaction', u'of', u'glycerol', u'in', u'the', u'presence', u'of', u'molecular', u'oxygen', u'the', u'reaction', u'preferably', u'carried', u'out', u'in', u'gaseous_phase', u'in', u'the', u'presence', u'of', u'a', u'suitable', u'catalyst']\n",
      "07910479\t[u'a', u'method', u'for', u'manufacturing', u'a', u'photodiode', u'array', u'includes', u'providing', u'a', u'semiconductor_substrate', u'having', u'first', u'and', u'second', u'main_surfaces', u'opposite', u'to', u'each', u'other', u'the', u'semiconductor_substrate', u'has', u'a', u'first_layer', u'of', u'a', u'first_conductivity', u'proximate', u'the', u'first', u'main_surface', u'and', u'a', u'second', u'layer', u'of', u'a', u'second', u'conductivity', u'proximate', u'the', u'second', u'main_surface', u'a', u'via', u'is', u'formed', u'in', u'the', u'substrate', u'which', u'extends', u'to', u'a', u'first', u'depth_position', u'relative', u'to', u'the', u'first', u'main_surface', u'the', u'via', u'has', u'a', u'first_aspect', u'ratio', u'generally', u'simultaneously', u'with', u'forming', u'the', u'via', u'an', u'isolation_trench', u'is', u'formed', u'in', u'the', u'substrate', u'spaced', u'apart', u'from', u'the', u'via', u'which', u'extends', u'to', u'a', u'second', u'depth_position', u'relative', u'to', u'the', u'first', u'main_surface', u'the', u'isolation_trench', u'has', u'a', u'second', u'aspect_ratio', u'different', u'from', u'the', u'first_aspect', u'ratio']\n",
      "07909645\t[u'a', u'housing', u'is', u'provided', u'for', u'a', u'coaxial_cable', u'connector', u'that', u'terminates', u'a', u'coaxial_cable', u'the', u'housing', u'includes', u'a', u'end_portion', u'configured', u'to', u'engage', u'an', u'insulating_cover', u'of', u'the', u'coaxial_cable', u'a', u'mating_end', u'portion', u'configured', u'to', u'engage', u'another', u'coaxial_connector', u'and', u'a', u'base', u'extending', u'between', u'the', u'end_portion', u'and', u'the', u'mating_end', u'portion', u'the', u'base', u'is', u'configured', u'to', u'engage', u'an', u'outer', u'electrical_conductor', u'of', u'the', u'coaxial_cable', u'the', u'housing', u'also', u'includes', u'an', u'electrical_contact', u'the', u'electrical_contact', u'includes', u'an', u'extension', u'extending', u'outwardly', u'from', u'the', u'electrical_contact', u'wherein', u'the', u'extension', u'is', u'configured', u'to', u'engage', u'the', u'outer', u'electrical_conductor', u'of', u'the', u'coaxial_cable']\n"
     ]
    }
   ],
   "source": [
    "print_dictionary(mwe_tokenized_abs, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5.4 Filter tokens\n",
    "\n",
    "Now let's remove certain tokens from our bag. Remove tokens that are:\n",
    "- stop words\n",
    "- in the top 20 most frequent words based on document frequency\n",
    "- words that appear only in one abstract\n",
    "\n",
    "#### Remove stop words\n",
    "Remove stop words based from the NLTK's built in stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove stop words from the tokens\n",
    "mwe_tokenized_abs_stop = {}\n",
    "for key,value in mwe_tokenized_abs.iteritems():\n",
    "    mwe_tokenized_abs_stop[key] = [word for word in value if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have removed the stop words. Are they still there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07910771\t[u'invention', u'relates', u'method', u'producing', u'acrylic_acid', u'one', u'step', u'oxydehydration', u'reaction', u'glycerol', u'presence', u'molecular', u'oxygen', u'reaction', u'preferably', u'carried', u'gaseous_phase', u'presence', u'suitable', u'catalyst']\n"
     ]
    }
   ],
   "source": [
    "print_dictionary(mwe_tokenized_abs_stop, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the initial vocabulary -- how many words are there right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17320"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(chain.from_iterable(mwe_tokenized_abs_stop.values()))\n",
    "vocab_list = list(set(words))\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a lot! Now let's go and filter our tokens further\n",
    "\n",
    "#### Remove most frequent words and hapaxes\n",
    "Let's remove the most frequent words based on document frequency and hapaxes (words that appear only once) by using NLTK's **FreqDist()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82835"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract a set of unique words per abstract\n",
    "# this means each instance of a word means it was used in an abstract (once or multiple times--we don't care)\n",
    "words = list(chain.from_iterable([set(value) for value in mwe_tokenized_abs_stop.values()]))\n",
    "fd = FreqDist(words)\n",
    "\n",
    "# get the 20 most frequent words in terms of document frequency\n",
    "most_common_words = fd.most_common(20)\n",
    "\n",
    "# get words that occur only once \n",
    "abstract_hapaxes = fd.hapaxes()\n",
    "\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'includes', 1119),\n",
       " (u'one', 820),\n",
       " (u'provided', 559),\n",
       " (u'first', 536),\n",
       " (u'least', 525),\n",
       " (u'method', 523),\n",
       " (u'second', 506),\n",
       " (u'plurality', 413),\n",
       " (u'system', 376),\n",
       " (u'may', 333),\n",
       " (u'including', 333),\n",
       " (u'device', 314),\n",
       " (u'connected', 309),\n",
       " (u'formed', 289),\n",
       " (u'also', 274),\n",
       " (u'apparatus', 271),\n",
       " (u'portion', 269),\n",
       " (u'two', 265),\n",
       " (u'wherein', 247),\n",
       " (u'comprises', 243)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the words appear only in one abstract?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'reciprocation',\n",
       " u'rail_element',\n",
       " u'life_indicator',\n",
       " u'converter_converts',\n",
       " u'bus_device',\n",
       " u'radial_grind',\n",
       " u'radio_communication',\n",
       " u'positioning_indicia',\n",
       " u'localized',\n",
       " u'electrophotographic_device',\n",
       " u'debug_software',\n",
       " u'canes',\n",
       " u'hydrofluoric_acid',\n",
       " u'lineman_pole',\n",
       " u'arrangement_direction',\n",
       " u'computer_machine',\n",
       " u'packers',\n",
       " u'nozzle_member',\n",
       " u'aircraft_features',\n",
       " u'herbicide']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(abstract_hapaxes)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10650"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstract_hapaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove these words from our bag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_abstract_tokens = {}\n",
    "for key,value in mwe_tokenized_abs_stop.iteritems():\n",
    "    final_abstract_tokens[key] = [word for word in value if word not in most_common_words and word not in abstract_hapaxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07910771\t[u'invention', u'relates', u'method', u'producing', u'one', u'step', u'reaction', u'glycerol', u'presence', u'molecular', u'oxygen', u'reaction', u'preferably', u'carried', u'presence', u'suitable', u'catalyst']\n",
      "07910479\t[u'method', u'manufacturing', u'photodiode', u'array', u'includes', u'providing', u'semiconductor_substrate', u'first', u'second', u'opposite', u'semiconductor_substrate', u'first_layer', u'first_conductivity', u'proximate', u'first', u'main_surface', u'second', u'layer', u'second', u'conductivity', u'proximate', u'second', u'main_surface', u'via', u'formed', u'substrate', u'extends', u'first', u'relative', u'first', u'main_surface', u'via', u'first_aspect', u'ratio', u'generally', u'simultaneously', u'forming', u'via', u'formed', u'substrate', u'spaced', u'apart', u'via', u'extends', u'second', u'relative', u'first', u'main_surface', u'second', u'aspect_ratio', u'different', u'first_aspect', u'ratio']\n",
      "07909645\t[u'housing', u'provided', u'coaxial_cable', u'connector', u'terminates', u'coaxial_cable', u'housing', u'includes', u'end_portion', u'configured', u'engage', u'insulating_cover', u'coaxial_cable', u'mating_end', u'portion', u'configured', u'engage', u'another', u'coaxial_connector', u'base', u'extending', u'end_portion', u'mating_end', u'portion', u'base', u'configured', u'engage', u'outer', u'electrical_conductor', u'coaxial_cable', u'housing', u'also', u'includes', u'electrical_contact', u'electrical_contact', u'includes', u'extension', u'extending', u'outwardly', u'electrical_contact', u'wherein', u'extension', u'configured', u'engage', u'outer', u'electrical_conductor', u'coaxial_cable']\n",
      "07909644\t[u'provided', u'first', u'connector', u'second', u'connector', u'printed_circuit', u'board_pcb', u'connector', u'respectively', u'comprises', u'insulative_housing', u'plurality', u'terminals', u'mounted', u'therein', u'insulative_housing', u'upper', u'wall', u'lower', u'wall', u'mating_cavity', u'defined', u'therebetween', u'receiving', u'memory', u'module', u'therein', u'pcb', u'defines', u'first', u'second', u'top', u'face_thereof', u'first', u'second_terminals', u'respectively', u'soldered', u'first', u'second', u'memory_modules', u'respectively', u'inserted', u'first', u'second', u'mating', u'cavities', u'along', u'different_directions', u'avoid', u'interference', u'memory_modules']\n",
      "07909131\t[u'provided', u'extracting_section', u'capable', u'extracting', u'specific', u'input_signal', u'extracting_section', u'extracts', u'pinion', u'angle', u'corresponding', u'signal', u'indicating', u'state', u'steering', u'system', u'corresponding', u'vibration', u'increases', u'generation', u'noise', u'speed', u'reducing', u'mechanism', u'extracting_section', u'outputs', u'component', u'power', u'spectrum', u'power', u'spectrum', u'output', u'extracting_section', u'equal', u'predetermined', u'threshold', u'value', u'outputs', u'motor', u'control_signal', u'reduce', u'generated']\n"
     ]
    }
   ],
   "source": [
    "print_dictionary(final_abstract_tokens, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Create the final vocabulary\n",
    "\n",
    "The final tokens list contains all the words that we will use for the vocabulary. I will now use the **set()** function to get all the unique words from the remaining tokens.\n",
    "\n",
    "Let's see how many words I have left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6670"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the final list of words and the vocabulary\n",
    "words = list(chain.from_iterable(final_abstract_tokens.values()))\n",
    "vocab_list = list(set(words))\n",
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Generate the sparse count vectors\n",
    "\n",
    "This is the final step! I generated a sparse count vector using the **CountVectorizer()**. \n",
    "For each abstract, I generated the count vector, then matched the word and its word count using the **zip()** method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to generate the sparse count vectors\n",
    "vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "count_vectors_strings = []\n",
    "count_vectors = {}\n",
    "\n",
    "# get the word count vector per abstract\n",
    "for key,value in final_abstract_tokens.iteritems():\n",
    "    data_features = vectorizer.fit_transform([' '.join(value)]).toarray()\n",
    "    wordcount = []\n",
    "    for word,count in zip(vectorizer.get_feature_names(), data_features[0]):\n",
    "        wordcount.append(str(vocab_list.index(word))+ \":\" + str(count))\n",
    "    count_vectors[key] = \",\".join(wordcount)\n",
    "\n",
    "# format into a string\n",
    "for key,value in count_vectors.iteritems():\n",
    "    count_vectors_strings.append(key + \",\" + value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if I got the format correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'07910771,6616:1,2080:1,4104:1,1861:1,845:1,986:1,574:1,1808:1,80:1,5387:2,2582:1,5682:2,6396:1,2866:1,3419:1',\n",
       " u'07909794,2451:2,5181:4,5418:1,4573:1,3904:1,6163:1,2273:1,651:3,6006:1,4340:1,1852:2,3358:1,435:1,5785:1,1848:1,237:1,529:1,398:1',\n",
       " u'07892023,4827:1,4645:6,2975:2,4007:1,1259:1,2281:1,3801:1,6500:1,5837:1,5115:1,5815:4,6175:1,1402:1,4308:1,3897:2,3576:2,3042:1,1634:1,4714:1,5848:1,1070:2,2626:2,2710:2,6491:2,1184:3,2435:1,39:2,221:2,2000:2,5055:2,3530:2,3985:1,2804:1,2430:2,5184:1,3974:4,2102:2,1237:1,1149:1,2997:1,4223:3,51:2,2142:1,863:1',\n",
       " u'07909645,1003:1,1842:1,2668:2,3224:5,2922:1,2293:4,3801:1,5503:2,2729:3,3181:2,1092:4,1890:2,6617:2,3081:3,5848:3,2323:1,3665:2,3031:2,1054:1,3358:2,5785:1,1633:1,4469:1',\n",
       " u'07909644,450:1,1606:1,3842:1,6524:1,2283:1,3801:3,1222:1,1227:1,2651:1,376:1,3897:5,617:1,4312:2,3874:1,6169:1,209:1,5617:1,5507:1,2037:2,6491:1,2581:1,4283:1,3530:1,395:1,5785:1,2263:1,1294:3,24:4,4885:1,3082:1,4323:1,5132:1,3284:2,5185:1,2142:1,6513:2']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectors_strings[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fine. Now write the list of count vectors into the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_file(\"count_vectors.txt\", count_vectors_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write the corresponding vocabulary into the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0:selection_signal',\n",
       " u'1:first_lens',\n",
       " u'2:watermarked',\n",
       " u'3:four',\n",
       " u'4:encoded_data']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "vocab_strings = []\n",
    "for i in range(len(vocab_list)):\n",
    "    vocab_strings.append(str(i) + \":\" + vocab_list[i]) \n",
    "\n",
    "vocab_strings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_file(\"vocab.txt\", vocab_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Check the sparse count vector\n",
    "\n",
    "Let's see if the generated sparse count vector is correct. Here's the check done for patent id **07910771**.\n",
    "\n",
    "Compare the tokens (alphabetically sorted) and the generated sparse count vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carried\n",
      "catalyst\n",
      "glycerol\n",
      "invention\n",
      "method\n",
      "molecular\n",
      "one\n",
      "oxygen\n",
      "preferably\n",
      "presence\n",
      "presence\n",
      "producing\n",
      "reaction\n",
      "reaction\n",
      "relates\n",
      "step\n",
      "suitable\n",
      "\n",
      "\n",
      "carried - 1\n",
      "catalyst - 1\n",
      "glycerol - 1\n",
      "invention - 1\n",
      "method - 1\n",
      "molecular - 1\n",
      "one - 1\n",
      "oxygen - 1\n",
      "preferably - 1\n",
      "presence - 2\n",
      "producing - 1\n",
      "reaction - 2\n",
      "relates - 1\n",
      "step - 1\n",
      "suitable - 1\n"
     ]
    }
   ],
   "source": [
    "sorted_tokens = []\n",
    "sorted_tokens.extend(final_abstract_tokens[\"07910771\"])\n",
    "sorted_tokens.sort()\n",
    "print '\\n'.join(str(p) for p in sorted_tokens) \n",
    "print '\\n'\n",
    "\n",
    "i = 0\n",
    "\n",
    "# print the sorted tokens based on the number of count\n",
    "for cv in count_vectors[\"07910771\"].split(\",\"):\n",
    "    # get the index from the vocabulary\n",
    "    index = int(cv[:(cv.find(\":\"))])\n",
    "    \n",
    "    # get the count\n",
    "    count = int(cv[cv.find(\":\")+1:])\n",
    "    \n",
    "    print vocab_list[index] + ' - ' + str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the final tokens and the count vectors match, so the generated count vector must be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In summary, assessment 1 required us to extract the patent's classification, citation network, and its abstract's sparse count vectors. This is one of the challenges of text processing and data science in particular--working on a huge amount of data and wrangling it in preparation for further modeling and analysis. However, doing them is now easier thanks to these built-in tools in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
